<doc id="800" url="https://en.wikipedia.org/wiki?curid=800" title="Anime">
Anime

The word "anime" is the Japanese term for "animation", which means all forms of animated media. Outside Japan, "anime" refers specifically to animation from Japan or as a Japanese-disseminated animation style often characterized by colorful graphics, vibrant characters and fantastical themes. The culturally abstract approach to the word's meaning may open up the possibility of anime produced in countries other than Japan. Many Westerners strictly view anime as a Japanese animation product. Some scholars suggest defining anime as specifically or quintessentially Japanese may be related to a new form of Orientalism.

The earliest commercial Japanese animation dates to 1917, and Japanese anime production has since continued to increase steadily. The characteristic anime art style emerged in the 1960s with the works of Osamu Tezuka and spread internationally in the late twentieth century, developing a large domestic and international audience. Anime is distributed theatrically, by way of television broadcasts, directly to home media, and over the Internet. It is classified into numerous genres targeting diverse broad and niche audiences.

Anime is a diverse art form with distinctive production methods and techniques that have been adapted over time in response to emergent technologies. It combines graphic art, characterization, cinematography, and other forms of imaginative and individualistic techniques. The production of anime focuses less on the animation of movement and more on the realism of settings as well as the use of camera effects, including panning, zooming, and angle shots. Being hand-drawn, anime is separated from reality by a crucial gap of fiction that provides an ideal path for escapism that audiences can immerse themselves into with relative ease. Diverse art styles are used and character proportions and features can be quite varied, including characteristically large emotive or realistically sized eyes.

The anime industry consists of over 430 production studios, including major names like Studio Ghibli, Gainax, and Toei Animation. Despite comprising only a fraction of Japan's domestic film market, anime makes up a majority of Japanese DVD and Blu-ray sales. It has also seen international success after the rise of English-dubbed and subbed programming. This rise in international popularity has resulted in non-Japanese productions using the anime art style. Whether these works are anime-influenced animation or proper anime is a subject for debate amongst fans. Japanese anime accounts for 60% of the world's animated television shows, as of 2016. In 2015, the 44th President of the United States, Barack Obama, referred to anime as one of the loved ones of American youth.

Anime is an art form, specifically animation, that includes all genres found in cinema, but it can be mistakenly classified as a genre. In Japanese, the term "anime" is used as a blanket term to refer to all forms of animation from around the world. In English, "anime" () is more restrictively used to denote a "Japanese-style animated film or television entertainment" or as "a style of animation created in Japan".

The etymology of the word "anime" is disputed. The English term "animation" is written in Japanese "katakana" as ("animēshon", ) and is ("anime") in its shortened form. The pronunciation of "anime" in Japanese differs from pronunciations in other languages such as Standard English (pronunciation: ), which has different vowels and stress with regards to Japanese, where each mora carries equal stress. As with a few other Japanese words such as "saké", "Pokémon", and "Kobo Abé," English-language texts sometimes spell "anime" as "animé" (as in French), with an acute accent over the final "e", to cue the reader to pronounce the letter, not to leave it silent as Standard English orthography may suggest.

Some sources claim that "anime" derives from the French term for animation "dessin animé", but others believe this to be a myth derived from the French popularity of the medium in the late 1970s and 1980s. In English, "anime"—when used as a common noun—normally functions as a mass noun. (For example: "Do you watch anime?" or "How much anime have you collected?") Prior to the widespread use of "anime", the term "Japanimation" was prevalent throughout the 1970s and 1980s. In the mid-1980s, the term "anime" began to supplant "Japanimation". In general, the latter term now only appears in period works where it is used to distinguish and identify Japanese animation.

The word "anime" has also been criticised, e.g. in 1987, when Hayao Miyazaki stated that he despised the truncated word "anime" because to him it represented the desolation of the Japanese animation industry. He equated the desolation with animators lacking motivation and with mass-produced, overly expressionistic products relying upon a fixed iconography of facial expressions and protracted and exaggerated action scenes but lacking depth and sophistication in that they do not attempt to convey emotion or thought.

The first format of anime was theatrical viewing which originally began with commercial productions in 1917. Originally the animated flips were crude and required played musical components before adding sound and vocal components to the production. On July 14, 1958, Nippon Television aired "Mogura no Abanchūru" ("Mole's Adventure"), both the first televised and first color anime to debut. It was not until the 1960s when the first televised series were broadcast and it has remained a popular medium since. Works released in a direct to video format are called "original video animation" (OVA) or "original animation video" (OAV); and are typically not released theatrically or televised prior to home media release. The emergence of the Internet has led some animators to distribute works online in a format called "original net anime" (ONA).

The home distribution of anime releases were popularized in the 1980s with the VHS and LaserDisc formats. The VHS NTSC video format used in both Japan and the United States is credited as aiding the rising popularity of anime in the 1990s. The Laser Disc and VHS formats were transcended by the DVD format which offered the unique advantages; including multiple subtitling and dubbing tracks on the same disc. The DVD format also has its drawbacks in its usage of region coding; adopted by the industry to solve licensing, piracy and export problems and restricted region indicated on the DVD player. The Video CD (VCD) format was popular in Hong Kong and Taiwan, but became only a minor format in the United States that was closely associated with bootleg copies.

Japanese animation began in the early 20th century, when Japanese filmmakers experimented with the animation techniques also pioneered in France, Germany, the United States and Russia. A claim for the earliest Japanese animation is "Katsudō Shashin", an undated and private work by an unknown creator. In 1917, the first professional and publicly displayed works began to appear. Animators such as Ōten Shimokawa and Seitarou Kitayama produced numerous works, with the oldest surviving film being Kouchi's "Namakura Gatana", a two-minute clip of a samurai trying to test a new sword on his target only to suffer defeat. The 1923 Great Kantō earthquake resulted in widespread destruction to Japan's infrastructure and the destruction of Shimokawa's warehouse, destroying most of these early works.

By the 1930s animation was well established in Japan as an alternative format to the live-action industry. It suffered competition from foreign producers and many animators—like Noburō Ōfuji and Yasuji Murata—still worked in cheaper cutout animation rather than cel animation. Other creators, Kenzō Masaoka and Mitsuyo Seo, nonetheless made great strides in animation technique; they benefited from the patronage of the government, which employed animators to produce educational shorts and propaganda. The first talkie anime was "Chikara to Onna no Yo no Naka", produced by Masaoka in 1933. By 1940, numerous anime artists' organizations had risen, including the Shin Mangaha Shudan and Shin Nippon Mangaka. The first feature-length animated film was "Momotaro's Divine Sea Warriors" directed by Seo in 1944 with sponsorship by the Imperial Japanese Navy.

The success of The Walt Disney Company's 1937 feature film "Snow White and the Seven Dwarfs" profoundly influenced many Japanese animators. The 1950s saw a proliferation of short, animated advertisements made in Japan for television broadcasting. In the 1960s, manga artist and animator Osamu Tezuka adapted and simplified many Disney animation techniques to reduce costs and to limit the number of frames in productions. He intended this as a temporary measure to allow him to produce material on a tight schedule with inexperienced animation staff. "Three Tales", aired in 1960, was the first anime shown on television. The first anime television series was "Otogi Manga Calendar", aired from 1961 to 1964.

The 1970s saw a surge of growth in the popularity of "manga", Japanese comic books and graphic novels, many of which were later animated. The work of Osamu Tezuka drew particular attention: he has been called a "legend" and the "god of manga". His work—and that of other pioneers in the field—inspired characteristics and genres that remain fundamental elements of anime today. The giant robot genre (known as "mecha" outside Japan), for instance, took shape under Tezuka, developed into the Super Robot genre under Go Nagai and others, and was revolutionized at the end of the decade by Yoshiyuki Tomino who developed the Real Robot genre. Robot anime like the "Gundam" and "The Super Dimension Fortress Macross" series became instant classics in the 1980s, and the robot genre of anime is still one of the most common in Japan and worldwide today. In the 1980s, anime became more accepted in the mainstream in Japan (although less than manga), and experienced a boom in production. Following a few successful adaptations of anime in overseas markets in the 1980s, anime gained increased acceptance in those markets in the 1990s and even more at the turn of the 21st century. In 2002, "Spirited Away", a Studio Ghibli production directed by Hayao Miyazaki won the Golden Bear at the Berlin International Film Festival and in 2003 at the 75th Academy Awards it won the Academy Award for Best Animated Feature. 

Anime differs greatly from other forms of animation by its diverse art styles, methods of animation, its production, and its process. Visually, anime is a diverse art form that contains a wide variety of art styles, differing from one creator, artist, and studio. While no one art style predominates anime as a whole, they do share some similar attributes in terms of animation technique and character design.

Anime follows the typical production of animation, including storyboarding, voice acting, character design, and cel production ("Shirobako", itself a series, highlights many of the aspects involved in anime production). Since the 1990s, animators have increasingly used computer animation to improve the efficiency of the production process. Artists like Noburō Ōfuji pioneered the earliest anime works, which were experimental and consisted of images drawn on blackboards, stop motion animation of paper cutouts, and silhouette animation. Cel animation grew in popularity until it came to dominate the medium. In the 21st century, the use of other animation techniques is mostly limited to independent short films, including the stop motion puppet animation work produced by Tadahito Mochinaga, Kihachirō Kawamoto and Tomoyasu Murata. Computers were integrated into the animation process in the 1990s, with works such as "Ghost in the Shell" and "Princess Mononoke" mixing cel animation with computer-generated images. Fuji Film, a major cel production company, announced it would stop cel production, producing an industry panic to procure cel imports and hastening the switch to digital processes.

Prior to the digital era, anime was produced with traditional animation methods using a pose to pose approach. The majority of mainstream anime uses fewer expressive key frames and more in-between animation.

Japanese animation studios were pioneers of many limited animation techniques, and have given anime a distinct set of conventions. Unlike Disney animation, where the emphasis is on the movement, anime emphasizes the art quality and let limited animation techniques make up for the lack of time spent on movement. Such techniques are often used not only to meet deadlines but also as artistic devices. Anime scenes place emphasis on achieving three-dimensional views, and backgrounds are instrumental in creating the atmosphere of the work. The backgrounds are not always invented and are occasionally based on real locations, as exemplified in "Howl's Moving Castle" and "The Melancholy of Haruhi Suzumiya". Oppliger stated that anime is one of the rare mediums where putting together an all-star cast usually comes out looking "tremendously impressive".

The cinematic effects of anime differentiates itself from the stage plays found in American animation. Anime is cinematically shot as if by camera, including panning, zooming, distance and angle shots to more complex dynamic shots that would be difficult to produce in reality. In anime, the animation is produced before the voice acting, contrary to American animation which does the voice acting first; this can cause lip sync errors in the Japanese version.

Body proportions of human anime characters tend to accurately reflect the proportions of the human body in reality. The height of the head is considered by the artist as the base unit of proportion. Head heights can vary, but most anime characters are about seven to eight heads tall. Anime artists occasionally make deliberate modifications to body proportions to produce super deformed characters that feature a disproportionately small body compared to the head; many super deformed characters are two to four heads tall. Some anime works like "Crayon Shin-chan" completely disregard these proportions, in such a way that they resemble caricatured Western cartoons.

A common anime character design convention is exaggerated eye size. The animation of characters with large eyes in anime can be traced back to Osamu Tezuka, who was deeply influenced by such early animation characters as Betty Boop, who was drawn with disproportionately large eyes. Tezuka is a central figure in anime and manga history, whose iconic art style and character designs allowed for the entire range of human emotions to be depicted solely through the eyes. The artist adds variable color shading to the eyes and particularly to the cornea to give them greater depth. Generally, a mixture of a light shade, the tone color, and a dark shade is used. Cultural anthropologist Matt Thorn argues that Japanese animators and audiences do not perceive such stylized eyes as inherently more or less foreign. However, not all anime characters have large eyes. For example, the works of Hayao Miyazaki are known for having realistically proportioned eyes, as well as realistic hair colors on their characters.

Hair in anime is often unnaturally lively and colorful or uniquely styled. The movement of hair in anime is exaggerated and "hair action" is used to emphasize the action and emotions of characters for added visual effect. Poitras traces hairstyle color to cover illustrations on manga, where eye-catching artwork and colorful tones are attractive for children's manga. Despite being produced for a domestic market, anime features characters whose race or nationality is not always defined, and this is often a deliberate decision, such as in the "Pokémon" animated series.

Anime and manga artists often draw from a common canon of iconic facial expression illustrations to denote particular moods and thoughts. These techniques are often different in form than their counterparts in Western animation, and they include a fixed iconography that is used as shorthand for certain emotions and moods. For example, a male character may develop a nosebleed when aroused. A variety of visual symbols are employed, including sweat drops to depict nervousness, visible blushing for embarrassment, or glowing eyes for an intense glare.

The opening and credits sequences of most anime television episodes are accompanied by Japanese pop or rock songs, often by reputed bands. They may be written with the series in mind, but are also aimed at the general music market, and therefore often allude only vaguely or not at all to the themes or plot of the series. Pop and rock songs are also sometimes used as incidental music ("insert songs") in an episode, often to highlight particularly important scenes.

Anime are often classified by target demographic, including , , and a diverse range of genres targeting an adult audience. Shoujo and shounen anime sometimes contain elements popular with children of both sexes in an attempt to gain crossover appeal. Adult anime may feature a slower pace or greater plot complexity that younger audiences may typically find unappealing, as well as adult themes and situations. A subset of adult anime works featuring pornographic elements are labeled "R18" in Japan, and are internationally known as "hentai" (originating from ). By contrast, some anime subgenres incorporate "ecchi", sexual themes or undertones without depictions of sexual intercourse, as typified in the comedic or harem genres; due to its popularity among adolescent and adult anime enthusiasts, the inclusion of such elements is considered a form of fan service. Some genres explore homosexual romances, such as "yaoi" (male homosexuality) and "yuri" (female homosexuality). While often used in a pornographic context, the terms can also be used broadly in a wider context to describe or focus on the themes or the development of the relationships themselves.

Anime's genre classification differs from other types of animation and does not lend itself to simple classification. Gilles Poitras compared the labeling "Gundam 0080" and its complex depiction of war as a "giant robot" anime akin to simply labeling "War and Peace" a "war novel". Science fiction is a major anime genre and includes important historical works like Tezuka's "Astro Boy" and Yokoyama's "Tetsujin 28-go". A major subgenre of science fiction is mecha, with the "Gundam" metaseries being iconic. The diverse fantasy genre includes works based on Asian and Western traditions and folklore; examples include the Japanese feudal fairytale "InuYasha", and the depiction of Scandinavian goddesses who move to Japan to maintain a computer called Yggdrasil in "Ah! My Goddess". Genre crossing in anime is also prevalent, such as the blend of fantasy and comedy in "Dragon Half", and the incorporation of slapstick humor in the crime anime film "Castle of Cagliostro". Other subgenres found in anime include magical girl, harem, sports, martial arts, literary adaptations, medievalism, and war.

The animation industry consists of more than 430 production companies with some of the major studios including Toei Animation, Gainax, Madhouse, Gonzo, Sunrise, Bones, TMS Entertainment, Nippon Animation, P.A.Works, Studio Pierrot and Studio Ghibli. Many of the studios are organized into a trade association, The Association of Japanese Animations. There is also a labor union for workers in the industry, the Japanese Animation Creators Association. Studios will often work together to produce more complex and costly projects, as done with Studio Ghibli's "Spirited Away". An anime episode can cost between US$100,000 and US$300,000 to produce. In 2001, animation accounted for 7% of the Japanese film market, above the 4.6% market share for live-action works. The popularity and success of anime is seen through the profitability of the DVD market, contributing nearly 70% of total sales. According to a 2016 article on Nikkei Asian Review, Japanese television stations have bought over worth of anime from production companies "over the past few years", compared with under from overseas. There has been a rise in sales of shows to television stations in Japan, caused by late night anime with adults as the target demographic. This type of anime is less popular outside Japan, being considered "more of a niche product". "Spirited Away" (2001) is the all-time highest-grossing film in Japan. It was also the highest-grossing anime film worldwide until it was overtaken by Makoto Shinkai's 2016 film "Your Name". Anime films represent a large part of the highest-grossing Japanese films yearly in Japan, with 6 out of the top 10 in 2014, in 2015 and also in 2016.

Anime has to be licensed by companies in other countries in order to be legally released. While anime has been licensed by its Japanese owners for use outside Japan since at least the 1960s, the practice became well-established in the United States in the late 1970s to early 1980s, when such TV series as "Gatchaman" and "Captain Harlock" were licensed from their Japanese parent companies for distribution in the US market. The trend towards American distribution of anime continued into the 1980s with the licensing of titles such as "Voltron" and the 'creation' of new series such as "Robotech" through use of source material from several original series.

In the early 1990s, several companies began to experiment with the licensing of less children-oriented material. Some, such as A.D. Vision, and Central Park Media and its imprints, achieved fairly substantial commercial success and went on to become major players in the now very lucrative American anime market. Others, such as AnimEigo, achieved limited success. Many companies created directly by Japanese parent companies did not do as well, most releasing only one or two titles before completing their American operations.

Licenses are expensive, often hundreds of thousands of dollars for one series and tens of thousands for one movie. The prices vary widely; for example, "" cost only $91,000 to license while "Kurau Phantom Memory" cost $960,000. Simulcast Internet streaming rights can be cheaper, with prices around $1,000-$2,000 an episode, but can also be more expensive, with some series costing more than per episode.

The anime market for the United States was worth approximately $2.74 billion in 2009. Dubbed animation began airing in the United States in 2000 on networks like The WB and Cartoon Network's Adult Swim. In 2005, this resulted in five of the top ten anime titles having previously aired on Cartoon Network. As a part of localization, some editing of cultural references may occur to better follow the references of the non-Japanese culture. The cost of English localization averages US$10,000 per episode.

The industry has been subject to both praise and condemnation for fansubs, the addition of unlicensed and unauthorized subtitled translations of anime series or films. Fansubs, which were originally distributed on VHS bootlegged cassettes in the 1980s, have been freely available and disseminated online since the 1990s. Since this practice raises concerns for copyright and piracy issues, fansubbers tend to adhere to an unwritten moral code to destroy or no longer distribute an anime once an official translated or subtitled version becomes licensed. They also try to encourage viewers to buy an official copy of the release once it comes out in English, although fansubs typically continue to circulate through file sharing networks. Even so, the laid back regulations of the Japanese animation industry tends to overlook these issues, allowing it to grow underground and thus increasing the popularity until there is a demand for official high quality releases for animation companies. This has led to an increase in global popularity with Japanese animations, reaching $40 million in sales in 2004.

Legal international availability of anime on the Internet has changed in recent years, with simulcasts of series available on websites like Crunchyroll.

Japan External Trade Organization (JETRO) valued the domestic anime market in Japan at (), including from licensed products, in 2005. JETRO reported sales of overseas anime exports in 2004 to be (). JETRO valued the anime market in the United States at (), including in home video sales and over from licensed products, in 2005. JETRO projected in 2005 that the worldwide anime market, including sales of licensed products, would grow to (). The anime market in China was valued at in 2017, and is projected to reach by 2020.

The anime industry has several annual awards which honor the year's best works. Major annual awards in Japan include the Ōfuji Noburō Award, the Mainichi Film Award for Best Animation Film, the Animation Kobe Awards, the Japan Media Arts Festival animation awards, the Tokyo Anime Award and the Japan Academy Prize for Animation of the Year. In the United States, anime films compete in the Crunchyroll Anime Awards. There were also the American Anime Awards, which were designed to recognize excellence in anime titles nominated by the industry, and were held only once in 2006. Anime productions have also been nominated and won awards not exclusively for anime, like the Academy Award for Best Animated Feature or the Golden Bear.

Anime has become commercially profitable in Western countries, as demonstrated by early commercially successful Western adaptations of anime, such as "Astro Boy" and "Speed Racer". Early American adaptions in the 1960s made Japan expand into the continental European market, first with productions aimed at European and Japanese children, such as "Heidi", "Vicky the Viking" and "Barbapapa", which aired in various countries. Particularly Italy, Spain and France grew an interest into Japan's output, due to its cheap selling price and productive output. In fact, Italy imported the most anime outside of Japan. These mass imports influenced anime popularity in South American, Arabic and German markets.

The beginning of 1980 saw the introduction of Japanese anime series into the American culture. In the 1990s, Japanese animation slowly gained popularity in America. Media companies such as Viz and Mixx began publishing and releasing animation into the American market. The 1988 film "Akira" is largely credited with popularizing anime in the Western world during the early 1990s, before anime was further popularized by television shows such as "Pokémon" and "Dragon Ball Z" in the late 1990s. The growth of the Internet later provided Western audiences an easy way to access Japanese content. This is especially the case with net services such as Netflix and Crunchyroll. As a direct result, various interests surrounding Japan have increased.

Anime clubs gave rise to anime conventions in the 1990s with the "anime boom", a period marked by increased popularity of anime. These conventions are dedicated to anime and manga and include elements like cosplay contests and industry talk panels. Cosplay, a portmanteau for "costume play", is not unique to anime and has become popular in contests and masquerades at anime conventions. Japanese culture and words have entered English usage through the popularity of the medium, including "otaku", an unflattering Japanese term commonly used in English to denote a fan of anime and manga. Another word that has arisen describing fans in the United States is "wapanese" meaning White individuals who desire to be Japanese, or later known as "weeaboo" for individuals who demonstrate a strong interest in Japanese anime subculture, which is a term that originated from abusive content posted on the popular bulletin board website 4chan.org. Anime enthusiasts have produced fan fiction and fan art, including computer wallpapers and anime music videos.

As of the 2010s, many anime fans use online communities and databases such as MyAnimeList to discuss anime and track their progress watching respective series.

One of the key points that made anime different from a handful of the Western cartoons is the potential for visceral content. Once the expectation that the aspects of visual intrigue or animation being just for children is put aside, the audience can realize that themes involving violence, suffering, sexuality, pain, and death can all be storytelling elements utilized in anime as much as other types of media. However, as anime itself became increasingly popular, its styling has been inevitably the subject of both satire and serious creative productions. "South Park"s "Chinpokomon" and "Good Times with Weapons" episodes, Adult Swim's "Perfect Hair Forever", and Nickelodeon's "Kappa Mikey" are Western examples of satirical depictions of Japanese culture and anime, but anime tropes have also been satirized by some anime, such as "KonoSuba".

Traditionally only Japanese works have been considered anime, but some works have sparked debate for blurring the lines between anime and cartoons, such as the American anime style production "". These anime styled works have become defined as anime-influenced animation, in an attempt to classify all anime styled works of non-Japanese origin. Some creators of these works cite anime as a source of inspiration, for example the French production team for "Ōban Star-Racers" that moved to Tokyo to collaborate with a Japanese production team. When anime is defined as a "style" rather than as a national product it leaves open the possibility of anime being produced in other countries, but this has been contentious amongst fans, with John Oppliger stating, "The insistence on referring to original American art as Japanese "anime" or "manga" robs the work of its cultural identity."

A U.A.E.-Filipino produced TV series called "Torkaizer" is dubbed as the "Middle East's First Anime Show", and is currently in production and looking for funding. Netflix has produced multiple anime series in collaboration with Japanese animation studios, and in doing so, has offered a more accessible channel for distribution to Western markets.

The web-based series "RWBY", produced by Texas-based company Rooster Teeth, is produced using an anime art style, and the series has been described as "anime" by multiple sources. For example, "Adweek", in the headline to one of its articles, described the series as "American-made anime", and in another headline, "The Huffington Post" described it as simply "anime", without referencing its country of origin. In 2013, Monty Oum, the creator of "RWBY", said “Some believe just like Scotch needs to be made in Scotland, an American company can’t make anime. I think that’s a narrow way of seeing it. Anime is an art form, and to say only one country can make this art is wrong." "RWBY" has been released in Japan with a Japanese language dub; the CEO of Rooster Teeth, Matt Hullum, commented "This is the first time any American-made anime has been marketed to Japan. It definitely usually works the other way around, and we're really pleased about that."

In Japanese culture and entertainment, media mix is a strategy to disperse content across multiple representations: different broadcast media, gaming technologies, cell phones, toys, amusement parks, and other methods. It is the Japanese term for a transmedia franchise. The term gained its circulation in late 1980s, but the origins of the strategy can be traced back to the 1960s with the proliferation of anime, with its interconnection of media and commodity goods.

A number of anime media franchises have gained considerable global popularity, and are among the world's highest-grossing media franchises. "Pokémon" in particular is the highest-grossing media franchise of all time, bigger than "Star Wars" and "Marvel Cinematic Universe". Other anime media franchises among the world's top 15 highest-grossing media franchises include "Hello Kitty", "Gundam", and "Dragon Ball", while the top 30 also includes "Fist of the North Star", "Yu-Gi-Oh" and "Evangelion".




</doc>
<doc id="801" url="https://en.wikipedia.org/wiki?curid=801" title="Asterism">
Asterism

Asterism may refer to:




</doc>
<doc id="802" url="https://en.wikipedia.org/wiki?curid=802" title="Ankara">
Ankara

Ankara (, , ), historically known as Ancyra () and Angora (, ), is the capital of Turkey. With a population of 4,587,558 in the urban centre and 5,150,072 in its province , it is Turkey's second largest city after Istanbul (the former imperial capital), having outranked İzmir in the 20th century. Ankara covers an area of 24,521 km (9,468 sq mi).

On 23 April 1920 the Grand National Assembly of Turkey was established in Ankara, which became the headquarters of Atatürk and the Turkish National Movement during the Turkish War of Independence. Ankara became the new Turkish capital upon the establishment of the Republic on 29 October 1923, succeeding in this role the former Turkish capital Istanbul (Constantinople) following the fall of the Ottoman Empire. The government is a prominent employer, but Ankara is also an important commercial and industrial city, located at the centre of Turkey's road and railway networks. The city gave its name to the Angora wool shorn from Angora rabbits, the long-haired Angora goat (the source of mohair), and the Angora cat. The area is also known for its pears, honey and muscat grapes. Although situated in one of the driest places of Turkey and surrounded mostly by steppe vegetation except for the forested areas on the southern periphery, Ankara can be considered a green city in terms of green areas per inhabitant, at per head.

Ankara is a very old city with various Hittite, Phrygian, Hellenistic, Roman, Byzantine, and Ottoman archaeological sites. The historical centre of town is a rocky hill rising over the left bank of the Ankara Çayı, a tributary of the Sakarya River, the classical Sangarius. The hill remains crowned by the ruins of the old citadel. Although few of its outworks have survived, there are well-preserved examples of Roman and Ottoman architecture throughout the city, the most remarkable being the 20  Temple of Augustus and Rome that boasts the Monumentum Ancyranum, the inscription recording the "Res Gestae Divi Augusti".

The orthography of the name Ankara has varied over the ages. It has been identified with the Hittite cult center "Ankuwaš", although this remains a matter of debate. In classical antiquity and during the medieval period, the city was known as "Ánkyra" (,  "anchor") in Greek and "Ancyra" in Latin; the Galatian Celtic name was probably a similar variant. Following its annexation by the Seljuk Turks in 1073, the city became known in many European languages as "Angora"; it was also known in Ottoman Turkish as "Engürü". The form "Angora" is preserved in the names of breeds of many different kinds of animals, and in the names of several locations in the US (see Angora).

Ankara has a hot-summer Mediterranean climate (Köppen "Csa") which closely borders a hot summer Mediterranean continental climate (Köppen "Dsa"). Under the Trewartha climate classification, Ankara has a middle latitude steppe climate ("BSk"). Due to its elevation and inland location, Ankara has cold, somewhat snowy winters and hot, dry summers. Rainfall occurs mostly during the spring and autumn. Ankara lies in USDA Hardiness zone 7b, and its annual average precipitation is fairly low at , nevertheless precipitation can be observed throughout the year. Monthly mean temperatures range from in January to in July, with an annual mean of .

Ankara had a population of 75,000 in 1927. As of 2016, Ankara Province has a population of 5,346,518.

When Ankara became the capital of the Republic of Turkey in 1923, it was designated as a planned city for 500,000 future inhabitants. During the 1920s, 1930s and 1940s, the city grew in a planned and orderly pace. However, from the 1950s onward, the city grew much faster than envisioned, because unemployment and poverty forced people to migrate from the countryside into the city in order to seek a better standard of living. As a result, many illegal houses called gecekondu were built around the city, causing the unplanned and uncontrolled urban landscape of Ankara, as not enough planned housing could be built fast enough. Although precariously built, the vast majority of them have electricity, running water and modern household amenities.

Nevertheless, many of these gecekondus have been replaced by huge public housing projects in the form of tower blocks such as Elvankent, Eryaman and Güzelkent; and also as mass housing compounds for military and civil service accommodation. Although many gecekondus still remain, they too are gradually being replaced by mass housing compounds, as empty land plots in the city of Ankara for new construction projects are becoming impossible to find.

Çorum and Yozgat, which are located in Central Anatolia and whose population is decreasing, are the provinces with the highest net migration to Ankara. About half of the Central Anatolia population of 15,608,868 people resides in Ankara.

The population of Ankara has a higher education level than the country average. According to 2008 data, 15-years-higher literacy rate creates 88% of the total provincial population (91% in men and 86% in women). This ratio was 83% for Turkey (88% males, 79% females). This difference is particularly evident in the university educated segment of the population. The ratio of university and high school graduates to total population is 10.6% in Ankara, while 5.4% in Turkey.

The region's history can be traced back to the Bronze Age Hattic civilization, which was succeeded in the 2nd millennium BC by the Hittites, in the 10th century BC by the Phrygians, and later by the Lydians, Persians, Greeks, Galatians, Romans, Byzantines, and Turks (the Seljuk Sultanate of Rûm, the Ottoman Empire and finally republican Turkey).

The oldest settlements in and around the city center of Ankara belonged to the Hattic civilization which existed during the Bronze Age and was gradually absorbed c. 2000–1700 BC by the Indo-European Hittites. The city grew significantly in size and importance under the Phrygians starting around 1000 BC, and experienced a large expansion following the mass migration from Gordion, (the capital of Phrygia), after an earthquake which severely damaged that city around that time. In Phrygian tradition, King Midas was venerated as the founder of Ancyra, but Pausanias mentions that the city was actually far older, which accords with present archaeological knowledge.

Phrygian rule was succeeded first by Lydian and later by Persian rule, though the strongly Phrygian character of the peasantry remained, as evidenced by the gravestones of the much later Roman period. Persian sovereignty lasted until the Persians' defeat at the hands of Alexander the Great who conquered the city in 333 BC. Alexander came from Gordion to Ankara and stayed in the city for a short period. After his death at Babylon in 323 BC and the subsequent division of his empire among his generals, Ankara and its environs fell into the share of Antigonus.

Another important expansion took place under the Greeks of Pontos who came there around 300 BC and developed the city as a trading center for the commerce of goods between the Black Sea ports and Crimea to the north; Assyria, Cyprus, and Lebanon to the south; and Georgia, Armenia and Persia to the east. By that time the city also took its name Ἄγκυρα ("Ánkyra", meaning "anchor" in Greek) which, in slightly modified form, provides the modern name of "Ankara".

In 278 BC, the city, along with the rest of central Anatolia, was occupied by a Celtic group, the Galatians, who were the first to make Ankara one of their main tribal centers, the headquarters of the Tectosages tribe. Other centers were Pessinos, today's "Balhisar", for the Trocmi tribe, and Tavium, to the east of Ankara, for the "Tolstibogii" tribe. The city was then known as "Ancyra". The Celtic element was probably relatively small in numbers; a warrior aristocracy which ruled over Phrygian-speaking peasants. However, the Celtic language continued to be spoken in Galatia for many centuries. At the end of the 4th century, St. Jerome, a native of Dalmatia, observed that the language spoken around Ankara was very similar to that being spoken in the northwest of the Roman world near Trier.

The city was subsequently passed under the control of the Roman Empire. In 25 BC, Emperor Augustus raised it to the status of a "polis" and made it the capital city of the Roman province of Galatia. Ankara is famous for the "Monumentum Ancyranum" ("Temple of Augustus and Rome") which contains the official record of the "Acts of Augustus", known as the "Res Gestae Divi Augusti", an inscription cut in marble on the walls of this temple. The ruins of Ancyra still furnish today valuable bas-reliefs, inscriptions and other architectural fragments. Two other Galatian tribal centers, Tavium near Yozgat, and Pessinus (Balhisar) to the west, near Sivrihisar, continued to be reasonably important settlements in the Roman period, but it was Ancyra that grew into a grand metropolis.

An estimated 200,000 people lived in Ancyra in good times during the Roman Empire, a far greater number than was to be the case from after the fall of the Roman Empire until the early 20th century. A small river, the Ankara Çayı, ran through the center of the Roman town. It has now been covered and diverted, but it formed the northern boundary of the old town during the Roman, Byzantine and Ottoman periods. Çankaya, the rim of the majestic hill to the south of the present city center, stood well outside the Roman city, but may have been a summer resort. In the 19th century, the remains of at least one Roman villa or large house were still standing not far from where the Çankaya Presidential Residence stands today. To the west, the Roman city extended until the area of the Gençlik Park and Railway Station, while on the southern side of the hill, it may have extended downwards as far as the site presently occupied by Hacettepe University. It was thus a sizeable city by any standards and much larger than the Roman towns of Gaul or Britannia.

Ancyra's importance rested on the fact that it was the junction point where the roads in northern Anatolia running north–south and east–west intersected, giving it major strategic importance for Rome's eastern frontier. The great imperial road running east passed through Ankara and a succession of emperors and their armies came this way. They were not the only ones to use the Roman highway network, which was equally convenient for invaders. In the second half of the 3rd century, Ancyra was invaded in rapid succession by the Goths coming from the west (who rode far into the heart of Cappadocia, taking slaves and pillaging) and later by the Arabs. For about a decade, the town was one of the western outposts of one of Palmyrean empress Zenobia in the Syrian Desert, who took advantage of a period of weakness and disorder in the Roman Empire to set up a short-lived state of her own.

The town was reincorporated into the Roman Empire under Emperor Aurelian in 272. The tetrarchy, a system of multiple (up to four) emperors introduced by Diocletian (284–305), seems to have engaged in a substantial programme of rebuilding and of road construction from Ankara westwards to Germe and Dorylaeum (now Eskişehir).

In its heyday, Roman Ankara was a large market and trading center but it also functioned as a major administrative capital, where a high official ruled from the city's Praetorium, a large administrative palace or office. During the 3rd century, life in Ancyra, as in other Anatolian towns, seems to have become somewhat militarized in response to the invasions and instability of the town.

The city is well known during the 4th century as a centre of Christian activity (see also below), due to frequent imperial visits, and through the letters of the pagan scholar Libanius. Bishop Marcellus of Ancyra and Basil of Ancyra were active in the theological controversies of their day, and the city was the site of no less than three church synods in 314, 358 and 375, the latter two in favour of Arianism. The city was visited by Emperor Constans I (r. 337–350) in 347 and 350, Julian (r. 361–363) during his Persian campaign in 362, and Julian's successor Jovian (r. 363–364) in winter 363/364 (he entered his consulship while in the city). After Jovian's death soon after, Valentinian I (r. 364–375) was acclaimed emperor at Ancyra, and in the next year his brother Valens (r. 364–378) used Ancyra as his base against the usurper Procopius. When the province of Galatia was divided sometime in 396/99, Ancyra remained the civil capital of Galatia I, as well as its ecclesiastical centre (metropolitan see). Emperor Arcadius (r. 395–408) frequently used the city as his summer residence, and some information about the ecclesiastical affairs of the city during the early 5th century is found in the works of Palladius of Galatia and Nilus of Galatia.

In 479, the rebel Marcian attacked the city, without being able to capture it. In 610/11, Comentiolus, brother of Emperor Phocas (r. 602–610), launched his own unsuccessful rebellion in the city against Heraclius (r. 610–641). Ten years later, in 620 or more likely 622, it was captured by the Sassanid Persians during the Byzantine–Sassanid War of 602–628. Although the city returned to Byzantine hands after the end of the war, the Persian presence left traces in the city's archaeology, and likely began the process of its transformation from a late antique city to a medieval fortified settlement.

In 654, the city was captured for the first time by the Arabs of the Rashidun Caliphate, under Muawiyah, the future founder of the Umayyad Caliphate. At about the same time, the themes were established in Anatolia, and Ancyra became capital of the Opsician Theme, which was the largest and most important theme until it was split up under Emperor Constantine V (r. 741–775); Ancyra then became the capital of the new Bucellarian Theme. The city was attacked without success by Abbasid forces in 776 and in 798/99. In 805, Emperor Nikephoros I (r. 802–811) strengthened its fortifications, a fact which probably saved it from sack during the large-scale invasion of Anatolia by Caliph Harun al-Rashid in the next year. Arab sources report that Harun and his successor al-Ma'mun (r. 813–833) took the city, but this information is later invention. In 838, however, during the Amorium campaign, the armies of Caliph al-Mu'tasim (r. 833–842) converged and met at the city; abandoned by its inhabitants, Ancara was razed to the ground, before the Arab armies went on to besiege and destroy Amorium. In 859, Emperor Michael III (r. 842–867) came to the city during a campaign against the Arabs, and ordered its fortifications restored. In 872, the city was menaced, but not taken, by the Paulicians under Chrysocheir. The last Arab raid to reach the city was undertaken in 931, by the Abbasid governor of Tarsus, Thamal al-Dulafi, but the city again was not captured.

After the Battle of Manzikert in 1071, the Seljuk Turks overran much of Anatolia. By 1073, the Turkish settlers had reached the vicinity of Ancyra, and the city was captured shortly after, at the latest by the time of the rebellion of Nikephoros Melissenos in 1081. In 1101, when the Crusade under Raymond IV of Toulouse arrived, the city had been under Danishmend control for some time. The Crusaders captured the city, and handed it over to the Byzantine emperor Alexios I Komnenos (r. 1081–1118). Byzantine rule did not last long, and the city was captured by the Seljuk Sultanate of Rum at some unknown point; in 1127, it returned to Danishmend control until 1143, when the Seljuks of Rum retook it.

After the Battle of Köse Dağ in 1243, in which the Mongols defeated the Seljuks, most of Anatolia became part of the dominion of the Mongols. Taking advantage of Seljuk decline, a semi-religious cast of craftsmen and trade people named "Ahiler" chose Angora as their independent city-state in 1290. Orhan I, the second Bey of the Ottoman Empire, captured the city in 1356. Timur defeated Bayezid I at the Battle of Ankara in 1402 and took the city, but in 1403 Angora was again under Ottoman control.

The Levant Company maintained a factory in the town from 1639 to 1768. In the 19th century, its population was estimated at 20,000 to 60,000. It was sacked by Egyptians under Ibrahim Pasha in 1832. Prior to World War I, the town had a British consulate and a population of around 28,000, roughly of whom were Christian.

Following the Ottoman defeat in World War I, the Ottoman capital Constantinople (modern Istanbul) and much of Anatolia were occupied by the Allies, who planned to share these lands between Armenia, France, Greece, Italy and the United Kingdom, leaving for the Turks the core piece of land in central Anatolia. In response, the leader of the Turkish nationalist movement, Mustafa Kemal Atatürk, established the headquarters of his resistance movement in Angora in 1920. After the Turkish War of Independence was won and the Treaty of Sèvres was superseded by the Treaty of Lausanne (1923), the Turkish nationalists replaced the Ottoman Empire with the Republic of Turkey on 29 October 1923. A few days earlier, Angora had officially replaced Constantinople as the new Turkish capital city, on 13 October 1923, and Republican officials declared that the city's name is Ankara.

After Ankara became the capital of the newly founded Republic of Turkey, new development divided the city into an old section, called "Ulus", and a new section, called "Yenişehir". Ancient buildings reflecting Roman, Byzantine, and Ottoman history and narrow winding streets mark the old section. The new section, now centered on Kızılay Square, has the trappings of a more modern city: wide streets, hotels, theaters, shopping malls, and high-rises. Government offices and foreign embassies are also located in the new section. Ankara has experienced a phenomenal growth since it was made Turkey's capital in 1923, when it was "a small town of no importance". In 1924, the year after the government had moved there, Ankara had about 35,000 residents. By 1927 there were 44,553 residents and by 1950 the population had grown to 286,781. Ankara continued to grow rapidly during the latter half of the 20th century and eventually outranked Izmir as Turkey's second largest city, after Istanbul. Ankara's urban population reached 4,587,558 in 2014, while the population of Ankara Province reached 5,150,072 in 2015.

After 1930, it became known officially in Western languages as Ankara. After the late 1930s the public stopped using the name "Angora".

Early Christian martyrs of Ancyra, about whom little is known, included Proklos and Hilarios who were natives of the otherwise unknown nearby village of Kallippi, and suffered repression under the emperor Trajan (98–117). In the 280s we hear of Philumenos, a Christian corn merchant from southern Anatolia, being captured and martyred in Ankara, and Eustathius.
As in other Roman towns, the reign of Diocletian marked the culmination of the persecution of the Christians. In 303, Ancyra was one of the towns where the co-Emperors Diocletian and his deputy Galerius launched their anti-Christian persecution. In Ancyra, their first target was the 38-year-old Bishop of the town, whose name was Clement. Clement's life describes how he was taken to Rome, then sent back, and forced to undergo many interrogations and hardship before he, and his brother, and various companions were put to death. The remains of the church of St. Clement can be found today in a building just off Işıklar Caddesi in the Ulus district. Quite possibly this marks the site where Clement was originally buried. Four years later, a doctor of the town named Plato and his brother Antiochus also became celebrated martyrs under Galerius. Theodotus of Ancyra is also venerated as a saint.

However, the persecution proved unsuccessful and in 314 Ancyra was the center of an important council of the early church; its 25 disciplinary canons constitute one of the most important documents in the early history of the administration of the Sacrament of Penance. The synod also considered ecclesiastical policy for the reconstruction of the Christian Church after the persecutions, and in particular the treatment of "lapsi"—Christians who had given in to forced paganism (sacrifices) to avoid martyrdom during these persecutions.
Though paganism was probably tottering in Ancyra in Clement's day, it may still have been the majority religion. Twenty years later, Christianity and monotheism had taken its place. Ancyra quickly turned into a Christian city, with a life dominated by monks and priests and theological disputes. The town council or senate gave way to the bishop as the main local figurehead. During the middle of the 4th century, Ancyra was involved in the complex theological disputes over the nature of Christ, and a form of Arianism seems to have originated there.

In 362–363, the Emperor Julian passed through Ancyra on his way to an ill-fated campaign against the Persians, and according to Christian sources, engaged in a persecution of various holy men. The stone base for a statue, with an inscription describing Julian as "Lord of the whole world from the British Ocean to the barbarian nations", can still be seen, built into the eastern side of the inner circuit of the walls of Ankara Castle. The Column of Julian which was erected in honor of the emperor's visit to the city in 362 still stands today. In 375, Arian bishops met at Ancyra and deposed several bishops, among them St. Gregory of Nyssa.

In the late 4th century, Ancyra became something of an imperial holiday resort. After Constantinople became the East Roman capital, emperors in the 4th and 5th centuries would retire from the humid summer weather on the Bosporus to the drier mountain atmosphere of Ancyra. Theodosius II (408–450) kept his court in Ancyra in the summers. Laws issued in Ancyra testify to the time they spent there.

The Metropolis of Ancyra continued to be a residential see of the Eastern Orthodox Church until the 20th century, with about 40,000 faithful, mostly Turkish-speaking, but that situation ended as a result of the 1923 Convention Concerning the Exchange of Greek and Turkish Populations. The earlier Armenian Genocide put an end to the residential eparchy of Ancyra of the Armenian Catholic Church, which had been established in 1850. It is also a titular metropolis of the Ecumenical Patriarchate of Constantinople.
Both the Ancient Byzantine Metropolitan archbishopric and the 'modern' Armenian eparchy are now listed by the Catholic Church as titular sees, with separate apostolic successions.

In 1735 an Armenian Catholic diocese was established (Curiate Italian: "Ancira degli Ameni"). Having fallen into disuse, it was restored on 30 April 1850.

The Armenian Genocide brought an effective end to the residential diocese, which was only formally suppressed in 1972 and instantly transformed into an Armenian Catholic titular bishopric. The titular see has had a single occupant:

No later than 1696, the Catholic Church also established a Latin Rite titular archbishopric of Ancyra. The last incumbent died in 1976.

The Saint Clement Church is the only structure survived from the Byzantine era in Ankara. 
The church is believed to have been built between the 4th and 9th centuries.
At the time of the Ottoman Sultan Murad II, a mosque and madrasah were built on top of the church. Today, only the interior facade of a wall and marble blocks from the church have survived.

The city has exported mohair (from the Angora goat) and Angora wool (from the Angora rabbit) internationally for centuries. In the 19th century, the city also exported substantial amounts of goat and cat skins, gum, wax, honey, berries, and madder root. It was connected to Istanbul by railway before the First World War, continuing to export mohair, wool, berries, and grain.

The Central Anatolia Region is one of the primary locations of grape and wine production in Turkey, and Ankara is particularly famous for its Kalecik Karası and Muscat grapes; and its Kavaklıdere wine, which is produced in the Kavaklıdere neighbourhood within the Çankaya district of the city. Ankara is also famous for its pears. Another renowned natural product of Ankara is its indigenous type of honey ("Ankara Balı") which is known for its light color and is mostly produced by the Atatürk Forest Farm and Zoo in the Gazi district, and by other facilities in the Elmadağ, Çubuk and Beypazarı districts.

Ankara is the center of the state-owned and private Turkish defence and aerospace companies, where the industrial plants and headquarters of the Turkish Aerospace Industries, MKE, ASELSAN, Havelsan, Roketsan, FNSS, Nurol Makina, and numerous other firms are located. Exports to foreign countries from these defence and aerospace firms have steadily increased in the past decades. The IDEF in Ankara is one of the largest international expositions of the global arms industry. A number of the global automotive companies also have production facilities in Ankara, such as the German bus and truck manufacturer MAN SE. Ankara hosts the OSTIM Industrial Zone, Turkey's largest industrial park.

A large percentage of the complicated employment in Ankara is provided by the state institutions; such as the ministries, subministries, and other administrative bodies of the Turkish government. There are also many foreign citizens working as diplomats or clerks in the embassies of their respective countries.

The "Electricity, Gas, Bus General Directorate" (EGO) operates the Ankara Metro and other forms of public transportation. Ankara is currently served by a suburban rail named Ankaray (A1) and three subway lines (M1, M2, M3) of the Ankara Metro with about 300,000 total daily commuters, while an additional subway line (M4) is currently under construction. A long gondola lift with four stations connects the district of Şentepe to the Yenimahalle metro station.

The Ankara Central Station is a major rail hub in Turkey. The Turkish State Railways operates passenger train service from Ankara to other major cities, such as: Istanbul, Eskişehir, Balıkesir, Kütahya, İzmir, Kayseri, Adana, Kars, Elâzığ, Malatya, Diyarbakır, Karabük, Zonguldak and Sivas. Commuter rail also runs between the stations of Sincan and Kayaş. On 13 March 2009, the new Yüksek Hızlı Tren (YHT) high-speed rail service began operation between Ankara and Eskişehir. On 23 August 2011, another YHT high-speed line commercially started its service between Ankara and Konya. On 25 July 2014, the Ankara–Istanbul high-speed line of YHT entered service.

Esenboğa International Airport, located in the north-east of the city, is Ankara's main airport.

The average amount of time people spend commuting on public transit in Ankara on a weekday is 71 minutes. 17% of public transit passengers, ride for more than two hours every day. The average amount of time people wait at a stop or station for public transit is sixteen minutes, while 28% of users wait for over twenty minutes on average every day. The average distance people usually ride in a single trip with public transit is , while 27% travel for over in a single direction.

Ankara is politically a triple battleground between the ruling conservative Justice and Development Party (AKP), the opposition Kemalist centre-left Republican People's Party (CHP) and the nationalist far-right Nationalist Movement Party (MHP). The province of Ankara is divided into 25 districts. The CHP's key and almost only political stronghold in Ankara lies within the central area of Çankaya, which is the city's most populous district. While the CHP has always gained between 60 and 70% of the vote in Çankaya since 2002, political support elsewhere throughout Ankara is minimal. The high population within Çankaya, as well as Yenimahalle to an extent, has allowed the CHP to take overall second place behind the AKP in both local and general elections, with the MHP a close third, despite the fact that the MHP is politically stronger than the CHP in almost every other district. Overall, the AKP enjoys the most support throughout the city. The electorate of Ankara thus tend to vote in favour of the political right, far more so than the other main cities of Istanbul and İzmir. In retrospect, the 2013–14 protests against the AKP government were particularly strong in Ankara, proving to be fatal on multiple occasions.
Melih Gökçek has been the Metropolitan Mayor of Ankara since 1994 as a politician from the Welfare Party. He later joined the Virtue Party and then the AKP. Initially elected in the 1994 local elections, he was re-elected in 1999, 2004 and 2009. In the 2014 local election, Gökçek stood for a fifth term. The MHP metropolitan mayoral candidate for the 2009 local elections, conservative politician Mansur Yavaş, stood as the CHP candidate against Gökçek. In a heavily controversial election, Gökçek was declared the winner by just 1% ahead of Yavaş amid allegations of systematic electoral fraud. With the Supreme Electoral Council and courts rejecting Yavaş's appeals, he has declared intention to take the irregularities to the European Court of Human Rights. Although Gökçek was inaugurated for a fifth term, most election observers believe that Yavaş was the winner of the election.

Gökçek resigned on 28 October 2017, replaced by the former mayor of Sincan, Mustafa Tuna.

The city suffered from a series of terrorist attacks in 2015 and 2016, most notably on 10 October 2015; 17 February 2016; 13 March 2016; and 15 July 2016.

The foundations of the Ankara castle and citadel were laid by the Galatians on a prominent lava outcrop (), and the rest was completed by the Romans. The Byzantines and Seljuks further made restorations and additions. The area around and inside the citadel, being the oldest part of Ankara, contains many fine examples of traditional architecture. There are also recreational areas to relax. Many restored traditional Turkish houses inside the citadel area have found new life as restaurants, serving local cuisine.

The citadel was depicted in various Turkish banknotes during 1927–1952 and 1983–1989.

The remains, the stage, and the backstage of the Roman theatre can be seen outside the castle. Roman statues that were found here are exhibited in the Museum of Anatolian Civilizations. The seating area is still under excavation.

The Augusteum, now known as the Temple of Augustus and Rome, was built 25  20  following the conquest of Central Anatolia by the Roman Empire. Ancyra then formed the capital of the new province of Galatia. After the death of Augustus in  14, a copy of the text of the "Res Gestae Divi Augusti" (the "Monumentum Ancyranum") was inscribed on the interior of the temple's ' in Latin and a Greek translation on an exterior wall of the '. The temple on the ancient acropolis of Ancyra was enlarged in the 2nd century and converted into a church in the 5th century. It is located in the Ulus quarter of the city. It was subsequently publicized by the Austrian ambassador Ogier Ghiselin de Busbecq in the 16th century.

The Roman Baths of Ankara have all the typical features of a classical Roman bath complex: a "frigidarium" (cold room), a "tepidarium" (warm room) and a "caldarium" (hot room). The baths were built during the reign of the Roman emperor Caracalla in the early 3rd century to honor Asclepios, the God of Medicine. Today, only the basement and first floors remain. It is situated in the Ulus quarter.

The Roman Road of Ankara or "Cardo Maximus" was found in 1995 by Turkish archaeologist Cevdet Bayburtluoğlu. It is long and wide. Many ancient artifacts were discovered during the excavations along the road and most of them are currently displayed at the Museum of Anatolian Civilizations.

The Column of Julian or Julianus, now in the Ulus district, was erected in honor of the Roman emperor Julian the Apostate's visit to Ancyra in 362.

Kocatepe Mosque is the largest mosque in the city. Located in the Kocatepe quarter, it was constructed between 1967 and 1987 in classical Ottoman style with four minarets. Its size and prominent location have made it a landmark for the city.

Ahmet Hamdi Akseki Mosque is located near the Presidency of Religious Affairs on the Eskişehir Road. Built in the Turkish neoclassical style, it is one of the largest new mosques in the city, completed and opened in 2013. It can accommodate 6 thousand people during general prayers, and up to 30 thousand people during funeral prayers. The mosque was decorated with Anatolian Seljuk style patterns.

It is the largest Ottoman mosque in Ankara and was built by the famous architect Sinan in the 16th century. The mimber (pulpit) and mihrap (prayer niche) are of white marble, and the mosque itself is of Ankara stone, an example of very fine workmanship.

This mosque, in the Ulus quarter next to the Temple of Augustus, was built in the early 15th century in Seljuk style by an unknown architect. It was subsequently restored by architect Mimar Sinan in the 16th century, with Kütahya tiles being added in the 18th century. The mosque was built in honor of Hacı Bayram-ı Veli, whose tomb is next to the mosque, two years before his death (1427–28). The usable space inside this mosque is on the first floor and on the second floor.

It was founded in the Ulus quarter near the Ankara Citadel and was constructed by the Ahi fraternity during the late 14th and early 15th centuries. The finely carved walnut mimber (pulpit) is of particular interest.

The Alâeddin Mosque is the oldest mosque in Ankara. It has a carved walnut mimber, the inscription on which records that the mosque was completed in early AH 574 (which corresponds to the summer of 1178 AD) and was built by the Seljuk prince Muhiddin Mesud Şah (d. 1204), the Bey of Ankara, who was the son of the Anatolian Seljuk sultan Kılıç Arslan II (reigned 1156–1192.)

The " Victory Monument" (Turkish: "") was crafted by Austrian sculptor Heinrich Krippel in 1925 and was erected in 1927 at Ulus Square. The monument is made of marble and bronze and features an equestrian statue of Mustafa Kemal Atatürk, who wears a Republic era modern military uniform, with the rank Field Marshal.

Located at Zafer Square (Turkish: "Zafer Meydanı"), the marble and bronze statue was crafted by the renowned Italian sculptor Pietro Canonica in 1927 and depicts a standing Atatürk who wears a Republic era modern military uniform, with the rank Field Marshal.

This monument, located in Güven Park near Kızılay Square, was erected in 1935 and bears Atatürk's advice to his people: "Turk! Be proud, work hard, and believe in yourself."

The monument was depicted on the reverse of the Turkish 5 lira banknote of 1937–1952 and of the 1000 lira banknotes of 1939–1946.

Erected in 1978 at Sıhhiye Square, this impressive monument symbolizes the Hatti Sun Disc (which was later adopted by the Hittites) and commemorates Anatolia's earliest known civilization. The Hatti Sun Disc has been used in the previous logo of Ankara Metropolitan Municipality. It was also used in the previous logo of the Ministry of Culture & Tourism.

Suluhan is a historical Inn in Ankara. It is also called the "Hasanpaşa Han". It is about southeast of Ulus Square and situated in the Hacıdoğan neighbourhood. According to the "vakfiye" (inscription) of the building, the Ottoman era "han" was commissioned by Hasan Pasha, a regional beylerbey, and was constructed between 1508 and 1511, during the final years of the reign of Sultan Bayezid II.
There are 102 rooms (now shops) which face the two yards. In each room there is a window, a niche and a chimney.

Çengelhan Rahmi Koç Museum is a museum of industrial technology situated in , an Ottoman era Inn which was completed in 1523, during the early years of the reign of Sultan Suleiman the Magnificent. The exhibits include industrial/technological artifacts from the 1850s onwards. There are also sections about Mustafa Kemal Atatürk, the founder of modern Turkey; Vehbi Koç, Rahmi Koç's father and one of the first industrialists of Turkey, and Ankara city.

Foreign visitors to Ankara usually like to visit the old shops in "Çıkrıkçılar Yokuşu" (Weavers' Road) near Ulus, where myriad things ranging from traditional fabrics, hand-woven carpets and leather products can be found at bargain prices. "Bakırcılar Çarşısı" (Bazaar of Coppersmiths) is particularly popular, and many interesting items, not just of copper, can be found here like jewelry, carpets, costumes, antiques and embroidery. Up the hill to the castle gate, there are many shops selling a huge and fresh collection of spices, dried fruits, nuts, and other produce.

Modern shopping areas are mostly found in Kızılay, or on Tunalı Hilmi Avenue, including the modern mall of Karum (named after the ancient Assyrian merchant colonies called "Kârum" that were established in central Anatolia at the beginning of the 2nd millennium BC) which is located towards the end of the Avenue; and in Çankaya, the quarter with the highest elevation in the city. Atakule Tower next to Atrium Mall in Çankaya has views over Ankara and also has a revolving restaurant at the top. The symbol of the Armada Shopping Mall is an anchor, and there's a large anchor monument at its entrance, as a reference to the ancient Greek name of the city, Ἄγκυρα (Ánkyra), which means anchor. Likewise, the anchor monument is also related with the Spanish name of the mall, Armada, which means naval fleet.

As Ankara started expanding westward in the 1970s, several modern, suburbia-style developments and mini-cities began to rise along the western highway, also known as the Eskişehir Road. The "Armada", "CEPA" and "Kentpark" malls on the highway, the "Galleria", "Arcadium" and "Gordion" in Ümitköy, and a huge mall, "Real" in Bilkent Center, offer North American and European style shopping opportunities (these places can be reached through the Eskişehir Highway.) There is also the newly expanded "ANKAmall" at the outskirts, on the Istanbul Highway, which houses most of the well-known international brands. This mall is the largest throughout the Ankara region. In 2014 a few more shopping malls were open in Ankara. They are "Next Level" and "Taurus" on the Boulevard of Mevlana (also known as Konya Road).

Turkish State Opera and Ballet, the national directorate of opera and ballet companies of Turkey, has its headquarters in Ankara, and serves the city with three venues:


Ankara is host to five classical music orchestras:

There are four concert halls in the city:

The city has been host to several well-established, annual theatre, music, film festivals:


Ankara also has a number of concert venues such as "Eskiyeni", "IF Performance Hall", "Jolly Joker", "Kite", "Nefes Bar", "Noxus Pub", "Passage Pub" and "Route", which host the live performances and events of popular musicians.

The Turkish State Theatres also has its head office in Ankara and runs the following stages in the city:

In addition, the city is served by several private theatre companies, among which , who have their own stage in the city center, is a notable example.

There are about 50 museums in the city.

The Museum of Anatolian Civilizations ("Anadolu Medeniyetleri Müzesi") is situated at the entrance of the Ankara Castle. It is an old 15th century bedesten (covered bazaar) that has been restored and now houses a collection of Paleolithic, Neolithic, Hatti, Hittite, Phrygian, Urartian and Roman works as well as a major section dedicated to Lydian treasures.

Anıtkabir is located on an imposing hill, which forms the "Anıttepe" quarter of the city, where the mausoleum of Mustafa Kemal Atatürk, founder of the Republic of Turkey, stands. Completed in 1953, it is an impressive fusion of ancient and modern architectural styles. An adjacent museum houses a wax statue of Atatürk, his writings, letters and personal items, as well as an exhibition of photographs recording important moments in his life and during the establishment of the Republic. Anıtkabir is open every day, while the adjacent museum is open every day except Mondays.

Ankara Ethnography Museum ("Etnoğrafya Müzesi") is located opposite to the Ankara Opera House on Talat Paşa Boulevard, in the Ulus district. There is a fine collection of folkloric items, as well as artifacts from the Seljuk and Ottoman periods. In front of the museum building, there is a marble and bronze equestrian statue of Mustafa Kemal Atatürk (who wears a Republic era modern military uniform, with the rank Field Marshal) which was crafted in 1927 by the renowned Italian sculptor Pietro Canonica.

The State Art and Sculpture Museum ("Resim-Heykel Müzesi") which opened to the public in 1980 is close to the Ethnography Museum and houses a rich collection of Turkish art from the late 19th century to the present day. There are also galleries which host guest exhibitions.

Cer Modern is the modern-arts museum of Ankara, inaugurated on 1 April 2010. It is situated in the renovated building of the historic TCDD Cer Atölyeleri, formerly a workshop of the Turkish State Railways. The museum incorporates the largest exhibition hall in Turkey. The museum holds periodic exhibitions of modern and contemporary art as well as hosting other contemporary arts events.

The War of Independence Museum ("Kurtuluş Savaşı Müzesi") is located on Ulus Square. It was originally the first Parliament building (TBMM) of the Republic of Turkey. The War of Independence was planned and directed here as recorded in various photographs and items presently on exhibition. In another display, wax figures of former presidents of the Republic of Turkey are on exhibit.

The Mehmet Akif Literature Museum Library is an important literary museum and archive opened in 2011 and dedicated to Mehmet Akif Ersoy (1873–1936), the poet of the Turkish National Anthem.

The TCDD Open Air Steam Locomotive Museum is an open-air museum which traces the history of steam locomotives.

Ankara Aviation Museum ("Hava Kuvvetleri Müzesi Komutanlığı") is located near the Istanbul Road in Etimesgut. The museum opened to the public in September 1998. It is home to various missiles, avionics, aviation materials and aircraft that have served in the Turkish Air Force (e.g. combat aircraft such as the F-86 Sabre, F-100 Super Sabre, F-102 Delta Dagger, F-104 Starfighter, F-5 Freedom Fighter, F-4 Phantom; and cargo planes such as the Transall C-160.) Also a Hungarian MiG-21, a Pakistani MiG-19, and a Bulgarian MiG-17 are on display at the museum.

The METU Science and Technology Museum ("ODTÜ Bilim ve Teknoloji Müzesi") is located inside the Middle East Technical University campus.

As with all other cities of Turkey, football is the most popular sport in Ankara. The city has two football clubs currently competing in the Turkish Super League: Ankaragücü, founded in 1910, is the oldest club in Ankara and is associated with Ankara's military arsenal manufacturing company MKE. They were the Turkish Cup winners in 1972 and 1981. Gençlerbirliği, founded in 1923, are known as the "Ankara Gale" or the "Poppies" because of their colors: red and black. They were the Turkish Cup winners in 1987 and 2001. Gençlerbirliği's B team, Hacettepe S.K. (formerly known as Gençlerbirliği OFTAŞ) played in the Turkish Super League but currently plays in the TFF Second League. A fourth team, Büyükşehir Belediye Ankaraspor, played in the Turkish Super League until 2010, when they were expelled. The club was reconstituted in 2014 as Osmanlıspor and currently play in the TFF First League at the Osmanlı Stadium in the Sincan district of Yenikent, outside the city center. Keçiörengücü were promoted to the TFF First League for the 2019-20 season.

Ankara has a large number of minor teams, playing at regional levels. In the TFF Second League: BAKspor in Sincan, Ankara Demirspor in Çankaya, Etimesgut Belediyespor in Etimesgut; in the TFF Third League: Çankaya FK in Keçiören; Altındağ Belediyespor in Altındağ; in the Amateur League: Turanspor in Etimesgut, Türk Telekomspor owned by the phone company in Yenimahalle, Çubukspor in Çubuk, and Bağlumspor in Keçiören.

In the Turkish Basketball League, Ankara is represented by Türk Telekom, whose home is the Ankara Arena, and CASA TED Kolejliler, whose home is the TOBB Sports Hall.

Halkbank Ankara is currently the leading domestic powerhouse in Men's Volleyball, having won many championships and cups in the Turkish Men's Volleyball League and even the CEV Cup in 2013.

Ankara Buz Pateni Sarayı is where the ice skating and ice hockey competitions take place in the city.

There are many popular spots for skateboarding which is active in the city since the 1980s. Skaters in Ankara usually meet in the park near the Grand National Assembly of Turkey.

The 2012-built THF Sport Hall hosts the Handball Super League and Women's Handball Super League matches scheduled in Ankara.

Ankara has many parks and open spaces mainly established in the early years of the Republic and well maintained and expanded thereafter. The most important of these parks are: Gençlik Parkı (houses an amusement park with a large pond for rowing), the Botanical garden, Seğmenler Park, Anayasa Park, Kuğulu Park (famous for the swans received as a gift from the Chinese government), Abdi İpekçi Park, Esertepe Parkı, Güven Park (see above for the monument), Kurtuluş Park (has an ice-skating rink), Altınpark (also a prominent exposition/fair area), Harikalar Diyarı (claimed to be Biggest Park of Europe inside city borders) and Göksu Park.

Gençlik Park was depicted on the reverse of the Turkish 100 lira banknotes of 1952–1976.

Atatürk Forest Farm and Zoo ("Atatürk Orman Çiftliği") is an expansive recreational farming area which houses a zoo, several small agricultural farms, greenhouses, restaurants, a dairy farm and a brewery. It is a pleasant place to spend a day with family, be it for having picnics, hiking, biking or simply enjoying good food and nature. There is also an exact replica of the house where Atatürk was born in 1881, in Thessaloniki, Greece. Visitors to the "Çiftlik" (farm) as it is affectionately called by Ankarans, can sample such famous products of the farm such as old-fashioned beer and ice cream, fresh dairy products and meat rolls/kebaps made on charcoal, at a traditional restaurant ("Merkez Lokantası", Central Restaurant), cafés and other establishments scattered around the farm.

Ankara is noted, within Turkey, for the multitude of universities it is home to. These include the following, several of them being among the most reputable in the country:

Ankara is home to a world-famous domestic cat breed – the Turkish Angora, called "Ankara kedisi" (Ankara cat) in Turkish. Turkish Angoras are one of the ancient, naturally occurring cat breeds, having originated in Ankara and its surrounding region in central Anatolia.

They mostly have a white, silky, medium to long length coat, no undercoat and a fine bone structure. There seems to be a connection between the Angora Cats and Persians, and the Turkish Angora is also a distant cousin of the Turkish Van. Although they are known for their shimmery white coat, currently there are more than twenty varieties including black, blue and reddish fur. They come in tabby and tabby-white, along with smoke varieties, and are in every color other than pointed, lavender, and cinnamon (all of which would indicate breeding to an outcross.)

Eyes may be blue, green, or amber, or even one blue and one amber or green. The W gene which is responsible for the white coat and blue eye is closely related to the hearing ability, and the presence of a blue eye can indicate that the cat is deaf to the side the blue eye is located. However, a great many blue and odd-eyed white cats have normal hearing, and even deaf cats lead a very normal life if kept indoors.

Ears are pointed and large, eyes are almond shaped and the head is massive with a two plane profile. Another characteristic is the tail, which is often kept parallel to the back.

The Angora rabbit () is a variety of domestic rabbit bred for its long, soft hair. The Angora is one of the oldest types of domestic rabbit, originating in Ankara and its surrounding region in central Anatolia, along with the Angora cat and Angora goat. The rabbits were popular pets with French royalty in the mid-18th century, and spread to other parts of Europe by the end of the century. They first appeared in the United States in the early 20th century. They are bred largely for their long Angora wool, which may be removed by shearing, combing, or plucking (gently pulling loose wool.)

Angoras are bred mainly for their wool because it is silky and soft. They have a humorous appearance, as they oddly resemble a fur ball. Most are calm and docile but should be handled carefully. Grooming is necessary to prevent the fiber from matting and felting on the rabbit. A condition called "wool block" is common in Angora rabbits and should be treated quickly. Sometimes they are shorn in the summer as the long fur can cause the rabbits to overheat.

The Angora goat () is a breed of domestic goat that originated in Ankara and its surrounding region in central Anatolia.

This breed was first mentioned in the time of Moses, roughly in 1500 BC. The first Angora goats were brought to Europe by Charles V, Holy Roman Emperor, about 1554, but, like later imports, were not very successful. Angora goats were first introduced in the United States in 1849 by Dr. James P. Davis. Seven adult goats were a gift from Sultan Abdülmecid I in appreciation for his services and advice on the raising of cotton.

The fleece taken from an Angora goat is called mohair. A single goat produces between of hair per year. Angoras are shorn twice a year, unlike sheep, which are shorn only once. Angoras have high nutritional requirements due to their rapid hair growth. A poor quality diet will curtail mohair development. The United States, Turkey, and South Africa are the top producers of mohair.

For a long period of time, Angora goats were bred for their white coat. In 1998, the Colored Angora Goat Breeders Association was set up to promote breeding of colored Angoras. Today, Angora goats produce white, black (deep black to greys and silver), red (the color fades significantly as the goat gets older), and brownish fiber.

Angora goats were depicted on the reverse of the Turkish 50 lira banknotes of 1938–1952.

Ankara is twinned with:

43. ilişki durumu evli izle

Attribution



</doc>
<doc id="803" url="https://en.wikipedia.org/wiki?curid=803" title="Arabic">
Arabic

Arabic (, ', or , ', or ) is a Semitic language that first emerged in the 1st to 4th centuries CE. It is now the of the Arab world. It is named after the Arabs, a term initially used to describe peoples living in the area bounded by Mesopotamia in the east and the Anti-Lebanon mountains in the west, in Northwestern Arabia and in the Sinai Peninsula. The ISO assigns language codes to thirty varieties of Arabic, including its standard form, Modern Standard Arabic, also referred to as Literary Arabic, which is modernized Classical Arabic. This distinction exists primarily among Western linguists; Arabic speakers themselves generally do not distinguish between Modern Standard Arabic and Classical Arabic, but rather refer to both as (, "the purest Arabic") or simply " ().

Arabic is widely taught in schools and universities and is used to varying degrees in workplaces, government and the media. Arabic, in its standard form, is the official language of 26 states, as well as the liturgical language of the religion of Islam, since the Quran and Hadith were written in Arabic. 

During the Middle Ages, Arabic was a major vehicle of culture in Europe, especially in science, mathematics and philosophy. As a result, many European languages have also borrowed many words from it. Arabic influence, mainly in vocabulary, is seen in European languages—mainly Spanish and to a lesser extent Portuguese and Catalan—owing to both the proximity of Christian European and Muslim Arab civilizations and almost 800 years of Arabic culture and language presence in the Iberian Peninsula, referred to in Arabic as ". Sicilian has about 500 Arabic words, many of which relate to agriculture and related activities, as a legacy of the Emirate of Sicily from the mid-9th to mid-10th centuries, while Maltese language is a Semitic language developed from a dialect of Arabic and written in the Latin alphabet. The Balkan languages, including Greek and Bulgarian, have also acquired a significant number of Arabic words through contact with Ottoman Turkish.

Arabic has influenced many other languages around the globe throughout its history. Some of the most influenced languages are Persian, Turkish, Azeri, Armenian, Hindustani (Hindi and Urdu), Kashmiri, Kurdish, Bosnian, Kazakh, Bengali, Malay (Indonesian and Malaysian), Maldivian, Pashto, Punjabi, Spanish, Tagalog, Assamese, Sindhi, Odia and Hausa and some languages in parts of Africa. Conversely, Arabic has borrowed words from other languages, including Hebrew, Greek, Aramaic, and Persian in medieval times and languages such as English and French in modern times.

Arabic is the liturgical language of 1.8 billion Muslims and Arabic is one of six official languages of the United Nations. All varieties of Arabic combined are spoken by perhaps as many as 422 million speakers (native and non-native) in the Arab world, making it the fifth most spoken language in the world. Arabic is written with the Arabic alphabet, which is an abjad script and is written from right to left, although the spoken varieties are sometimes written in ASCII Latin from left to right with no standardized orthography.

Arabic is usually, but not universally, classified as a Central Semitic language. It is related to languages in other subgroups of the Semitic language group (Northwest Semitic, South Semitic, East Semitic, West Semitic), such as Aramaic, Syriac, Hebrew, Ugaritic, Phoenician, Canaanite, Amorite, Ammonite, Eblaite, epigraphic Ancient North Arabian, epigrahic Ancient South Arabian, Ethiopic, Modern South Arabian, and numerous other dead and modern languages. Linguists still differ as to the best classification of Semitic language sub-groups.
The Semitic languages changed a great deal between Proto-Semitic and the emergence of the Central Semitic languages, particularly in grammar. Innovations of the Central Semitic languages—all maintained in Arabic—include:
There are several features which Classical Arabic, the modern Arabic varieties, as well as the Safaitic and Hismaic inscriptions share which are unattested in any other Central Semitic language variety, including the Dadanitic and Taymanitic languages of the northern Hejaz. These features are evidence of common descent from a hypothetical ancestor, Proto-Arabic. The following features can be reconstructed with confidence for Proto-Arabic:

Arabia boasted a wide variety of Semitic languages in antiquity. In the southwest, various Central Semitic languages both belonging to and outside of the Ancient South Arabian family (e.g. Southern Thamudic) were spoken. It is also believed that the ancestors of the Modern South Arabian languages (non-Central Semitic languages) were also spoken in southern Arabia at this time. To the north, in the oases of northern Hejaz, Dadanitic and Taymanitic held some prestige as inscriptional languages. In Najd and parts of western Arabia, a language known to scholars as Thamudic C is attested. In eastern Arabia, inscriptions in a script derived from ASA attest to a language known as Hasaitic. Finally, on the northwestern frontier of Arabia, various languages known to scholars as Thamudic B, Thamudic D, Safaitic, and Hismaic are attested. The last two share important isoglosses with later forms of Arabic, leading scholars to theorize that Safaitic and Hismaic are in fact early forms of Arabic and that they should be considered Old Arabic.

Linguists generally believe that "Old Arabic" (a collection of related dialects that constitute the precursor of Arabic) first emerged around the 1st century CE. Previously, the earliest attestation of Old Arabic was thought to be a single 1st century CE inscription in Sabaic script at Qaryat Al-Faw, in southern present-day Saudi Arabia. However, this inscription does not participate in several of the key innovations of the Arabic language group, such as the conversion of Semitic mimation to nunation in the singular. It is best reassessed as a separate language on the Central Semitic dialect continuum.

It was also thought that Old Arabic coexisted alongside—and then gradually displaced--epigraphic Ancient North Arabian (ANA), which was theorized to have been the regional tongue for many centuries. ANA, despite its name, was considered a very distinct language, and mutually unintelligible, from "Arabic". Scholars named its variant dialects after the towns where the inscriptions were discovered (Dadanitic, Taymanitic, Hismaic, Safaitic). However, most arguments for a single ANA language or language family were based on the shape of the definite article, a prefixed h-. It has been argued that the h- is an archaism and not a shared innovation, and thus unsuitable for language classification, rendering the hypothesis of an ANA language family untenable. Safaitic and Hismaic, previously considered ANA, should be considered Old Arabic due to the fact that they participate in the innovations common to all forms of Arabic.

The earliest attestation of continuous Arabic text in an ancestor of the modern Arabic script are three lines of poetry by a man named Garm(')allāhe found in En Avdat, Israel, and dated to around 125 CE. This is followed by the epitaph of the Lakhmid king Mar 'al-Qays bar 'Amro, dating to 328 CE, found at Namaraa, Syria. From the 4th to the 6th centuries, the Nabataean script evolves into the Arabic script recognizable from the early Islamic era. There are inscriptions in an undotted, 17-letter Arabic script dating to the 6th century CE, found at four locations in Syria (Zabad, Jabal ‘Usays, Harran, Umm al-Jimaal). The oldest surviving papyrus in Arabic dates to 643 CE, and it uses dots to produce the modern 28-letter Arabic alphabet. The language of that papyrus and of the Qur'an are referred to by linguists as "Quranic Arabic", as distinct from its codification soon thereafter into "Classical Arabic".

In late pre-Islamic times, a transdialectal and transcommunal variety of Arabic emerged in the Hejaz which continued living its parallel life after literary Arabic had been institutionally standardized in the 2nd and 3rd century of the Hijra, most strongly in Judeo-Christian texts, keeping alive ancient features eliminated from the "learned" tradition (Classical Arabic). This variety and both its classicizing and "lay" iterations have been termed Middle Arabic in the past, but they are thought to continue an Old Higazi register. It is clear that the orthography of the Qur'an was not developed for the standardized form of Classical Arabic; rather, it shows the attempt on the part of writers to record an archaic form of Old Higazi.

In the late 6th century AD, a relatively uniform intertribal "poetic koine" distinct from the spoken vernaculars developed based on the Bedouin dialects of Najd, probably in connection with the court of al-Ḥīra. During the first Islamic century, the majority of Arabic poets and Arabic-writing persons spoke Arabic as their mother tongue. Their texts, although mainly preserved in far later manuscripts, contain traces of non-standardized Classical Arabic elements in morphology and syntax. The standardization of Classical Arabic reached completion around the end of the 8th century. The first comprehensive description of the "ʿarabiyya" "Arabic", Sībawayhi's "al"-"Kitāb", is based first of all upon a corpus of poetic texts, in addition to Qur'an usage and Bedouin informants whom he considered to be reliable speakers of the "ʿarabiyya". By the 8th century, knowledge of Classical Arabic had become an essential prerequisite for rising into the higher classes throughout the Islamic world.

Charles Ferguson's koine theory (Ferguson 1959) claims that the modern Arabic dialects collectively descend from a single military koine that sprang up during the Islamic conquests; this view has been challenged in recent times. Ahmad al-Jallad proposes that there were at least two considerably distinct types of Arabic on the eve of the conquests: Northern and Central (Al-Jallad 2009). The modern dialects emerged from a new contact situation produced following the conquests. Instead of the emergence of a single or multiple koines, the dialects contain several sedimentary layers of borrowed and areal features, which they absorbed at different points in their linguistic histories.
According to Veersteegh and Bickerton, colloquial Arabic dialects arose from pidginized Arabic formed from contact between Arabs and conquered peoples. Pidginization and subsequent creolization among Arabs and arabized peoples could explain relative morphological and phonological simplicity of vernacular Arabic compared to Classical and MSA.

"Arabic" usually designates one of three main variants: Classical Arabic, Modern Standard Arabic and "colloquial" or "dialectal" Arabic. Classical Arabic is the language found in the Quran, used from the period of Pre-Islamic Arabia to that of the Abbasid Caliphate. Theoretically, Classical Arabic is considered normative, according to the syntactic and grammatical norms laid down by classical grammarians (such as Sibawayh) and the vocabulary defined in classical dictionaries (such as the "Lisān al-ʻArab"). In practice, however, modern authors almost never write in pure Classical Arabic, instead using a literary language with its own grammatical norms and vocabulary, commonly known as Modern Standard Arabic (MSA).

Modern Standard Arabic largely follows the grammatical standards of Classical Arabic and uses much of the same vocabulary. However, it has discarded some grammatical constructions and vocabulary that no longer have any counterpart in the spoken varieties and has adopted certain new constructions and vocabulary from the spoken varieties. Much of the new vocabulary is used to denote concepts that have arisen in the industrial and post-industrial era, especially in modern times. Due to its grounding in Classical Arabic, Modern Standard Arabic is removed over a millennium from everyday speech, which is construed as a multitude of dialects of this language. These dialects and Modern Standard Arabic are described by some scholars as not mutually comprehensible. The former are usually acquired in families, while the latter is taught in formal education settings. However, there have been studies reporting some degree of comprehension of stories told in the standard variety among preschool-aged children. The relation between Modern Standard Arabic and these dialects is sometimes compared to that of Classical Latin and Vulgar Latin vernaculars (which became Romance languages) in medieval and early modern Europe. This view though does not take into account the widespread use of Modern Standard Arabic as a medium of audiovisual communication in today's mass media—a function Latin has never performed.

MSA is the variety used in most current, printed Arabic publications, spoken by some of the Arabic media across North Africa and the Middle East, and understood by most educated Arabic speakers. "Literary Arabic" and "Standard Arabic" ( ") are less strictly defined terms that may refer to Modern Standard Arabic or Classical Arabic.

Some of the differences between Classical Arabic (CA) and Modern Standard Arabic (MSA) are as follows:

MSA uses much Classical vocabulary (e.g., ' 'to go') that is not present in the spoken varieties, but deletes Classical words that sound obsolete in MSA. In addition, MSA has borrowed or coined many terms for concepts that did not exist in Quranic times, and MSA continues to evolve. Some words have been borrowed from other languages—notice that transliteration mainly indicates spelling and not real pronunciation (e.g., ' 'film' or " 'democracy').

However, the current preference is to avoid direct borrowings, preferring to either use loan translations (e.g., ' 'branch', also used for the branch of a company or organization; ' 'wing', is also used for the wing of an airplane, building, air force, etc.), or to coin new words using forms within existing roots ( ' 'apoptosis', using the root "m/w/t" 'death' put into the Xth form, or ' 'university', based on ' 'to gather, unite'; ' 'republic', based on ' 'multitude'). An earlier tendency was to redefine an older word although this has fallen into disuse (e.g., ' 'telephone' < 'invisible caller (in Sufism)'; "" 'newspaper' < 'palm-leaf stalk').

"Colloquial" or "dialectal" Arabic refers to the many national or regional varieties which constitute the everyday spoken language and evolved from Classical Arabic. Colloquial Arabic has many regional variants; geographically distant varieties usually differ enough to be mutually unintelligible, and some linguists consider them distinct languages. The varieties are typically unwritten. They are often used in informal spoken media, such as soap operas and talk shows, as well as occasionally in certain forms of written media such as poetry and printed advertising.

The only variety of modern Arabic to have acquired official language status is Maltese, which is spoken in (predominantly Catholic) Malta and written with the Latin script. It is descended from Classical Arabic through Siculo-Arabic, but is not mutually intelligible with any other variety of Arabic. Most linguists list it as a separate language rather than as a dialect of Arabic.

Even during Muhammad's lifetime, there were dialects of spoken Arabic. Muhammad spoke in the dialect of Mecca, in the western Arabian peninsula, and it was in this dialect that the Quran was written down. However, the dialects of the eastern Arabian peninsula were considered the most prestigious at the time, so the language of the Quran was ultimately converted to follow the eastern phonology. It is this phonology that underlies the modern pronunciation of Classical Arabic. The phonological differences between these two dialects account for some of the complexities of Arabic writing, most notably the writing of the glottal stop or "hamzah" (which was preserved in the eastern dialects but lost in western speech) and the use of ' (representing a sound preserved in the western dialects but merged with ' in eastern speech).

The sociolinguistic situation of Arabic in modern times provides a prime example of the linguistic phenomenon of diglossia, which is the normal use of two separate varieties of the same language, usually in different social situations. "Tawleed" is the process of giving a new shade of meaning to an old classical word. For example, "al-hatif" lexicographically, means the one whose sound is heard but whose person remains unseen. Now the term "al-hatif" is used for a telephone. Therefore, the process of "tawleed" can express the needs of modern civilization in a manner that would appear to be originally Arabic. In the case of Arabic, educated Arabs of any nationality can be assumed to speak both their school-taught Standard Arabic as well as their native, mutually unintelligible "dialects"; these dialects linguistically constitute separate languages which may have dialects of their own. When educated Arabs of different dialects engage in conversation (for example, a Moroccan speaking with a Lebanese), many speakers code-switch back and forth between the dialectal and standard varieties of the language, sometimes even within the same sentence. Arabic speakers often improve their familiarity with other dialects via music or film.

The issue of whether Arabic is one language or many languages is politically charged, in the same way it is for the varieties of Chinese, Hindi and Urdu, Serbian and Croatian, Scots and English, etc. In contrast to speakers of Hindi and Urdu who claim they cannot understand each other even when they can, speakers of the varieties of Arabic will claim they can all understand each other even when they cannot. The issue of diglossia between spoken and written language is a significant complicating factor: A single written form, significantly different from any of the spoken varieties learned natively, unites a number of sometimes divergent spoken forms. For political reasons, Arabs mostly assert that they all speak a single language, despite significant issues of mutual incomprehensibility among differing spoken versions.

From a linguistic standpoint, it is often said that the various spoken varieties of Arabic differ among each other collectively about as much as the Romance languages. This is an apt comparison in a number of ways. The period of divergence from a single spoken form is similar—perhaps 1500 years for Arabic, 2000 years for the Romance languages. Also, while it is comprehensible to people from the Maghreb, a linguistically innovative variety such as Moroccan Arabic is essentially incomprehensible to Arabs from the Mashriq, much as French is incomprehensible to Spanish or Italian speakers but relatively easily learned by them. This suggests that the spoken varieties may linguistically be considered separate languages.

The influence of Arabic has been most important in Islamic countries, because it is the language of the Islamic sacred book, the Quran. Arabic is also an important source of vocabulary for languages such as Amharic, Baluchi, Bengali, Berber, Bosnian, Chaldean, Chechen, Croatian, Dagestani, English, German, Gujarati, Hausa, Hindi, Kazakh, Kurdish, Kutchi, Kyrgyz, Malay (Malaysian and Indonesian), Pashto, Persian, Punjabi, Rohingya, Romance languages (French, Catalan, Italian, Portuguese, Sicilian, Spanish, etc.) Saraiki, Sindhi, Somali, Sylheti, Swahili, Tagalog, Tigrinya, Turkish, Turkmen, Urdu, Uyghur, Uzbek, Visayan and Wolof, as well as other languages in countries where these languages are spoken. The Education Minister of France has recently been emphasizing the learning and usage of Arabic in their schools.

In addition, English has many Arabic loanwords, some directly, but most via other Mediterranean languages. Examples of such words include admiral, adobe, alchemy, alcohol, algebra, algorithm, alkaline, almanac, amber, arsenal, assassin, candy, carat, cipher, coffee, cotton, ghoul, hazard, jar, kismet, lemon, loofah, magazine, mattress, sherbet, sofa, sumac, tariff, and zenith. Other languages such as Maltese and Kinubi derive ultimately from Arabic, rather than merely borrowing vocabulary or grammatical rules.

Terms borrowed range from religious terminology (like Berber "taẓallit", "prayer", from "salat" ( "")), academic terms (like Uyghur "mentiq", "logic"), and economic items (like English "coffee") to placeholders (like Spanish "fulano", "so-and-so"), everyday terms (like Hindustani "lekin", "but", or Spanish "taza" and French "tasse", meaning "cup"), and expressions (like Catalan "a betzef", "galore, in quantity"). Most Berber varieties (such as Kabyle), along with Swahili, borrow some numbers from Arabic. Most Islamic religious terms are direct borrowings from Arabic, such as ("salat"), "prayer", and ("imam"), "prayer leader."

In languages not directly in contact with the Arab world, Arabic loanwords are often transferred indirectly via other languages rather than being transferred directly from Arabic. For example, most Arabic loanwords in Hindustani and Turkish entered though Persian is an Indo-Iranian language. Older Arabic loanwords in Hausa were borrowed from Kanuri.

Arabic words also made their way into several West African languages as Islam spread across the Sahara. Variants of Arabic words such as "kitāb" ("book") have spread to the languages of African groups who had no direct contact with Arab traders.

Since throughout the Islamic world, Arabic occupied a position similar to that of Latin in Europe, many of the Arabic concepts in the fields of science, philosophy, commerce, etc. were coined from Arabic roots by non-native Arabic speakers, notably by Aramaic and Persian translators, and then found their way into other languages. This process of using Arabic roots, especially in Kurdish and Persian, to translate foreign concepts continued through to the 18th and 19th centuries, when swaths of Arab-inhabited lands were under Ottoman rule.

The most important sources of borrowings into (pre-Islamic) Arabic are from the related (Semitic) languages Aramaic, which used to be the principal, international language of communication throughout the ancient Near and Middle East, Ethiopic, and to a lesser degree Hebrew (mainly religious concepts). In addition, many cultural, religious and political terms have entered Arabic from Iranian languages, notably Middle Persian, Parthian, and (Classical) Persian, and Hellenistic Greek ("kīmiyāʼ" has as origin the Greek "khymia", meaning in that language the melting of metals; see Roger Dachez, "Histoire de la Médecine de l'Antiquité au XXe siècle", Tallandier, 2008, p. 251), "alembic" (distiller) from "ambix" (cup), "almanac" (climate) from "almenichiakon" (calendar). (For the origin of the last three borrowed words, see Alfred-Louis de Prémare, "Foundations of Islam", Seuil, L'Univers Historique, 2002.) Some Arabic borrowings from Semitic or Persian languages are, as presented in De Prémare's above-cited book:

There have been many instances of national movements to convert Arabic script into Latin script or to Romanize the language. Currently, the only language derived from Classical Arabic to use Latin script is Maltese.

The Beirut newspaper "La Syrie" pushed for the change from Arabic script to Latin letters in 1922. The major head of this movement was Louis Massignon, a French Orientalist, who brought his concern before the Arabic Language Academy in Damascus in 1928. Massignon's attempt at Romanization failed as the Academy and population viewed the proposal as an attempt from the Western world to take over their country. Sa'id Afghani, a member of the Academy, mentioned that the movement to Romanize the script was a Zionist plan to dominate Lebanon.

After the period of colonialism in Egypt, Egyptians were looking for a way to reclaim and re-emphasize Egyptian culture. As a result, some Egyptians pushed for an Egyptianization of the Arabic language in which the formal Arabic and the colloquial Arabic would be combined into one language and the Latin alphabet would be used. There was also the idea of finding a way to use Hieroglyphics instead of the Latin alphabet, but this was seen as too complicated to use. A scholar, Salama Musa agreed with the idea of applying a Latin alphabet to Arabic, as he believed that would allow Egypt to have a closer relationship with the West. He also believed that Latin script was key to the success of Egypt as it would allow for more advances in science and technology. This change in alphabet, he believed, would solve the problems inherent with Arabic, such as a lack of written vowels and difficulties writing foreign words that made it difficult for non-native speakers to learn. Ahmad Lutfi As Sayid and Muhammad Azmi, two Egyptian intellectuals, agreed with Musa and supported the push for Romanization. The idea that Romanization was necessary for modernization and growth in Egypt continued with Abd Al-Aziz Fahmi in 1944. He was the chairman for the Writing and Grammar Committee for the Arabic Language Academy of Cairo. However, this effort failed as the Egyptian people felt a strong cultural tie to the Arabic alphabet. In particular, the older Egyptian generations believed that the Arabic alphabet had strong connections to Arab values and history, due to the long history of the Arabic alphabet (Shrivtiel, 189) in Muslim societies.

The Quran introduced a new way of writing to the world. People began studying and applying the unique styles they learned from the Quran to not only their own writing, but also their culture. Writers studied the unique structure and format of the Quran in order to identify and apply the figurative devices and their impact on the reader.

The Quran inspired musicality in poetry through the internal rhythm of the verses. The arrangement of words, how certain sounds create harmony, and the agreement of rhymes create the sense of rhythm within each verse. At times, the chapters of the Quran only have the rhythm in common.

The repetition in the Quran introduced the true power and impact repetition can have in poetry. The repetition of certain words and phrases made them appear more firm and explicit in the Quran. The Quran uses constant metaphors of blindness and deafness to imply unbelief. Metaphors were not a new concept to poetry, however the strength of extended metaphors was. The explicit imagery in the Quran inspired many poets to include and focus on the feature in their own work. The poet ibn al-Mu'tazz wrote a book regarding the figures of speech inspired by his study of the Quran. Poets such as badr Shakir al sayyab expresses his political opinion in his work through imagery inspired by the forms of more harsher imagery used in the Quran.
The Quran uses figurative devices in order to express the meaning in the most beautiful form possible. The study of the pauses in the Quran as well as other rhetoric allow it to be approached in a multiple ways.

Although the Quran is known for its fluency and harmony, the structure can be best described as not always being inherently chronological, but can also flow thematically instead(the chapters in the Quran have segments that flow in chronological order, however segments can transition into other segments not related in chronology, but could be related in topic). The suras, also known as chapters of the Quran, are not placed in chronological order. The only constant in their structure is that the longest are placed first and shorter ones follow. The topics discussed in the chapters can also have no direct relation to each other (as seen in many suras) and can share in their sense of rhyme. The Quran introduces to poetry the idea of abandoning order and scattering narratives throughout the text. Harmony is also present in the sound of the Quran. The elongations and accents present in the Quran create a harmonious flow within the writing. Unique sound of the Quran recited, due to the accents, create a deeper level of understanding through a deeper emotional connection.

The Quran is written in a language that is simple and understandable by people. The simplicity of the writing inspired later poets to write in a more clear and clear-cut style. The words of the Quran, although unchanged, are to this day understandable and frequently used in both formal and informal Arabic. The simplicity of the language makes memorizing and reciting the Quran a slightly easier task.

The writer al-Khattabi explains how culture is a required element to create a sense of art in work as well as understand it. He believes that the fluency and harmony which the Quran possess are not the only elements that make it beautiful and create a bond between the reader and the text.
While a lot of poetry was deemed comparable to the Quran in that it is equal to or better than the composition of the Quran, a debate rose that such statements are not possible because humans are incapable of composing work comparable to the Quran.
Because the structure of the Quran made it difficult for a clear timeline to be seen, Hadith were the main source of chronological order. The Hadith were passed down from generation to generation and this tradition became a large resource for understanding the context. Poetry after the Quran began possessing this element of tradition by including ambiguity and background information to be required to understand the meaning.

After the Quran came down to the people, the tradition of memorizing the verses became present. It is believed that the greater the amount of the Quran memorized, the greater the faith. As technology improved over time, hearing recitations of the Quran became more available as well as more tools to help memorize the verses.
The tradition of Love Poetry served as a symbolic representation of a Muslim's desire for a closer contact with their Lord.

While the influence of the Quran on Arabic poetry is explained and defended by numerous writers, some writers such as Al-Baqillani believe that poetry and the Quran are in no conceivable way related due to the uniqueness of the Quran. Poetry's imperfections prove his points that they cannot be compared with the fluency the Quran holds.

Classical Arabic is the language of poetry and literature (including news); it is also mainly the language of the Quran. Classical Arabic is closely associated with the religion of Islam because the Quran was written in it. Most of the world's Muslims do not speak Classical Arabic as their native language, but many can read the Quranic script and recite the Quran. Among non-Arab Muslims, translations of the Quran are most often accompanied by the original text. At present, Modern Standard Arabic (MSA) is also used in modernized versions of literary forms of the Quran.

Some Muslims present a monogenesis of languages and claim that the Arabic language was the language revealed by God for the benefit of mankind and the original language as a prototype system of symbolic communication, based upon its system of triconsonantal roots, spoken by man from which all other languages were derived, having first been corrupted. Judaism has a similar account with the Tower of Babel.

"Colloquial Arabic" is a collective term for the spoken dialects of Arabic used throughout the Arab world, which differ radically from the literary language. The main dialectal division is between the varieties within and outside of the Arabian peninsula, followed by that between sedentary varieties and the much more conservative Bedouin varieties. All the varieties outside of the Arabian peninsula (which include the large majority of speakers) have many features in common with each other that are not found in Classical Arabic. This has led researchers to postulate the existence of a prestige koine dialect in the one or two centuries immediately following the Arab conquest, whose features eventually spread to all newly conquered areas. (These features are present to varying degrees inside the Arabian peninsula. Generally, the Arabian peninsula varieties have much more diversity than the non-peninsula varieties, but these have been understudied.)

Within the non-peninsula varieties, the largest difference is between the non-Egyptian North African dialects (especially Moroccan Arabic) and the others. Moroccan Arabic in particular is hardly comprehensible to Arabic speakers east of Libya (although the converse is not true, in part due to the popularity of Egyptian films and other media).

One factor in the differentiation of the dialects is influence from the languages previously spoken in the areas, which have typically provided a significant number of new words and have sometimes also influenced pronunciation or word order; however, a much more significant factor for most dialects is, as among Romance languages, retention (or change of meaning) of different classical forms. Thus Iraqi "aku", Levantine "fīh" and North African "kayən" all mean 'there is', and all come from Classical Arabic forms ("yakūn", "fīhi", "kā'in" respectively), but now sound very different.

Transcription is a broad IPA transcription, so minor differences were ignored for easier comparison. Also, the pronunciation of Modern Standard Arabic differs significantly from region to region.

According to Charles A. Ferguson, the following are some of the characteristic features of the koiné that underlies all the modern dialects outside the Arabian peninsula. Although many other features are common to most or all of these varieties, Ferguson believes that these features in particular are unlikely to have evolved independently more than once or twice and together suggest the existence of the koine:


Of the 29 Proto-Semitic consonants, only one has been lost: , which merged with , while became (see Semitic languages). Various other consonants have changed their sound too, but have remained distinct. An original lenited to , and – consistently attested in pre-Islamic Greek transcription of Arabic languages – became palatalized to or by the time of the Quran and , , or after early Muslim conquests and in MSA (see Arabic phonology#Local variations for more detail). An original voiceless alveolar lateral fricative became . Its emphatic counterpart was considered by Arabs to be the most unusual sound in Arabic (Hence the Classical Arabic's appellation ' or "language of the '"); for most modern dialects, it has become an emphatic stop with loss of the laterality or with complete loss of any pharyngealization or velarization, . (The classical "" pronunciation of pharyngealization still occurs in the Mehri language, and the similar sound without velarization, , exists in other Modern South Arabian languages.)

Other changes may also have happened. Classical Arabic pronunciation is not thoroughly recorded and different reconstructions of the sound system of Proto-Semitic propose different phonetic values. One example is the emphatic consonants, which are pharyngealized in modern pronunciations but may have been velarized in the eighth century and glottalized in Proto-Semitic.

Reduction of and between vowels occurs in a number of circumstances and is responsible for much of the complexity of third-weak ("defective") verbs. Early Akkadian transcriptions of Arabic names shows that this reduction had not yet occurred as of the early part of the 1st millennium BC.

The Classical Arabic language as recorded was a poetic koine that reflected a consciously archaizing dialect, chosen based on the tribes of the western part of the Arabian Peninsula, who spoke the most conservative variants of Arabic. Even at the time of Muhammed and before, other dialects existed with many more changes, including the loss of most glottal stops, the loss of case endings, the reduction of the diphthongs and into monophthongs , etc. Most of these changes are present in most or all modern varieties of Arabic.

An interesting feature of the writing system of the Quran (and hence of Classical Arabic) is that it contains certain features of Muhammad's native dialect of Mecca, corrected through diacritics into the forms of standard Classical Arabic. Among these features visible under the corrections are the loss of the glottal stop and a differing development of the reduction of certain final sequences containing : Evidently, final became as in the Classical language, but final became a different sound, possibly (rather than again in the Classical language). This is the apparent source of the "alif maqṣūrah" 'restricted alif' where a final is reconstructed: a letter that would normally indicate or some similar high-vowel sound, but is taken in this context to be a logical variant of "alif" and represent the sound .

Although Classical Arabic was a unitary language and is now used in Quran, its pronunciation varies somewhat from country to country and from region to region within a country. It is influenced by colloquial dialects.

The "colloquial" spoken dialects of Arabic are learned at home and constitute the native languages of Arabic speakers. "Formal" Literary Arabic (usually specifically Modern Standard Arabic) is learned at school; although many speakers have a native-like command of the language, it is technically not the native language of any speakers. Both varieties can be both written and spoken, although the colloquial varieties are rarely written down and the formal variety is spoken mostly in formal circumstances, e.g., in radio and TV broadcasts, formal lectures, parliamentary discussions and to some extent between speakers of different colloquial dialects. Even when the literary language is spoken, however, it is normally only spoken in its pure form when reading a prepared text out loud and communication between speakers of different colloquial dialects. When speaking extemporaneously (i.e. making up the language on the spot, as in a normal discussion among people), speakers tend to deviate somewhat from the strict literary language in the direction of the colloquial varieties. In fact, there is a continuous range of "in-between" spoken varieties: from nearly pure Modern Standard Arabic (MSA), to a form that still uses MSA grammar and vocabulary but with significant colloquial influence, to a form of the colloquial language that imports a number of words and grammatical constructions in MSA, to a form that is close to pure colloquial but with the "rough edges" (the most noticeably "vulgar" or non-Classical aspects) smoothed out, to pure colloquial. The particular variant (or "register") used depends on the social class and education level of the speakers involved and the level of formality of the speech situation. Often it will vary within a single encounter, e.g., moving from nearly pure MSA to a more mixed language in the process of a radio interview, as the interviewee becomes more comfortable with the interviewer. This type of variation is characteristic of the diglossia that exists throughout the Arabic-speaking world.
Although Modern Standard Arabic (MSA) is a unitary language, its pronunciation varies somewhat from country to country and from region to region within a country. The variation in individual "accents" of MSA speakers tends to mirror corresponding variations in the colloquial speech of the speakers in question, but with the distinguishing characteristics moderated somewhat. Note that it is important in descriptions of "Arabic" phonology to distinguish between pronunciation of a given colloquial (spoken) dialect and the pronunciation of MSA by these same speakers. Although they are related, they are not the same. For example, the phoneme that derives from Classical Arabic has many different pronunciations in the modern spoken varieties, e.g., including the proposed original . Speakers whose native variety has either or will use the same pronunciation when speaking MSA. Even speakers from Cairo, whose native Egyptian Arabic has , normally use when speaking MSA. The of Persian Gulf speakers is the only variant pronunciation which isn't found in MSA; is used instead, but may use [j] in MSA for comfortable pronunciation. Another reason of different pronunciations is influence of colloquial dialects. The differentiation of pronunciation of colloquial dialects is the influence from other languages previously spoken and some still presently spoken in the regions, such as Coptic in Egypt, Berber, Punic, or Phoenician in North Africa, Himyaritic, Modern South Arabian, and Old South Arabian in Yemen and Oman, and Aramaic and Canaanite languages (including Phoenician) in the Levant and Mesopotamia.

Another example: Many colloquial varieties are known for a type of vowel harmony in which the presence of an "emphatic consonant" triggers backed allophones of nearby vowels (especially of the low vowels , which are backed to in these circumstances and very often fronted to in all other circumstances). In many spoken varieties, the backed or "emphatic" vowel allophones spread a fair distance in both directions from the triggering consonant; in some varieties (most notably Egyptian Arabic), the "emphatic" allophones spread throughout the entire word, usually including prefixes and suffixes, even at a distance of several syllables from the triggering consonant. Speakers of colloquial varieties with this vowel harmony tend to introduce it into their MSA pronunciation as well, but usually with a lesser degree of spreading than in the colloquial varieties. (For example, speakers of colloquial varieties with extremely long-distance harmony may allow a moderate, but not extreme, amount of spreading of the harmonic allophones in their MSA speech, while speakers of colloquial varieties with moderate-distance harmony may only harmonize immediately adjacent vowels in MSA.)

Modern Standard Arabic has six pure vowels (while most modern dialects have eight pure vowels which includes the long vowels ), with short and corresponding long vowels . There are also two diphthongs: and .

The pronunciation of the vowels differs from speaker to speaker, in a way that tends to reflect the pronunciation of the corresponding colloquial variety. Nonetheless, there are some common trends. Most noticeable is the differing pronunciation of and , which tend towards fronted , or in most situations, but a back in the neighborhood of emphatic consonants. Some accents and dialects, such as those of the Hejaz region, have an open or a central in all situations. The vowel varies towards too. Listen to the final vowel in the recording of " at the beginning of this article, for example. The point is, Arabic has only three short vowel phonemes, so those phonemes can have a very wide range of allophones. The vowels and are often affected somewhat in emphatic neighborhoods as well, with generally more back or centralized allophones, but the differences are less great than for the low vowels. The pronunciation of short and tends towards and , respectively, in many dialects.

The definition of both "emphatic" and "neighborhood" vary in ways that reflect (to some extent) corresponding variations in the spoken dialects. Generally, the consonants triggering "emphatic" allophones are the pharyngealized consonants ; ; and , if not followed immediately by . Frequently, the fricatives also trigger emphatic allophones; occasionally also the pharyngeal consonants (the former more than the latter). Many dialects have multiple emphatic allophones of each vowel, depending on the particular nearby consonants. In most MSA accents, emphatic coloring of vowels is limited to vowels immediately adjacent to a triggering consonant, although in some it spreads a bit farther: e.g., ' 'time'; ' 'homeland'; " 'downtown' (sometimes or similar).

In a non-emphatic environment, the vowel in the diphthong tends to be fronted even more than elsewhere, often pronounced or : hence ' 'sword' but ' 'summer'. However, in accents with no emphatic allophones of (e.g., in the Hejaz), the pronunciation or occurs in all situations.

The phoneme is represented by the Arabic letter ' () and has many standard pronunciations. is characteristic of north Algeria, Iraq, and most of the Arabian peninsula but with an allophonic in some positions; occurs in most of the Levant and most North Africa; and is used in most of Egypt and some regions in Yemen and Oman. Generally this corresponds with the pronunciation in the colloquial dialects. In some regions in Sudan and Yemen, as well as in some Sudanese and Yemeni dialects, it may be either or , representing the original pronunciation of Classical Arabic. Foreign words containing may be transcribed with , , , , , or , mainly depending on the regional spoken variety of Arabic or the commonly diacriticized Arabic letter. Note also that in northern Egypt, where the Arabic letter ' () is normally pronounced , a separate phoneme , which may be transcribed with , occurs in a small number of mostly non-Arabic loanwords, e.g., 'jacket'.

In many varieties, () are epiglottal in Western Asia.

The emphatic consonant was actually pronounced , or possibly —either way, a highly unusual sound. The medieval Arabs actually termed their language " 'the language of the Ḍād' (the name of the letter used for this sound), since they thought the sound was unique to their language. (In fact, it also exists in a few other minority Semitic languages, e.g., Mehri.)

Arabic has consonants traditionally termed "emphatic" (), which exhibit simultaneous pharyngealization as well as varying degrees of velarization (depending on the region), so they may be written with the "Velarized or pharyngealized" diacritic () as: . This simultaneous articulation is described as "Retracted Tongue Root" by phonologists. In some transcription systems, emphasis is shown by capitalizing the letter, for example, is written ; in others the letter is underlined or has a dot below it, for example, .

Vowels and consonants can be phonologically short or long. Long (geminate) consonants are normally written doubled in Latin transcription (i.e. bb, dd, etc.), reflecting the presence of the Arabic diacritic mark ', which indicates doubled consonants. In actual pronunciation, doubled consonants are held twice as long as short consonants. This consonant lengthening is phonemically contrastive: ' 'he accepted' vs. " 'he kissed'.

Arabic has two kinds of syllables: open syllables (CV) and (CVV)—and closed syllables (CVC), (CVVC) and (CVCC). The syllable types with two morae (units of time), i.e. CVC and CVV, are termed "heavy syllables", while those with three morae, i.e. CVVC and CVCC, are "superheavy syllables". Superheavy syllables in Classical Arabic occur in only two places: at the end of the sentence (due to pausal pronunciation) and in words such as ' 'hot', ' 'stuff, substance', ' 'they disputed with each other', where a long ' occurs before two identical consonants (a former short vowel between the consonants has been lost). (In less formal pronunciations of Modern Standard Arabic, superheavy syllables are common at the end of words or before clitic suffixes such as "" 'us, our', due to the deletion of final short vowels.)

In surface pronunciation, every vowel must be preceded by a consonant (which may include the glottal stop ). There are no cases of hiatus within a word (where two vowels occur next to each other, without an intervening consonant). Some words do have an underlying vowel at the beginning, such as the definite article "al-" or words such as ' 'he bought', ' 'meeting'. When actually pronounced, one of three things happens:

Word stress is not phonemically contrastive in Standard Arabic. It bears a strong relationship to vowel length. The basic rules for Modern Standard Arabic are:

Examples:' 'book', ' 'writer', ' 'desk', ' 'desks', ' 'library' (but ' 'library' in short pronunciation), ' (Modern Standard Arabic) 'they wrote' = ' (dialect), ' (Modern Standard Arabic) 'they wrote it' = ' (dialect), ' (Modern Standard Arabic) 'they (dual, fem) wrote', ' (Modern Standard Arabic) 'I wrote' = ' (short form or dialect). Doubled consonants count as two consonants: ' 'magazine', "" "place".

These rules may result in differently stressed syllables when final case endings are pronounced, vs. the normal situation where they are not pronounced, as in the above example of ' 'library' in full pronunciation, but ' 'library' in short pronunciation.

The restriction on final long vowels does not apply to the spoken dialects, where original final long vowels have been shortened and secondary final long vowels have arisen from loss of original final "-hu/hi".

Some dialects have different stress rules. In the Cairo (Egyptian Arabic) dialect a heavy syllable may not carry stress more than two syllables from the end of a word, hence ' 'school', ' 'Cairo'. This also affects the way that Modern Standard Arabic is pronounced in Egypt. In the Arabic of Sanaa, stress is often retracted: ' 'two houses', ' 'their table', ' 'desks', ' 'sometimes', "" 'their school'. (In this dialect, only syllables with long vowels or diphthongs are considered heavy; in a two-syllable word, the final syllable can be stressed only if the preceding syllable is light; and in longer words, the final syllable cannot be stressed.)

The final short vowels (e.g., the case endings "-a -i -u" and mood endings "-u -a") are often not pronounced in this language, despite forming part of the formal paradigm of nouns and verbs. The following levels of pronunciation exist:

This is the most formal level actually used in speech. All endings are pronounced as written, except at the end of an utterance, where the following changes occur:

This is a formal level of pronunciation sometimes seen. It is somewhat like pronouncing all words as if they were in pausal position (with influence from the colloquial varieties). The following changes occur:

This is the pronunciation used by speakers of Modern Standard Arabic in extemporaneous speech, i.e. when producing new sentences rather than simply reading a prepared text. It is similar to formal short pronunciation except that the rules for dropping final vowels apply "even" when a clitic suffix is added. Basically, short-vowel case and mood endings are never pronounced and certain other changes occur that echo the corresponding colloquial pronunciations. Specifically:

As mentioned above, many spoken dialects have a process of "emphasis spreading", where the "emphasis" (pharyngealization) of emphatic consonants spreads forward and back through adjacent syllables, pharyngealizing all nearby consonants and triggering the back allophone in all nearby low vowels. The extent of emphasis spreading varies. For example, in Moroccan Arabic, it spreads as far as the first full vowel (i.e. sound derived from a long vowel or diphthong) on either side; in many Levantine dialects, it spreads indefinitely, but is blocked by any or ; while in Egyptian Arabic, it usually spreads throughout the entire word, including prefixes and suffixes. In Moroccan Arabic, also have emphatic allophones and , respectively.

Unstressed short vowels, especially , are deleted in many contexts. Many sporadic examples of short vowel change have occurred (especially → and interchange ↔). Most Levantine dialects merge short /i u/ into in most contexts (all except directly before a single final consonant). In Moroccan Arabic, on the other hand, short triggers labialization of nearby consonants (especially velar consonants and uvular consonants), and then short /a i u/ all merge into , which is deleted in many contexts. (The labialization plus is sometimes interpreted as an underlying phoneme .) This essentially causes the wholesale loss of the short-long vowel distinction, with the original long vowels remaining as half-long , phonemically , which are used to represent "both" short and long vowels in borrowings from Literary Arabic.

Most spoken dialects have monophthongized original to in most circumstances, including adjacent to emphatic consonants, while keeping them as the original diphthongs in others e.g. . In most of the Moroccan, Algerian and Tunisian (except Sahil and Southeastern) Arabic dialects, they have subsequently merged into original .

In some dialects, there may be more or fewer phonemes than those listed in the chart above. For example, non-Arabic is used in the Maghrebi dialects as well in the written language mostly for foreign names. Semitic became extremely early on in Arabic before it was written down; a few modern Arabic dialects, such as Iraqi (influenced by Persian and Kurdish) distinguish between and . The Iraqi Arabic also uses sounds , and uses Persian adding letters, e.g.: "" – "a plum"; " – "a truffle" and so on.

Early in the expansion of Arabic, the separate emphatic phonemes and coalesced into a single phoneme . Many dialects (such as Egyptian, Levantine, and much of the Maghreb) subsequently lost fricatives, converting into . Most dialects borrow "learned" words from the Standard language using the same pronunciation as for inherited words, but some dialects without interdental fricatives (particularly in Egypt and the Levant) render original in borrowed words as .

Another key distinguishing mark of Arabic dialects is how they render the original velar and uvular plosives , (Proto-Semitic ), and :

Pharyngealization of the emphatic consonants tends to weaken in many of the spoken varieties, and to spread from emphatic consonants to nearby sounds. In addition, the "emphatic" allophone automatically triggers pharyngealization of adjacent sounds in many dialects. As a result, it may difficult or impossible to determine whether a given coronal consonant is phonemically emphatic or not, especially in dialects with long-distance emphasis spreading. (A notable exception is the sounds vs. in Moroccan Arabic, because the former is pronounced as an affricate but the latter is not.)

As in other Semitic languages, Arabic has a complex and unusual morphology (i.e. method of constructing words from a basic root). Arabic has a nonconcatenative "root-and-pattern" morphology: A root consists of a set of bare consonants (usually three), which are fitted into a discontinuous pattern to form words. For example, the word for 'I wrote' is constructed by combining the root ' 'write' with the pattern ' 'I Xed' to form ' 'I wrote'. Other verbs meaning 'I Xed' will typically have the same pattern but with different consonants, e.g. ' 'I read', ' 'I ate', ' 'I went', although other patterns are possible (e.g. ' 'I drank', ' 'I said', ' 'I spoke', where the subpattern used to signal the past tense may change but the suffix ' is always used).

From a single root , numerous words can be formed by applying different patterns:

Nouns in Literary Arabic have three grammatical cases (nominative, accusative, and genitive [also used when the noun is governed by a preposition]); three numbers (singular, dual and plural); two genders (masculine and feminine); and three "states" (indefinite, definite, and construct). The cases of singular nouns (other than those that end in long ā) are indicated by suffixed short vowels (/-u/ for nominative, /-a/ for accusative, /-i/ for genitive).

The feminine singular is often marked by ـَة /-at/, which is pronounced as /-ah/ before a pause. Plural is indicated either through endings (the sound plural) or internal modification (the broken plural). Definite nouns include all proper nouns, all nouns in "construct state" and all nouns which are prefixed by the definite article اَلْـ /al-/. Indefinite singular nouns (other than those that end in long ā) add a final /-n/ to the case-marking vowels, giving /-un/, /-an/ or /-in/ (which is also referred to as nunation or tanwīn).

Adjectives in Literary Arabic are marked for case, number, gender and state, as for nouns. However, the plural of all non-human nouns is always combined with a singular feminine adjective, which takes the ـَة /-at/ suffix.

Pronouns in Literary Arabic are marked for person, number and gender. There are two varieties, independent pronouns and enclitics. Enclitic pronouns are attached to the end of a verb, noun or preposition and indicate verbal and prepositional objects or possession of nouns. The first-person singular pronoun has a different enclitic form used for verbs (ـنِي /-nī/) and for nouns or prepositions (ـِي /-ī/ after consonants, ـيَ /-ya/ after vowels).

Nouns, verbs, pronouns and adjectives agree with each other in all respects. However, non-human plural nouns are grammatically considered to be feminine singular. Furthermore, a verb in a verb-initial sentence is marked as singular regardless of its semantic number when the subject of the verb is explicitly mentioned as a noun. Numerals between three and ten show "chiasmic" agreement, in that grammatically masculine numerals have feminine marking and vice versa.

Verbs in Literary Arabic are marked for person (first, second, or third), gender, and number. They are conjugated in two major paradigms (past and non-past); two voices (active and passive); and six moods (indicative, imperative, subjunctive, jussive, shorter energetic and longer energetic), the fifth and sixth moods, the energetics, exist only in Classical Arabic but not in MSA. There are also two participles (active and passive) and a verbal noun, but no infinitive.

The past and non-past paradigms are sometimes also termed perfective and imperfective, indicating the fact that they actually represent a combination of tense and aspect. The moods other than the indicative occur only in the non-past, and the future tense is signaled by prefixing سَـ ' or سَوْفَ ' onto the non-past. The past and non-past differ in the form of the stem (e.g., past كَتَبـ' vs. non-past ـكْتُبـ '), and also use completely different sets of affixes for indicating person, number and gender: In the past, the person, number and gender are fused into a single suffixal morpheme, while in the non-past, a combination of prefixes (primarily encoding person) and suffixes (primarily encoding gender and number) are used. The passive voice uses the same person/number/gender affixes but changes the vowels of the stem.

The following shows a paradigm of a regular Arabic verb, كَتَبَ "" 'to write'. Note that in Modern Standard, the energetic mood (in either long or short form, which have the same meaning) is almost never used.

Like other Semitic languages, and unlike most other languages, Arabic makes much more use of nonconcatenative morphology (applying many templates applied roots) to derive words than adding prefixes or suffixes to words.

For verbs, a given root can occur in many different derived verb stems (of which there are about fifteen), each with one or more characteristic meanings and each with its own templates for the past and non-past stems, active and passive participles, and verbal noun. These are referred to by Western scholars as "Form I", "Form II", and so on through "Form XV" (although Forms XI to XV are rare). These stems encode grammatical functions such as the causative, intensive and reflexive. Stems sharing the same root consonants represent separate verbs, albeit often semantically related, and each is the basis for its own conjugational paradigm. As a result, these derived stems are part of the system of derivational morphology, not part of the inflectional system.

Examples of the different verbs formed from the root كتب ' 'write' (using حمر ' 'red' for Form IX, which is limited to colors and physical defects):
Form II is sometimes used to create transitive denominative verbs (verbs built from nouns); Form V is the equivalent used for intransitive denominatives.

The associated participles and verbal nouns of a verb are the primary means of forming new lexical nouns in Arabic. This is similar to the process by which, for example, the English gerund "meeting" (similar to a verbal noun) has turned into a noun referring to a particular type of social, often work-related event where people gather together to have a "discussion" (another lexicalized verbal noun). Another fairly common means of forming nouns is through one of a limited number of patterns that can be applied directly to roots, such as the "nouns of location" in "ma-" (e.g. ' 'desk, office' < ' 'write', ' 'kitchen' < ' 'cook').

The only three genuine suffixes are as follows:

The spoken dialects have lost the case distinctions and make only limited use of the dual (it occurs only on nouns and its use is no longer required in all circumstances). They have lost the mood distinctions other than imperative, but many have since gained new moods through the use of prefixes (most often /bi-/ for indicative vs. unmarked subjunctive). They have also mostly lost the indefinite "nunation" and the internal passive.

The following is an example of a regular verb paradigm in Egyptian Arabic.

The Arabic alphabet derives from the Aramaic through Nabatean, to which it bears a loose resemblance like that of Coptic or Cyrillic scripts to Greek script. Traditionally, there were several differences between the Western (North African) and Middle Eastern versions of the alphabet—in particular, the "faʼ" had a dot underneath and "qaf" a single dot above in the Maghreb, and the order of the letters was slightly different (at least when they were used as numerals).

However, the old Maghrebi variant has been abandoned except for calligraphic purposes in the Maghreb itself, and remains in use mainly in the Quranic schools (zaouias) of West Africa. Arabic, like all other Semitic languages (except for the Latin-written Maltese, and the languages with the Ge'ez script), is written from right to left. There are several styles of scripts such as thuluth, muhaqqaq, tawqi, rayhan and notably naskh, which is used in print and by computers, and ruqʻah, which is commonly used for correspondence.

Originally Arabic was made up of only "rasm" without diacritical marks Later diacritical points (which in Arabic are referred to as "nuqaṯ") were added (which allowed readers to distinguish between letters such as b, t, th, n and y). Finally signs known as "Tashkil" were used for short vowels known as "harakat" and other uses such as final postnasalized or long vowels.

After Khalil ibn Ahmad al Farahidi finally fixed the Arabic script around 786, many styles were developed, both for the writing down of the Quran and other books, and for inscriptions on monuments as decoration.

Arabic calligraphy has not fallen out of use as calligraphy has in the Western world, and is still considered by Arabs as a major art form; calligraphers are held in great esteem. Being cursive by nature, unlike the Latin script, Arabic script is used to write down a verse of the Quran, a hadith, or simply a proverb. The composition is often abstract, but sometimes the writing is shaped into an actual form such as that of an animal. One of the current masters of the genre is Hassan Massoudy.

In modern times the intrinsically calligraphic nature of the written Arabic form is haunted by the thought that a typographic approach to the language, necessary for digitized unification, will not always accurately maintain meanings conveyed through calligraphy.

There are a number of different standards for the romanization of Arabic, i.e. methods of accurately and efficiently representing Arabic with the Latin script. There are various conflicting motivations involved, which leads to multiple systems. Some are interested in transliteration, i.e. representing the "spelling" of Arabic, while others focus on transcription, i.e. representing the "pronunciation" of Arabic. (They differ in that, for example, the same letter is used to represent both a consonant, as in "you" or "yet", and a vowel, as in "me" or "eat".) Some systems, e.g. for scholarly use, are intended to accurately and unambiguously represent the phonemes of Arabic, generally making the phonetics more explicit than the original word in the Arabic script. These systems are heavily reliant on diacritical marks such as "š" for the sound equivalently written "sh" in English. Other systems (e.g. the Bahá'í orthography) are intended to help readers who are neither Arabic speakers nor linguists with intuitive pronunciation of Arabic names and phrases. These less "scientific" systems tend to avoid diacritics and use digraphs (like "sh" and "kh"). These are usually simpler to read, but sacrifice the definiteness of the scientific systems, and may lead to ambiguities, e.g. whether to interpret "sh" as a single sound, as in "gash", or a combination of two sounds, as in "gashouse". The ALA-LC romanization solves this problem by separating the two sounds with a prime symbol ( ′ ); e.g., "as′hal" 'easier'.

During the last few decades and especially since the 1990s, Western-invented text communication technologies have become prevalent in the Arab world, such as personal computers, the World Wide Web, email, bulletin board systems, IRC, instant messaging and mobile phone text messaging. Most of these technologies originally had the ability to communicate using the Latin script only, and some of them still do not have the Arabic script as an optional feature. As a result, Arabic speaking users communicated in these technologies by transliterating the Arabic text using the Latin script, sometimes known as IM Arabic.

To handle those Arabic letters that cannot be accurately represented using the Latin script, numerals and other characters were appropriated. For example, the numeral "3" may be used to represent the Arabic letter . There is no universal name for this type of transliteration, but some have named it Arabic Chat Alphabet. Other systems of transliteration exist, such as using dots or capitalization to represent the "emphatic" counterparts of certain consonants. For instance, using capitalization, the letter , may be represented by d. Its emphatic counterpart, , may be written as D.

In most of present-day North Africa, the Western Arabic numerals (0, 1, 2, 3, 4, 5, 6, 7, 8, 9) are used. However, in Egypt and Arabic-speaking countries to the east of it, the Eastern Arabic numerals ( – – – – – – – – – ) are in use. When representing a number in Arabic, the lowest-valued position is placed on the right, so the order of positions is the same as in left-to-right scripts. Sequences of digits such as telephone numbers are read from left to right, but numbers are spoken in the traditional Arabic fashion, with units and tens reversed from the modern English usage. For example, 24 is said "four and twenty" just like in the German language ("vierundzwanzig") and Classical Hebrew, and 1975 is said "a thousand and nine-hundred and five and seventy" or, more eloquently, "a thousand and nine-hundred five seventy"

Academy of the Arabic Language is the name of a number of language-regulation bodies formed in the Arab League. The most active are in Damascus and Cairo. They review language development, monitor new words and approve inclusion of new words into their published standard dictionaries. They also publish old and historical Arabic manuscripts.

Arabic has been taught worldwide in many elementary and secondary schools, especially Muslim schools. Universities around the world have classes that teach Arabic as part of their foreign languages, Middle Eastern studies, and religious studies courses. Arabic language schools exist to assist students to learn Arabic outside the academic world. There are many Arabic language schools in the Arab world and other Muslim countries. Because the Quran is written in Arabic and all Islamic terms are in Arabic, millions of Muslims (both Arab and non-Arab) study the language. Software and books with tapes are also important part of Arabic learning, as many of Arabic learners may live in places where there are no academic or Arabic language school classes available. Radio series of Arabic language classes are also provided from some radio stations. A number of websites on the Internet provide online classes for all levels as a means of distance education; most teach Modern Standard Arabic, but some teach regional varieties from numerous countries.

With the sole example of Medieval linguist Abu Hayyan al-Gharnati – who, while a scholar of the Arabic language, was not ethnically Arab – Medieval scholars of the Arabic language made no efforts at studying comparative linguistics, considering all other languages inferior.

In modern times, the educated upper classes in the Arab world have taken a nearly opposite view. Yasir Suleiman wrote in 2011 that "studying and knowing English or French in most of the Middle East and North Africa have become a badge of sophistication and modernity and ... feigning, or asserting, weakness or lack of facility in Arabic is sometimes paraded as a sign of status, class, and perversely, even education through a mélange of code-switching practises."




</doc>
<doc id="808" url="https://en.wikipedia.org/wiki?curid=808" title="Alfred Hitchcock">
Alfred Hitchcock

Sir Alfred Joseph Hitchcock, (13 August 1899 – 29 April 1980) was an English film director and producer. He is one of the most influential and extensively studied filmmakers in the history of cinema. Known as "the Master of Suspense", he directed over 50 feature films in a career spanning six decades, becoming as well known as any of his actors thanks to his many interviews, his cameo roles in most of his films, and his hosting and producing of the television anthology "Alfred Hitchcock Presents" (1955–1965). His films garnered a total of 46 Oscar nominations and six wins.

Born in Leytonstone, Essex, Hitchcock entered the film industry in 1919 as a title card designer after training as a technical clerk and copy writer for a telegraph-cable company. He made his directorial debut with the silent film "The Pleasure Garden" (1925). His first successful film, "" (1927), helped to shape the thriller genre, while his 1929 film, "Blackmail", was the first British "". Two of his 1930s thrillers, "The 39 Steps" (1935) and "The Lady Vanishes" (1938), are ranked among the greatest British films of the 20th century.

By 1939 Hitchcock was a filmmaker of international importance, and film producer David O. Selznick persuaded him to move to Hollywood. A string of successful films followed, including "Rebecca" (1940), "Foreign Correspondent" (1940), "Suspicion" (1941), "Shadow of a Doubt" (1943), and "Notorious" (1946). "Rebecca" won the Academy Award for Best Picture, although Hitchcock himself was only nominated as Best Director; he was also nominated for "Lifeboat" (1944) and "Spellbound" (1945), although he never won the Best Director Academy Award. 

The "Hitchcockian" style includes the use of camera movement to mimic a person's gaze, thereby turning viewers into voyeurs, and framing shots to maximise anxiety and fear. The film critic Robin Wood wrote that the meaning of a Hitchcock film "is there in the method, in the progression from shot to shot. A Hitchcock film is an organism, with the whole implied in every detail and every detail related to the whole." 

After a brief lull of commercial success in the late 1940s, Hitchcock returned to form with "Strangers on a Train" (1951) and "Dial M For Murder" (1954). By 1960 Hitchcock had directed four films often ranked among the greatest of all time: "Rear Window" (1954), "Vertigo" (1958), "North by Northwest" (1959), and "Psycho" (1960) with the first and last of these garnering him Best Director nominations. In 2012, "Vertigo" replaced Orson Welles's "Citizen Kane" (1941) as the British Film Institute's greatest film ever made based on its world-wide poll of hundreds of film critics. By 2018 eight of his films had been selected for preservation in the United States National Film Registry, including his personal favourite, "Shadow of a Doubt" (1943). He received the BAFTA Fellowship in 1971, the AFI Life Achievement Award in 1979 and was knighted in December that year, four months before he died.

Hitchcock was born on 13 August 1899 in the flat above his parents' leased grocer's shop at 517 High Road, Leytonstone, on the outskirts of east London (then part of Essex), the youngest of three children: William Daniel (born 1890), Ellen Kathleen ("Nellie") (1892), and Alfred Joseph (1899). His parents, Emma Jane Hitchcock, née Whelan (1863–1942), and William Edgar Hitchcock (1862–1914), were both Roman Catholics, with partial roots in Ireland; William was a greengrocer as his father had been. There was a large extended family, including Uncle John Hitchcock with his five-bedroom Victorian house on Campion Road, Putney, complete with maid, cook, chauffeur and gardener. Every summer John rented a seaside house for the family in Cliftonville, Kent. Hitchcock said that he first became class-conscious there, noticing the differences between tourists and locals.

Describing himself as a well-behaved boy—his father called him his "little lamb without a spot"—Hitchcock said he could not remember ever having had a playmate. One of his favourite stories for interviewers was about his father sending him to the local police station with a note when he was five; the policeman looked at the note and locked him in a cell for a few minutes, saying, "This is what we do to naughty boys." The experience left him, he said, with a lifelong fear of policemen; in 1973 he told Tom Snyder that he was "scared stiff of anything ... to do with the law" and wouldn't even drive a car in case he got a parking ticket.

When he was six, the family moved to Limehouse and leased two stores at 130 and 175 Salmon Lane, which they ran as a fish-and-chips shop and fishmongers' respectively; they lived above the former. It seems that Hitchcock was seven when he attended his first school, the Howrah House Convent in Poplar, which he entered in 1907. According to Patrick McGilligan, he stayed at Howrah House for at most two years. He also attended a convent school, the Wode Street School "for the daughters of gentlemen and little boys", run by the Faithful Companions of Jesus; briefly attended a primary school near his home; and was for a very short time, when he was nine, a boarder at Salesian College in Battersea.

The family moved again when he was 11, this time to Stepney, and on 5 October 1910 Hitchcock was sent to St Ignatius College in Stamford Hill, Tottenham (now in the London Borough of Haringey), a Jesuit grammar school with a reputation for discipline. The priests used a hard rubber cane on the boys, always at the end of the day, so the boys had to sit through classes anticipating the punishment once they knew they'd been written up for it. He said it was here that he developed his sense of fear. The school register lists his year of birth as 1900 rather than 1899; Spoto writes that it seems he was deliberately enrolled as a 10-year-old, perhaps because he was a year behind with his schooling. While biographer Gene Adair reports that Hitchcock was "an average, or slightly above-average, pupil", Hitchcock said he was "usually among the four or five at the top of the class"; at the end of his first year, his work in Latin, English, French and religious education was noted. His favourite subject was geography, and he became interested in maps, and railway and bus timetables; according to Taylor, he could recite all the stops on the Orient Express. He told Peter Bogdanovich: "The Jesuits taught me organization, control and, to some degree, analysis."

Hitchcock told his parents that he wanted to be an engineer, and on 25 July 1913, he left St Ignatius and enrolled in night classes at the London County Council School of Engineering and Navigation in Poplar. In a book-length interview in 1962, he told François Truffaut that he had studied "mechanics, electricity, acoustics, and navigation". Then on 12 December 1914 his father, who had been suffering from emphysema and kidney disease, died at the age of 52. To support himself and his mother—his older siblings had left home by then—Hitchcock took a job, for 15 shillings a week (£ in 2017), as a technical clerk at the Henley Telegraph and Cable Company in Blomfield Street near London Wall. He kept up his night classes, this time in art history, painting, economics, and political science. His older brother ran the family shops, while he and his mother continued to live in Salmon Lane.

Hitchcock was too young to enlist when the First World War broke out in July 1914, and when he reached the required age of 18 in 1917, he received a C3 classification ("free from serious organic disease, able to stand service conditions in garrisons at home ... only suitable for sedentary work"). He joined a cadet regiment of the Royal Engineers and took part in theoretical briefings, weekend drills, and exercises. John Russell Taylor wrote that, in one session of practical exercises in Hyde Park, Hitchcock was required to wear puttees. He could never master wrapping them around his legs, and they repeatedly fell down around his ankles.

After the war, Hitchcock began dabbling in creative writing. In June 1919 he became a founding editor and business manager of Henley's in-house publication, "The Henley Telegraph" (sixpence a copy), to which he submitted several short stories. Henley's promoted him to the advertising department, where he wrote copy and drew graphics for advertisements for electric cable. He apparently loved the job and would stay late at the office to examine the proofs; he told Truffaut that this was his "first step toward cinema". He enjoyed watching films, especially American cinema, and from the age of 16 read the trade papers; he watched Charlie Chaplin, D. W. Griffith and Buster Keaton, and particularly liked Fritz Lang's "Der müde Tod" (1921).

While still at Henley's, he read in a trade paper that Famous Players-Lasky, the production arm of Paramount Pictures, was opening a studio in London. They were planning to film "The Sorrows of Satan" by Marie Corelli, so he produced some drawings for the title cards and sent his work to the studio. They hired him, and in 1919 he began working for Islington Studios in Poole Street, Hoxton, as a title-card designer. Donald Spoto writes that most of the staff were Americans with strict job specifications, but the English workers were encouraged to try their hand at anything, which meant that Hitchcock gained experience as a co-writer, art director and production manager on at least 18 silent films. "The Times" wrote in February 1922 about the studio's "special art title department under the supervision of Mr. A. J. Hitchcock". His work there included "Number 13" (1922), also known as "Mrs. Peabody", cancelled because of financial problems—the few finished scenes are lost—and "Always Tell Your Wife" (1923), which he and Seymour Hicks finished together when Hicks was about to give up on it. Hicks wrote later about being helped by "a fat youth who was in charge of the property room ... [n]one other than Alfred Hitchcock".

When Paramount pulled out of London in 1922, Hitchcock was hired as an assistant director by a new firm run in the same location by Michael Balcon, later known as Gainsborough Pictures. Hitchcock worked on "Woman to Woman" (1923) with the director Graham Cutts, designing the set, writing the script and producing. He said: "It was the first film that I had really got my hands onto." The editor and "script girl" on "Woman to Woman" was Alma Reville, his future wife. He also worked as an assistant to Cutts on "The White Shadow" (1924), "The Passionate Adventure" (1924), "The Blackguard" (1925), and "The Prude's Fall" (1925). "The Blackguard" was produced at the Babelsberg Studios in Potsdam, where Hitchcock watched part of the making of F. W. Murnau's film "The Last Laugh" (1924). He was impressed with Murnau's work and later used many of his techniques for the set design in his own productions.

In the summer of 1925, Balcon asked Hitchcock to direct "The Pleasure Garden" (1925), starring Virginia Valli, a co-production of Gainsborough and the German firm Emelka at the Geiselgasteig studio near Munich. Reville, by then Hitchcock's fiancée, was assistant director-editor. Although the film was a commercial flop, Balcon liked Hitchcock's work; a "Daily Express" headline called him the "Young man with a master mind". Balcon asked him to direct a second film in Munich, "The Mountain Eagle" (1926), based on an original story titled "Fear o' God". The film is lost; Hitchcock called it "a very bad movie".

Hitchcock's luck changed with his first thriller, "" (1927), about the hunt for a serial killer who, wearing a black cloak and carrying a black bag, is murdering young blonde women in London, and only on Tuesdays. A landlady suspects that her lodger is the killer, but he turns out to be innocent. To convey the impression footsteps were being heard from an upper floor, Hitchcock had a glass floor made so that the audience could see the lodger pacing up and down in his room above the landlady. Hitchcock had wanted the leading man to be guilty, or for the film at least to end ambiguously, but the star was Ivor Novello, a matinée idol, and the "star system" meant that Novello could not be the villain. Hitchcock told Truffaut: "You have to clearly spell it out in big letters: 'He is innocent.'" (He had the same problem years later with Cary Grant in "Suspicion" (1941).)

Released in January 1927, "The Lodger" was a commercial and critical success in the UK. Hitchcock told Truffaut that the film was the first of his to be influenced by the Expressionist techniques he had witnessed in Germany: "In truth, you might almost say that "The Lodger" was my first picture." He made his first cameo appearance in the film because an extra was needed, and was depicted sitting in a newsroom. A second appearance, standing in a crowd as the leading man is arrested, is in doubt.

On 2 December 1926, Hitchcock married the English-American screenwriter Alma Reville (1899–1982) at the Brompton Oratory in South Kensington. The couple honeymooned in Paris, Lake Como and St. Moritz, before returning to London to live in a leased flat on the top two floors of 153 Cromwell Road, Kensington. Reville, who was born just hours after Hitchcock, converted from Protestantism to Catholicism, apparently at the insistence of Hitchcock's mother; she was baptized on 31 May 1927 and confirmed at Westminster Cathedral by Cardinal Francis Bourne on 5 June.

In 1928, when they learned that she was pregnant, the Hitchcocks purchased "Winter's Grace", a Tudor farmhouse set in 11 acres on Stroud Lane, Shamley Green, Surrey, for £2,500. Their daughter and only child, Patricia Alma Hitchcock, was born on 7 July that year. 

Reville became her husband's closest collaborator; Charles Champlin wrote in 1982: "The Hitchcock touch had four hands, and two were Alma's.". When Hitchcock accepted the AFI Life Achievement Award in 1979, he said he wanted to mention "four people who have given me the most affection, appreciation and encouragement, and constant collaboration. The first of the four is a film editor, the second is a scriptwriter, the third is the mother of my daughter, Pat, and the fourth is as fine a cook as ever performed miracles in a domestic kitchen. And their names are Alma Reville." Reville wrote or co-wrote on many of Hitchcock's films, including "Shadow of a Doubt", "Suspicion" and "The 39 Steps".

Hitchcock began work on his tenth film, "Blackmail" (1929), when its production company, British International Pictures (BIP), converted its Elstree studios to sound. The film was the first British ""; it followed the first American sound feature film, "The Jazz Singer" (1927). "Blackmail" began the Hitchcock tradition of using famous landmarks as a backdrop for suspense sequences, with the climax taking place on the dome of the British Museum. It also features one of his longest cameo appearances, which shows him being bothered by a small boy as he reads a book on the London Underground. In the PBS series "The Men Who Made The Movies", Hitchcock explained how he used early sound recording as a special element of the film, stressing the word "knife" in a conversation with the woman suspected of murder. During this period, Hitchcock directed segments for a BIP revue, "Elstree Calling" (1930), and directed a short film, "An Elastic Affair" (1930), featuring two "Film Weekly" scholarship winners. "An Elastic Affair" is one of the lost films.

In 1933 Hitchcock was once again working for Michael Balcon at Gaumont-British. His first film for the company, "The Man Who Knew Too Much" (1934), was a success; his second, "The 39 Steps" (1935), was acclaimed in the UK and made Hitchcock a star in the US. It also established the quintessential English "Hitchcock blonde" (Madeleine Carroll) as the template for his succession of ice-cold, elegant leading ladies. Screenwriter Robert Towne remarked, "It's not much of an exaggeration to say that all contemporary escapist entertainment begins with "The 39 Steps"". This film was one of the first to introduce the "MacGuffin" plot device, a term coined by the English screenwriter Angus MacPhail. The MacGuffin is an item or goal the protagonist is pursuing, one that otherwise has no narrative value; in "The 39 Steps", the MacGuffin is a stolen set of design plans.

Hitchcock released two spy thrillers in 1936. "Sabotage" was loosely based on Joseph Conrad's novel, "The Secret Agent" (1907), about a woman who discovers that her husband is a terrorist, and "Secret Agent", based on two stories in "" (1928) by W. Somerset Maugham.

At this time, Hitchcock also became notorious for pranks against the cast and crew. These jokes ranged from simple and innocent to crazy and maniacal. For instance, he hosted a dinner party where he dyed all the food blue because, as he claimed, there weren't enough blue foods. He also had a horse delivered to the dressing room of his friend, actor Sir Gerald du Maurier.

Hitchcock's next major success was "The Lady Vanishes" (1938), "one of the greatest train movies from the genre's golden era", according to Philip French, in which Miss Froy (May Whitty), a British spy posing as a governess, disappears on a train journey through the fictional European country of Bandrika. The film saw Hitchcock receive the 1939 New York Film Critics Circle Award for Best Director, the only time he won an award for his direction. Benjamin Crisler, the "New York Times" film critic, wrote in June 1938: "Three unique and valuable institutions the British have that we in America have not: Magna Carta, the Tower Bridge and Alfred Hitchcock, the greatest director of screen melodramas in the world."

David O. Selznick signed Hitchcock to a seven-year contract beginning in March 1939, and the Hitchcocks moved to Hollywood. In June that year "Life" magazine called him the "greatest master of melodrama in screen history". The working arrangements with Selznick were less than ideal. Selznick suffered from constant financial problems, and Hitchcock was often unhappy about Selznick's creative control over his films. In a later interview, Hitchcock said: "[Selznick] was the Big Producer. ... Producer was king. The most flattering thing Mr. Selznick ever said about me—and it shows you the amount of control—he said I was the 'only director' he'd 'trust with a film'." At the same time, Selznick complained about Hitchcock's "goddamn jigsaw cutting", which meant that the producer had to follow Hitchcock's vision of the finished product.

Selznick lent Hitchcock to the larger studios more often than producing Hitchcock's films himself. Selznick made only a few films each year, as did fellow independent producer Samuel Goldwyn, so he did not always have projects for Hitchcock to direct. Goldwyn had also negotiated with Hitchcock on a possible contract, only to be outbid by Selznick. Hitchcock was quickly impressed by the superior resources of the American studios compared to the financial limits he had often faced in Britain.

The Selznick picture "Rebecca" (1940) was Hitchcock's first American film, set in a Hollywood version of England's Cornwall and based on a novel by English novelist Daphne du Maurier. The film stars Laurence Olivier and Joan Fontaine. The story concerns a naïve (and unnamed) young woman who marries a widowed aristocrat. She goes to live in his huge English country house, and struggles with the lingering reputation of his elegant and worldly first wife Rebecca, who died under mysterious circumstances. The film won Best Picture at the 13th Academy Awards; the statuette was given to Selznick, as the film's producer. Hitchcock was nominated for Best Director, his first of five such nominations.

Hitchcock's second American film was the thriller "Foreign Correspondent" (1940), set in Europe, based on Vincent Sheean's book "Personal History" (1935) and produced by Walter Wanger. It was nominated for Best Picture that year. Hitchcock felt uneasy living and working in Hollywood while his country was at war; his concern resulted in a film that overtly supported the British war effort. Filmed in the first year of the Second World War, it was inspired by the rapidly changing events in Europe, as covered by an American newspaper reporter played by Joel McCrea. Mixing footage of European scenes with scenes filmed on a Hollywood backlot, the film avoided direct references to Nazism, Nazi Germany, and Germans to comply with Hollywood's Motion Picture Production Code censorship at the time.

In September 1940 the Hitchcocks bought the Cornwall Ranch near Scotts Valley, California, in the Santa Cruz Mountains. Their primary residence was an English-style home in Bel Air, purchased in 1942. Hitchcock's films were diverse during this period, ranging from the romantic comedy "Mr. & Mrs. Smith" (1941) to the bleak film noir "Shadow of a Doubt" (1943).
"Suspicion" (1941) marked Hitchcock's first film as a producer and director. It is set in England; Hitchcock used the north coast of Santa Cruz for the English coastline sequence. The film is the first of four projects on which Cary Grant worked with Hitchcock, and it is one of the rare occasions that Grant was cast in a sinister role. Grant plays Johnnie Aysgarth, an English con man whose actions raise suspicion and anxiety in his shy young English wife, Lina McLaidlaw (Joan Fontaine). In one scene Hitchcock placed a light inside a glass of milk, perhaps poisoned, that Grant is bringing to his wife; the light makes sure that the audience's attention is on the glass. Grant's character is a killer in the book on which the film was based, "Before the Fact" by Francis Iles, but the studio felt that Grant's image would be tarnished by that. Hitchcock therefore settled for an ambiguous finale, although, as he told François Truffaut, he would have preferred to end with the wife's murder. Fontaine won Best Actress for her performance.

"Saboteur" (1942) is the first of two films that Hitchcock made for Universal during the decade. Hitchcock was forced by Universal Studios to use Universal contract player Robert Cummings and Priscilla Lane, a freelancer who signed a one-picture deal with Universal, both known for their work in comedies and light dramas. Breaking with Hollywood conventions of the time, Hitchcock did extensive location filming, especially in New York City, and depicted a confrontation between a suspected saboteur (Cummings) and a real saboteur (Norman Lloyd) atop the Statue of Liberty. He also directed "Have You Heard?" (1942), a photographic dramatisation for "Life" magazine of the dangers of rumours during wartime. In 1943 he wrote a mystery story for "Look" magazine, "The Murder of Monty Woolley", a sequence of captioned photographs inviting the reader to find clues to the murderer's identity; Hitchcock cast the performers as themselves, such as Woolley, Doris Merrick, and make-up man Guy Pearce.

"Shadow of a Doubt" (1943) was Hitchcock's personal favourite and the second of the early Universal films. Charlotte "Charlie" Newton (Teresa Wright) suspects her beloved uncle Charlie Oakley (Joseph Cotten) of being a serial killer. Hitchcock again filmed extensively on location, this time in the Northern California city of Santa Rosa.

Working at 20th Century Fox, Hitchcock approached John Steinbeck with an idea for a film, which recorded the experiences of the survivors of a German U-boat attack. Steinbeck then began work on the script which would become the film "Lifeboat" (1944). However, Steinbeck was unhappy with the film and asked that his name be removed from the credits, to no avail. The idea was rewritten as a short story by Harry Sylvester and published in "Collier's" in 1943. The action sequences were shot in a small boat in the studio water tank. The locale posed problems for Hitchcock's traditional cameo appearance. That was solved by having Hitchcock's image appear in a newspaper that William Bendix is reading in the boat, showing the director in a before-and-after advertisement for "Reduco-Obesity Slayer". He told Truffaut in 1962:

Hitchcock's typical dinner before the weight loss had been a roast chicken, boiled ham, potatoes, bread, vegetables, relishes, salad, dessert, a bottle of wine and some brandy. To lose weight, he stopped drinking, drank black coffee for breakfast and lunch, and ate steak and salad for dinner, but it was hard to maintain; Spoto writes that his weight fluctuated considerably over the next 40 years. At the end of 1943, despite the weight loss, the Occidental Insurance Company of Los Angeles refused him life insurance.

Hitchcock returned to the UK for an extended visit in late 1943 and early 1944. While there he made two short propaganda films, "Bon Voyage" (1944) and "Aventure Malgache" (1944), for the Ministry of Information. In June and July 1945 Hitchcock served as "treatment advisor" on a Holocaust documentary that used Allied Forces footage of the liberation of Nazi concentration camps. The film was assembled in London and produced by Sidney Bernstein of the Ministry of Information, who brought Hitchcock (a friend of his) on board. It was originally intended to be broadcast to the Germans, but the British government deemed it too traumatic to be shown to a shocked post-war population. Instead, it was transferred in 1952 from the British War Office film vaults to London's Imperial War Museum and remained unreleased until 1985, when an edited version was broadcast as an episode of PBS "Frontline", under the title the Imperial War Museum had given it: "Memory of the Camps". The full-length version of the film, "German Concentration Camps Factual Survey", was restored in 2014 by scholars at the Imperial War Museum.

Hitchcock worked for David Selznick again when he directed "Spellbound" (1945), which explores psychoanalysis and features a dream sequence designed by Salvador Dalí. The dream sequence as it appears in the film is ten minutes shorter than was originally envisioned; Selznick edited it to make it "play" more effectively. Gregory Peck plays amnesiac Dr. Anthony Edwardes under the treatment of analyst Dr. Peterson (Ingrid Bergman), who falls in love with him while trying to unlock his repressed past. Two point-of-view shots were achieved by building a large wooden hand (which would appear to belong to the character whose point of view the camera took) and out-sized props for it to hold: a bucket-sized glass of milk and a large wooden gun. For added novelty and impact, the climactic gunshot was hand-coloured red on some copies of the black-and-white film. The original musical score by Miklós Rózsa makes use of the theremin, and some of it was later adapted by the composer into Rozsa's Piano Concerto Op. 31 (1967) for piano and orchestra.

"Notorious" (1946) followed "Spellbound".
Hitchcock told François Truffaut that Selznick had sold him, Ingrid Bergman, Cary Grant, and the screenplay by Ben Hecht, to RKO Radio Pictures as a "package" for $500,000 () because of cost overruns on Selznick's "Duel in the Sun" (1946). "Notorious" stars Bergman and Grant, both Hitchcock regulars, and features a plot about Nazis, uranium and South America. His prescient use of uranium as a plot device led to him being briefly placed under surveillance by the Federal Bureau of Investigation. According to McGilligan, in or around March 1945 Hitchcock and Ben Hecht consulted Robert Millikan of the California Institute of Technology about the development of a uranium bomb. Selznick complained that the notion was "science fiction", only to be confronted by the news of the detonation of two atomic bombs on Hiroshima and Nagasaki in Japan in August 1945.

Hitchcock formed an independent production company, Transatlantic Pictures, with his friend Sidney Bernstein. He made two films with Transatlantic, one of which was his first colour film. With "Rope" (1948), Hitchcock experimented with marshalling suspense in a confined environment, as he had done earlier with "Lifeboat" (1944). The film appears to have been shot in a single take, but it was actually shot in 10 takes ranging from 4- to 10 minutes each; a 10-minute length of film was the most that a camera's film magazine could hold at the time. Some transitions between reels were hidden by having a dark object fill the entire screen for a moment. Hitchcock used those points to hide the cut, and began the next take with the camera in the same place. The film features James Stewart in the leading role, and was the first of four films that Stewart made with Hitchcock. It was inspired by the Leopold and Loeb case of the 1920s. The film was not well received.

"Under Capricorn" (1949), set in 19th-century Australia, also uses the short-lived technique of long takes, but to a more limited extent. He again used Technicolor in this production, then returned to black-and-white films for several years. Transatlantic Pictures became inactive after these two unsuccessful films. Hitchcock filmed "Stage Fright" (1950) at studios in Elstree, England, where he had worked during his British International Pictures contract many years before. He matched one of Warner Bros.' most popular stars, Jane Wyman, with the expatriate German actor Marlene Dietrich and used several prominent British actors, including Michael Wilding, Richard Todd and Alastair Sim. This was Hitchcock's first proper production for Warner Bros., which had distributed "Rope" and "Under Capricorn", because Transatlantic Pictures was experiencing financial difficulties.

His film "Strangers on a Train" (1951) was based on the novel of the same name by Patricia Highsmith. Hitchcock combined many elements from his preceding films. He approached Dashiell Hammett to write the dialogue, but Raymond Chandler took over, then left over disagreements with the director. In the film, two men casually meet, one of whom speculates on a foolproof method to murder; he suggests that two people, each wishing to do away with someone, should each perform the other's murder. Farley Granger's role was as the innocent victim of the scheme, while Robert Walker, previously known for "boy-next-door" roles, played the villain. "I Confess" (1953) was set in Quebec with Montgomery Clift as a Catholic priest.

"I Confess" was followed by three colour films starring Grace Kelly: "Dial M for Murder" (1954), "Rear Window" (1954), and "To Catch a Thief" (1955). In "Dial M for Murder", Ray Milland plays the villain who tries to murder his unfaithful wife (Kelly) for her money. She kills the hired assassin in self-defence, so Milland manipulates the evidence to make it look like murder. Her lover, Mark Halliday (Robert Cummings), and Police Inspector Hubbard (John Williams) save her from execution. Hitchcock experimented with 3D cinematography for "Dial M".

Hitchcock moved to Paramount Pictures and filmed "Rear Window" (1954), starring James Stewart and Kelly again, as well as Thelma Ritter and Raymond Burr. Stewart's character is a photographer (based on Robert Capa) who must temporarily use a wheelchair. Out of boredom, he begins observing his neighbours across the courtyard, then becomes convinced that one of them (Raymond Burr) has murdered his wife. Stewart eventually manages to convince his policeman buddy (Wendell Corey) and his girlfriend (Kelly). As with "Lifeboat" and "Rope", the principal characters are depicted in confined or cramped quarters, in this case Stewart's studio apartment. Hitchcock uses close-ups of Stewart's face to show his character's reactions, "from the comic voyeurism directed at his neighbours to his helpless terror watching Kelly and Burr in the villain's apartment".

From 1955 to 1965, Hitchcock was the host of the television series "Alfred Hitchcock Presents". With his droll delivery, gallows humour and iconic image, the series made Hitchcock a celebrity. The title-sequence of the show pictured a minimalist caricature of his profile (he drew it himself; it is composed of only nine strokes), which his real silhouette then filled. The series theme tune was "Funeral March of a Marionette" by the French composer Charles Gounod (1818–1893).

His introductions always included some sort of wry humour, such as the description of a recent multi-person execution hampered by having only one electric chair, while two are shown with a sign "Two chairs—no waiting!" He directed 18 episodes of the series, which aired from 1955 to 1965. It became "The Alfred Hitchcock Hour" in 1962, and NBC broadcast the final episode on 10 May 1965. In the 1980s, a new version of "Alfred Hitchcock Presents" was produced for television, making use of Hitchcock's original introductions in a colourised form.

In 1955 Hitchcock became a United States citizen. The same year, his third Grace Kelly film, "To Catch a Thief", was released; it is set in the French Riviera, and pairs Kelly with Cary Grant. Grant plays retired thief John Robie, who becomes the prime suspect for a spate of robberies in the Riviera. A thrill-seeking American heiress played by Kelly surmises his true identity and tries to seduce him. "Despite the obvious age disparity between Grant and Kelly and a lightweight plot, the witty script (loaded with double entendres) and the good-natured acting proved a commercial success." It was Hitchcock's last film with Kelly. She married Prince Rainier of Monaco in 1956, and ended her film career. Hitchcock then remade his own 1934 film "The Man Who Knew Too Much" in 1956. This time, the film starred James Stewart and Doris Day, who sang the theme song "Que Sera, Sera", which won the Oscar for Best Original Song and became a big hit for her. They play a couple whose son is kidnapped to prevent them from interfering with an assassination. As in the 1934 film, the climax takes place at the Royal Albert Hall, London.

"The Wrong Man" (1957), Hitchcock's final film for Warner Bros., is a low-key black-and-white production based on a real-life case of mistaken identity reported in "Life" magazine in 1953. This was the only film of Hitchcock to star Henry Fonda, playing a Stork Club musician mistaken for a liquor store thief, who is arrested and tried for robbery while his wife (Vera Miles) emotionally collapses under the strain. Hitchcock told Truffaut that his lifelong fear of the police attracted him to the subject and was embedded in many scenes.

Hitchcock's next film, "Vertigo" (1958) again starred James Stewart, this time with Kim Novak and Barbara Bel Geddes. He had wanted Vera Miles to play the lead, but she was pregnant. He told Oriana Fallaci: "I was offering her a big part, the chance to become a beautiful sophisticated blonde, a real actress. We'd have spent a heap of dollars on it, and she has the bad taste to get pregnant. I hate pregnant women, because then they have children."

In the film, James Stewart plays Scottie, a former police investigator suffering from acrophobia, who develops an obsession with a woman he has been hired to shadow (Kim Novak). Scottie's obsession leads to tragedy, and this time Hitchcock does not opt for a happy ending. Some critics, including Donald Spoto and Roger Ebert, agree that "Vertigo" is the director's most personal and revealing film, dealing with the "Pygmalion"-like obsessions of a man who crafts a woman into the woman he desires. "Vertigo" explores more frankly and at greater length his interest in the relation between sex and death than any other work in his filmography.

"Vertigo" contains a camera technique developed by Irmin Roberts, commonly referred to as a dolly zoom, that has been copied many times by filmmakers. The film premiered at the San Sebastián International Film Festival, where Hitchcock won a Silver Seashell. "Vertigo" is considered a classic, but it attracted some negative reviews and poor box-office receipts at the time, and it was the last collaboration between Stewart and Hitchcock. In the 2002 "Sight & Sound" polls, it ranked just behind "Citizen Kane" (1941); ten years later, in the same magazine, critics chose it as the best film ever made.

Hitchcock followed "Vertigo" with three more successful films, which are also recognised as among his best: "North by Northwest" (1959), "Psycho" (1960) and "The Birds" (1963). In "North by Northwest", Cary Grant portrays Roger Thornhill, a Madison Avenue advertising executive who is mistaken for a government secret agent. He is hotly pursued across the United States by enemy agents, including (it appears) Eve Kendall (Eva Marie Saint). Thornhill at first believes Kendall is helping him, then that she is an enemy agent; he eventually learns that she is working undercover for the CIA. During its opening two-week run at Radio City Music Hall, the film grossed $404,056 (), setting a record in that theatre's non-holiday gross. "Time" magazine called the film "smoothly troweled and thoroughly entertaining".
"Psycho" (1960) is arguably Hitchcock's best-known film. Based on Robert Bloch's novel "Psycho" (1959), which was inspired by the case of Ed Gein, the film was produced on a constrained budget of $800,000 () and shot in black-and-white on a spare set using crew members from "Alfred Hitchcock Presents". The unprecedented violence of the shower scene, the early death of the heroine, and the innocent lives extinguished by a disturbed murderer became the hallmarks of a new horror-film genre. The public loved the film, with lines stretching outside cinemas as people had to wait for the next showing. It broke box-office records in the United Kingdom, France, South America, the United States and Canada and was a moderate success in Australia for a brief period.

The film was the most profitable of Hitchcock's career; he personally earned well in excess of $15 million (equivalent to $ million in ). He subsequently swapped his rights to "Psycho" and his TV anthology for 150,000 shares of MCA, making him the third largest shareholder and his own boss at Universal, in theory at least, although that did not stop them from interfering with him. Following the first film, "Psycho" became an American horror franchise: "Psycho II", "Psycho III", "Bates Motel", "", and a colour 1998 remake of the original.

On 13 August 1962, Hitchcock's 63rd birthday, the French director François Truffaut began a 50-hour interview of Hitchcock, filmed over eight days at Universal Studios, during which Hitchcock agreed to answer 500 questions. It took four years to transcribe the tapes and organize the images; it was published as a book in 1967, which Truffaut nicknamed the "Hitchbook". The audio tapes were used as the basis of a documentary in 2015. Truffaut sought the interview because it was clear to him that Hitchcock was not simply the mass-market entertainer the American media made him out to be. It was obvious from his films, Truffaut wrote, that Hitchcock had "given more thought to the potential of his art than any of his colleagues". He compared the interview to "Oedipus' consultation of the oracle".

The film scholar Peter William Evans writes that "The Birds" (1963) and "Marnie" (1964) are regarded as "undisputed masterpieces". Hitchcock had intended to film "Marnie" first, and in March 1962 it was announced that Grace Kelly, Princess Grace of Monaco since 1956, would come out of retirement to star in it. When Kelly asked Hitchcock to postpone "Marnie" until 1963 or 1964, he recruited Evan Hunter, author of "The Blackboard Jungle" (1954), to develop a screenplay based on a Daphne du Maurier short story, "The Birds" (1952), which Hitchcock had republished in his "My Favorites in Suspense" (1959). He hired Tippi Hedren to play the lead role. It was her first role; she had been a model in New York when Hitchcock saw her, in October 1961, in an NBC television ad for Sego, a diet drink: "I signed her because she is a classic beauty. Movies don't have them any more. Grace Kelly was the last." He insisted, without explanation, that her first name be written in single quotation marks: 'Tippi'.

In "The Birds", Melanie Daniels, a young socialite, meets lawyer Mitch Brenner (Rod Taylor) in a bird shop; Jessica Tandy plays his possessive mother. Hedren visits him in Bodega Bay (where "The Birds" was filmed) carrying a pair of lovebirds as a gift. Suddenly waves of birds start gathering, watching, and attacking. The question: "What do the birds want?" is left unanswered. Hitchcock made the film with equipment from the Revue Studio, which made "Alfred Hitchcock Presents". He said it was his most technically challenging film yet, using a combination of trained and mechanical birds against a backdrop of wild ones. Every shot was sketched in advance.

An HBO/BBC television film, "The Girl" (2012), depicted Hedren's experiences on set; she said that Hitchcock became obsessed with her and sexually harassed her. He reportedly isolated her from the rest of the crew, had her followed, whispered obscenities to her, had her handwriting analysed, and had a ramp built from his private office directly into her trailer. Diane Baker, her co-star in "Marnie", said: "[N]othing could have been more horrible for me than to arrive on that movie set and to see her being treated the way she was." While filming the attack scene in the attic—which took a week to film—she was placed in a caged room while two men wearing elbow-length protective gloves threw live birds at her. Toward the end of the week, to stop the birds flying away from her too soon, one leg of each bird was attached by nylon thread to elastic bands sewn inside her clothes. She broke down after a bird cut her lower eyelid, and filming was halted on doctor's orders.

In June 1962, Grace Kelly announced that she had decided against appearing in "Marnie" (1964). Hedren had signed an exclusive seven-year, $500-a-week contract with Hitchcock in October 1961, and he decided to cast her in the lead role opposite Sean Connery. In 2016, describing Hedren's performance as "one of the greatest in the history of cinema", Richard Brody called the film a "story of sexual violence" inflicted on the character played by Hedren: "The film is, to put it simply, sick, and it's so because Hitchcock was sick. He suffered all his life from furious sexual desire, suffered from the lack of its gratification, suffered from the inability to transform fantasy into reality, and then went ahead and did so virtually, by way of his art." A 1964 "New York Times" film review called it Hitchcock's "most disappointing film in years", citing Hedren's and Connery's lack of experience, an amateurish script and "glaringly fake cardboard backdrops".

In the film, Marnie Edgar (Hedren) steals $10,000 () from her employer and goes on the run. She applies for a job at Mark Rutland's (Connery) company in Philadelphia and steals from there too. Earlier she is shown having a panic attack during a thunderstorm and fearing the colour red. Mark tracks her down and blackmails her into marrying him. She explains that she does not want to be touched, but during the "honeymoon", Mark rapes her. Marnie and Mark discover that Marnie's mother had been a prostitute when Marnie was a child, and that, while the mother was fighting with a client during a thunderstorm—the mother believed the client had tried to molest Marnie—Marnie had killed the client to save her mother. Cured of her fears when she remembers what happened, she decides to stay with Mark.

No longer speaking to her because she had rebuffed him, Hitchcock apparently referred to Hedren throughout as "the girl" rather than by name. He told Robert Burks, the cinematographer, that the camera had to be placed as close as possible to Hedren when he filmed her face. Evan Hunter, the screenwriter of "The Birds" who was writing "Marnie" too, explained to Hitchcock that, if Mark loved Marnie, he would comfort her, not rape her. Hitchcock reportedly replied: "Evan, when he sticks it in her, I want that camera right on her face!" When Hunter submitted two versions of the script, one without the rape scene, Hitchcock replaced him with Jay Presson Allen.

Failing health reduced Hitchcock's output during the last two decades of his life. Biographer Stephen Rebello claimed Universal "forced" two movies on him, "Torn Curtain" (1966) and "Topaz" (1969). Both were spy thrillers with Cold War-related themes. "Torn Curtain", with Paul Newman and Julie Andrews, precipitated the bitter end of the 12-year collaboration between Hitchcock and composer Bernard Herrmann. Hitchcock was unhappy with Herrmann's score and replaced him with John Addison, Jay Livingston and Ray Evans. "Topaz" (1967), based on a Leon Uris novel, is partly set in Cuba. Both films received mixed reviews.

Hitchcock returned to Britain to make his penultimate film, "Frenzy" (1972), based on the novel "Goodbye Piccadilly, Farewell Leicester Square" (1966). After two espionage films, the plot marked a return to the murder-thriller genre. Richard Blaney (Jon Finch), a volatile barman with a history of explosive anger, becomes the prime suspect in the investigation into the "Necktie Murders", which are actually committed by his friend Bob Rusk (Barry Foster). This time, Hitchcock makes the victim and villain kindreds, rather than opposites as in "Strangers on a Train".

In "Frenzy", Hitchcock allowed nudity for the first time. Two scenes show naked women, one of whom is being raped and strangled; Spoto called the latter "one of the most repellent examples of a detailed murder in the history of film". Both actors, Barbara Leigh-Hunt and Anna Massey, refused to do the scenes, so models were used instead. Biographers have noted that Hitchcock had always pushed the limits of film censorship, often managing to fool Joseph Breen, the longtime head of Hollywood's Motion Picture Production Code. Many times Hitchcock slipped in subtle hints of improprieties forbidden by censorship until the mid-1960s. Yet McGilligan wrote that Breen and others often realised that Hitchcock was inserting such things and were actually amused, as well as alarmed by Hitchcock's "inescapable inferences".

"Family Plot" (1976) was Hitchcock's last film. It relates the escapades of "Madam" Blanche Tyler, played by Barbara Harris, a fraudulent spiritualist, and her taxi-driver lover Bruce Dern, making a living from her phony powers. While "Family Plot" was based on the Victor Canning novel "The Rainbird Pattern" (1972), the novel's tone is more sinister. Screenwriter Ernest Lehman originally wrote the film with a dark tone but was pushed to a lighter, more comical tone by Hitchcock.

Toward the end of his life, Hitchcock was working on the script for a spy thriller, "The Short Night", collaborating with James Costigan, Ernest Lehman and David Freeman. Despite preliminary work, it was never filmed. Hitchcock's health was declining and he was worried about his wife, who had suffered a stroke. The screenplay was eventually published in Freeman's book "The Last Days of Alfred Hitchcock" (1999).

Having refused a CBE in 1962, Hitchcock was appointed a Knight Commander of the Most Excellent Order of the British Empire (KBE) in the 1980 New Year Honours. He was too ill to travel to London—he had a pacemaker and was being given cortisone injections for his arthritis—so on 3 January 1980 the British consul general presented him with the papers at Universal Studios. Asked by a reporter after the ceremony why it had taken the Queen so long, Hitchcock quipped, "I suppose it was a matter of carelessness." Cary Grant, Janet Leigh, and others attended a luncheon afterwards.

His last public appearance was on 16 March 1980, when he introduced the next year's winner of the American Film Institute award. He died of kidney failure the following month, on 29 April, in his Bel Air home. Donald Spoto, one of Hitchcock's biographers, wrote that Hitchcock had declined to see a priest, but according to Jesuit priest Mark Henninger, he and another priest, Tom Sullivan, celebrated Mass at the filmmaker's home, and Sullivan heard his confession. Hitchcock was survived by his wife and daughter. His funeral was held at Good Shepherd Catholic Church in Beverly Hills on 30 April, after which his body was cremated. His remains were scattered over the Pacific Ocean on 10 May 1980.

Hitchcock returned several times to cinematic devices such as the audience as voyeur, suspense, the wrong man or woman, and the "MacGuffin," a plot device essential to the characters but irrelevant to the audience.

Hitchcock appears briefly in most of his own films. For example, he is seen struggling to get a double bass onto a train ("Strangers on a Train"), walking dogs out of a pet shop ("The Birds"), fixing a neighbour's clock ("Rear Window"), as a shadow ("Family Plot"), sitting at a table in a photograph ("Dial M for Murder"), and riding a bus ("North by Northwest").

Hitchcock's portrayal of women has been the subject of much scholarly debate. Bidisha wrote in "The Guardian" in 2010: "There's the vamp, the tramp, the snitch, the witch, the slink, the double-crosser and, best of all, the demon mommy. Don't worry, they all get punished in the end." In a widely cited essay in 1975, Laura Mulvey introduced the idea of the male gaze; the view of the spectator in Hitchcock's films, she argued, is that of the heterosexual male protagonist. "The female characters in his films reflected the same qualities over and over again", Roger Ebert wrote in 1996. "They were blonde. They were icy and remote. They were imprisoned in costumes that subtly combined fashion with fetishism. They mesmerised the men, who often had physical or psychological handicaps. Sooner or later, every Hitchcock woman was humiliated."
The victims in "The Lodger" are all blondes. In "The 39 Steps" (1935), Madeleine Carroll is put in handcuffs. Ingrid Bergman, whom Hitchcock directed three times ("Spellbound" (1945), "Notorious" (1946), and "Under Capricorn" (1949)), is dark blonde. In "Rear Window" (1954), Lisa (Grace Kelly) risks her life by breaking into Lars Thorwald's apartment. In "To Catch a Thief" (1955), Francie (Grace Kelly again) offers to help a man she believes is a burglar. In "Vertigo" (1958) and "North by Northwest" (1959) respectively, Kim Novak and Eva Marie Saint play the blonde heroines. In "Psycho" (1960), Janet Leigh's character steals $40,000 () and is murdered by Norman Bates, a reclusive psychopath. Tippi Hedren, a blonde, appears to be the focus of the attacks in "The Birds" (1963). In "Marnie" (1964), the title character, again played by Hedren, is a thief. In "Topaz", French actresses Dany Robin as Stafford's wife and Claude Jade as Stafford's daughter are blonde heroines, the mistress was played by brunette Karin Dor. Hitchcock's last blonde heroine was Barbara Harris as a phony psychic turned amateur sleuth in "Family Plot" (1976), his final film. In the same film, the diamond smuggler played by Karen Black wears a long blonde wig in several scenes.

His films often feature characters struggling in their relationships with their mothers, such as Norman Bates in "Psycho". In "North by Northwest" (1959), Roger Thornhill (Cary Grant) is an innocent man ridiculed by his mother for insisting that shadowy, murderous men are after him. In "The Birds" (1963), the Rod Taylor character, an innocent man, finds his world under attack by vicious birds, and struggles to free himself from a clinging mother (Jessica Tandy). The killer in "Frenzy" (1972) has a loathing of women but idolises his mother. The villain Bruno in "Strangers on a Train" hates his father, but has an incredibly close relationship with his mother (played by Marion Lorne). Sebastian (Claude Rains) in "Notorious" has a clearly conflicting relationship with his mother, who is (rightly) suspicious of his new bride, Alicia Huberman (Ingrid Bergman).

Hitchcock became known for having remarked that "actors are cattle." During the filming of "Mr. & Mrs. Smith" (1941), Carole Lombard brought three cows onto the set wearing the name tags of Lombard, Robert Montgomery, and Gene Raymond, the stars of the film, to surprise him.

Hitchcock believed that actors should concentrate on their performances and leave work on script and character to the directors and screenwriters. He told Bryan Forbes in 1967: "I remember discussing with a method actor how he was taught and so forth. He said, 'We're taught using improvisation. We are given an idea and then we are turned loose to develop in any way we want to.' I said 'That's not acting. That's writing.'" Walter Slezak said that Hitchcock knew the mechanics of acting better than anyone he knew.

Critics observed that, despite his reputation as a man who disliked actors, actors who worked with him often gave brilliant performances. He used the same actors in many of his films; Cary Grant worked with Hitchcock four times, and Ingrid Bergman three. James Mason said that Hitchcock regarded actors as "animated props". For Hitchcock, the actors were part of the film's setting. He told François Truffaut: "The chief requisite for an actor is the ability to do nothing well, which is by no means as easy as it sounds. He should be willing to be used and wholly integrated into the picture by the director and the camera. He must allow the camera to determine the proper emphasis and the most effective dramatic highlights."

Hitchcock planned his scripts in detail with his writers. In "Writing with Hitchcock" (2001), Steven DeRosa noted that Hitchcock supervised them through every draft, asking that they tell the story visually. He told Roger Ebert in 1969:

Hitchcock's films were extensively storyboarded to the finest detail. He was reported to have never even bothered looking through the viewfinder, since he did not need to, although in publicity photos he was shown doing so. He also used this as an excuse to never have to change his films from his initial vision. If a studio asked him to change a film, he would claim that it was already shot in a single way, and that there were no alternative takes to consider.
This view of Hitchcock as a director who relied more on pre-production than on the actual production itself has been challenged by Bill Krohn, the American correspondent of French film magazine "Cahiers du cinéma", in his book "Hitchcock at Work". After investigating script revisions, notes to other production personnel written by or to Hitchcock, and other production material, Krohn observed that Hitchcock's work often deviated from how the screenplay was written or how the film was originally envisioned. He noted that the myth of storyboards in relation to Hitchcock, often regurgitated by generations of commentators on his films, was to a great degree perpetuated by Hitchcock himself or the publicity arm of the studios. For example, the celebrated crop-spraying sequence of "North by Northwest" was not storyboarded at all. After the scene was filmed, the publicity department asked Hitchcock to make storyboards to promote the film, and Hitchcock in turn hired an artist to match the scenes in detail.

Even when storyboards were made, scenes that were shot differed from them significantly. Krohn's analysis of the production of Hitchcock classics like "Notorious" reveals that Hitchcock was flexible enough to change a film's conception during its production. Another example Krohn notes is the American remake of "The Man Who Knew Too Much," whose shooting schedule commenced without a finished script and moreover went over schedule, something that, as Krohn notes, was not an uncommon occurrence on many of Hitchcock's films, including "Strangers on a Train" and "Topaz". While Hitchcock did do a great deal of preparation for all his films, he was fully cognisant that the actual film-making process often deviated from the best-laid plans and was flexible to adapt to the changes and needs of production as his films were not free from the normal hassles faced and common routines used during many other film productions.
Krohn's work also sheds light on Hitchcock's practice of generally shooting in chronological order, which he notes sent many films over budget and over schedule and, more importantly, differed from the standard operating procedure of Hollywood in the Studio System Era. Equally important is Hitchcock's tendency to shoot alternative takes of scenes. This differed from coverage in that the films were not necessarily shot from varying angles so as to give the editor options to shape the film how they choose (often under the producer's aegis). Rather they represented Hitchcock's tendency to give himself options in the editing room, where he would provide advice to his editors after viewing a rough cut of the work. According to Krohn, this and a great deal of other information revealed through his research of Hitchcock's personal papers, script revisions and the like refute the notion of Hitchcock as a director who was always in control of his films, whose vision of his films did not change during production, which Krohn notes has remained the central long-standing myth of Alfred Hitchcock. Both his fastidiousness and attention to detail also found their way into each film poster for his films. Hitchcock preferred to work with the best talent of his day—film poster designers such as Bill Gold and Saul Bass—who would produce posters that accurately represented his films.

Hitchcock was inducted into the Hollywood Walk of Fame on 8 February 1960 with two stars: one for television and a second for his motion pictures. In 1978 John Russell Taylor described him as "the most universally recognizable person in the world" and "a straightforward middle-class Englishman who just happened to be an artistic genius". In 2002 "MovieMaker" named him the most influential director of all time, and a 2007 "The Daily Telegraph" critics' poll ranked him Britain's greatest director. David Gritten, the newspaper's film critic, wrote: "Unquestionably the greatest filmmaker to emerge from these islands, Hitchcock did more than any director to shape modern cinema, which would be utterly different without him. His flair was for narrative, cruelly withholding crucial information (from his characters and from us) and engaging the emotions of the audience like no one else."
He won two Golden Globes, eight Laurel Awards, and five lifetime achievement awards, including the first BAFTA Academy Fellowship Award and, in 1979, an AFI Life Achievement Award. He was nominated five times for an Academy Award for Best Director. "Rebecca", nominated for 11 Oscars, won the Academy Award for Best Picture of 1940; another Hitchcock film, "Foreign Correspondent", was also nominated that year. By 2018, eight of his films had been selected for preservation by the US National Film Registry: "Rebecca" (1940; inducted 2018), "Shadow of a Doubt" (1943; inducted 1991), "Notorious" (1946; inducted 2006), "Rear Window" (1954; inducted 1997), "Vertigo" (1958; inducted 1989), "North by Northwest" (1959; inducted 1995), "Psycho" (1960; inducted 1992), and "The Birds" (1963; inducted 2016).

In 2012 Hitchcock was selected by artist Sir Peter Blake, author of the Beatles' "Sgt. Pepper's Lonely Hearts Club Band" album cover, to appear in a new version of the cover, along with other British cultural figures, and he was featured that year in a BBC Radio 4 series, "The New Elizabethans", as someone "whose actions during the reign of Elizabeth II have had a significant impact on lives in these islands and given the age its character". In June 2013 nine restored versions of Hitchcock's early silent films, including "The Pleasure Garden" (1925), were shown at the Brooklyn Academy of Music's Harvey Theatre; known as "The Hitchcock 9", the travelling tribute was organised by the British Film Institute.

The Alfred Hitchcock Collection is housed at the Academy Film Archive in Hollywood, California. It includes home movies, 16 mm film shot on the set of "Blackmail" (1929) and "Frenzy" (1972), and the earliest known colour footage of Hitchcock. The Academy Film Archive preserves many of his home movies. The Alfred Hitchcock Papers is housed at the Academy's Margaret Herrick Library. The David O. Selznick and the Ernest Lehman collections housed at the Harry Ransom Humanities Research Center in Austin, Texas, contain material related to Hitchcock's work on the production of "The Paradine Case", "Rebecca", "Spellbound", "North by Northwest" and "Family Plot."


Silent films
Sound films

Seven films
Six films
Five films
Four films
Three films
Two films
Many of the British actors additionally appeared in some of the two dozen or so early films Hitchcock worked on in other capacities, such as co-writer, title designer, art director and assistant director. For instance, Clare Greet was also in "Lord Camber's Ladies" (1932), which Hitchcock produced, as was Benita Hume, who had been in "Easy Virtue" (1928).


Biographies
"(chronological)"

Miscellaneous





</doc>
<doc id="809" url="https://en.wikipedia.org/wiki?curid=809" title="Anaconda">
Anaconda

Anacondas or water boas are a group of large snakes of the genus "Eunectes". They are found in tropical South America. Four species are currently recognized.

Although the name applies to a group of snakes, it is often used to refer only to one species, in particular, the common or green anaconda ("Eunectes murinus"), which is the largest snake in the world by weight, and the second longest.

The South American names "anacauchoa" and "anacaona" were suggested in an account by Peter Martyr d'Anghiera, but the idea of a South American origin was questioned by Henry Walter Bates who, in his travels in South America, failed to find any similar name in use. The word anaconda is derived from the name of a snake from Ceylon (Sri Lanka) that John Ray described in Latin in his "Synopsis Methodica Animalium" (1693) as "serpens indicus bubalinus anacandaia zeylonibus, ides bubalorum aliorumque jumentorum membra conterens". Ray used a catalogue of snakes from the Leyden museum supplied by Dr. Tancred Robinson, but the description of its habit was based on Andreas Cleyer who in 1684 described a gigantic snake that crushed large animals by coiling around their bodies and crushing their bones. Henry Yule in his Hobson-Jobson notes that the word became more popular due to a piece of fiction published in 1768 in the Scots Magazine by a certain R. Edwin. Edwin described a 'tiger' being crushed to death by an anaconda, when there actually never were any tigers in Sri Lanka. Yule and Frank Wall noted that the snake was in fact a python and suggested a Tamil origin "anai-kondra" meaning elephant killer. A Sinhalese origin was also suggested by Donald Ferguson who pointed out that the word "Henakandaya" ("hena" lightning/large and "kanda" stem/trunk) was used in Sri Lanka for the small whip snake ("Ahaetulla pulverulenta") and somehow got misapplied to the python before myths were created.

The name commonly used for the anaconda in Brazil is "sucuri", "sucuriju" or "sucuriuba".

The term "anaconda" has been used to refer to:




</doc>
<doc id="824" url="https://en.wikipedia.org/wiki?curid=824" title="Altaic languages">
Altaic languages

Altaic () is a hypothetical language family which would include the Turkic, Mongolian, and Tungusic language families; and possibly also the Japonic, Koreanic, and Ainu languages. Speakers of those languages are currently scattered over most of Asia north of 35 °N and in some eastern parts of Europe, extending in longitude from Turkey to Japan. The group is named after the Altai mountain range in the center of Asia. Most comparative linguists today reject the hypothesis, but it still has supporters.

The Altaic family was first proposed in the 18th century. It was widely accepted until the 1960s, and is still listed in many encyclopedias and handbooks. Since the 1950s, many comparative linguists have rejected the proposal, after supposed cognates were found not to be valid, and Turkic and Mongolic languages were found to be converging rather than diverging over the centuries. Opponents of the theory proposed that the similarities are due to mutual linguistic influences between the groups concerned.

The original hypothesis unified only the Turkic, Mongolian, and Tungusic groups. Later proposals to include the Korean and Japanese languages into a "Macro-Altaic" family have always been controversial. (The original proposal was sometimes called "Micro-Altaic" by retronymy.) Most proponents of Altaic continue to support the inclusion of Korean. A common ancestral Proto-Altaic language for the "Macro" family has been tentatively reconstructed by Sergei Starostin and others.

Micro-Altaic includes about 66 living languages, to which Macro-Altaic would add Korean, Jeju, Japanese and the Ryukyuan languages, for a total of 74 (depending on what is considered a language and what is considered a dialect). These numbers do not include earlier states of languages, such as Middle Mongol, Old Korean or Old Japanese.

The earliest known texts in a Turkic language are the Orkhon inscriptions, 720–735 AD. They were deciphered in 1893 by the Danish linguist Vilhelm Thomsen in a scholarly race with his rival, the German–Russian linguist Wilhelm Radloff. However, Radloff was the first to publish the inscriptions.

The first Tungusic language to be attested is Jurchen, the language of the ancestors of the Manchus. A writing system for it was devised in 1119 AD and an inscription using this system is known from 1185 (see List of Jurchen inscriptions).

The earliest Mongolic language of which we have written evidence is known as Middle Mongol. It is first attested by an inscription dated to 1224 or 1225 AD, the Stele of Yisüngge, and by the "Secret History of the Mongols", written in 1228 (see Mongolic languages). The earliest Para-Mongolic text is the Memorial for Yelü Yanning, written in the Khitan large script and dated to 986 AD. However, the Inscription of Hüis Tolgoi, discovered in 1975 and analysed as being in an early form of Mongolic, has been dated to 604-620 AD. The Bugut inscription dates back to 584 AD.

Japanese is first attested in the form of names contained in a few short inscriptions in Classical Chinese from the 5th century AD, such as found on the Inariyama Sword. The first substantial text in Japanese, however, is the Kojiki, which dates from 712 AD. It is followed by the Nihon shoki, completed in 720, and that by the Man'yōshū, which dates from c. 771–785, but includes material that is from about 400 years earlier.

The most important text for the study of early Korean is the Hyangga, a collection of 25 poems, of which some go back to the Three Kingdoms period (57 BC–668 AD), but are preserved in an orthography that only goes back to the 9th century AD. Korean is copiously attested from the mid-15th century on in the phonetically precise Hangul system of writing.

A proposed grouping of the Turkic, Mongolic, and Tungusic languages was published in 1730 by Philip Johan von Strahlenberg, a Swedish officer who traveled in the eastern Russian Empire while a prisoner of war after the Great Northern War. However, he may not have intended to imply a closer relationship among those languages.

In 1844, the Finnish philologist Matthias Castrén proposed a broader grouping, that later came to be called the Ural–Altaic family, which included Turkic, Mongolian, and Manchu-Tungus (=Tungusic) as an "Altaic" branch, and also the Finno-Ugric and Samoyedic languages as the "Uralic" branch (though Castrén himself used the terms "Tataric" and "Chudic"). The name "Altaic" referred to the Altai Mountains in East-Central Asia, which are approximately the center of the geographic range of the three main families. The name "Uralic" referred to the Ural Mountains.

While the Ural-Altaic family hypothesis can still be found in some encyclopedias, atlases, and similar general references, after the 1960s it has been heavily criticized. Even linguists who accept the basic Altaic family, like Sergei Starostin, completely discard the inclusion of the "Uralic" branch.

In 1857, the Austrian scholar Anton Boller suggested adding Japanese to the Ural–Altaic family.

In the 1920s, G.J. Ramstedt and E.D. Polivanov advocated the inclusion of Korean. Decades later, in his 1952 book, Ramstedt rejected the Ural–Altaic hypothesis but again included Korean in Altaic, an inclusion followed by most leading Altaicists (supporters of the theory) to date. His book contained the first comprehensive attempt to identify regular correspondences among the sound systems within the Altaic language families.

In 1960, Nicholas Poppe published what was in effect a heavily revised version of Ramstedt's volume on phonology that has since set the standard in Altaic studies. Poppe considered the issue of the relationship of Korean to Turkic-Mongolic-Tungusic not settled. In his view, there were three possibilities: (1) Korean did not belong with the other three genealogically, but had been influenced by an Altaic substratum; (2) Korean was related to the other three at the same level they were related to each other; (3) Korean had split off from the other three before they underwent a series of characteristic changes.

Roy Andrew Miller's 1971 book "Japanese and the Other Altaic Languages" convinced most Altaicists that Japanese also belonged to Altaic. Since then, the "Macro-Altaic" has been generally assumed to include Turkic, Mongolic, Tungusic, Korean, and Japanese.

In 1990, Unger advocated a family consisting of Tungusic, Korean, and Japonic languages, but not Turkic or Mongolic.

However, many linguists dispute the alleged affinities of Korean and Japanese to the other three groups. Some authors instead tried to connect Japanese to the Austronesian languages.

In 2017 Martine Robbeets proposed that Japanese (and possibly Korean) originated as a hybrid language. She proposed that the ancestral home of the Turkic, Mongolic, and Tungusic languages was somewhere in northwestern Manchuria. A group of those proto-Altaic ("Transeurasian") speakers would have migrated south into the modern Liaoning province, where they would have been mostly assimilated by an agricultural community with an Austronesian-like language. The fusion of the two languages would have resulted in proto-Japanese and proto-Korean.

In 1962 John C. Street proposed an alternative classification, with Turkic-Mongolic-Tungusic in one grouping and Korean-Japanese-Ainu in another, joined in what he designated as the "North Asiatic" family. The inclusion of Ainu was adopted also by James Patrie in 1982.

The Turkic-Mongolic-Tungusic and Korean-Japanese-Ainu groupings were also posited in 2000–2002 by Joseph Greenberg. However, he treated them as independent members of a larger family, which he termed Eurasiatic.

The inclusion of Ainu is not widely accepted by Altaicists. In fact, no convincing genealogical relationship between Ainu and any other language family has been demonstrated, and it is generally regarded as a language isolate.

Starting in the late 1950s, some linguists became increasingly critical of even the minimal Altaic family hypothesis, disputing the alleged evidence of genetic connection between Turkic, Mongolic and Tungusic languages.

Among the earlier critics were Gerard Clauson (1956), Gerhard Doerfer (1963), and Alexander Shcherbak. They claimed that the words and features shared by Turkic, Mongolic, and Tungusic languages were for the most part borrowings and that the rest could be attributed to chance resemblances. In 1988, Doerfer again rejected all the genetic claims over these major groups.

A major continuing supporter of the Altaic hypothesis has been S. Starostin, who published a comparative lexical analysis of the Altaic languages in (1991). He concluded that the analysis supported the Altaic grouping, although it was "older than most other language families in Eurasia, such as Indo-European or Finno-Ugric, and this is the reason why the modern Altaic languages preserve few common elements".

In 1991 and again in 1996, Roy Miller defended the Altaic hypothesis and claimed that the criticisms of Clauson and Doerfer apply exclusively to the lexical correspondences, whereas the most pressing evidence for the theory is the similarities in verbal morphology.

In 2003, Claus Schönig published a critical overview of the history of the Altaic hypothesis up to that time, siding with the earlier criticisms of Clauson, Doerfer, and Shcherbak.

In 2003, Starostin, Anna Dybo and Oleg Mudrak published the "Etymological Dictionary of the Altaic Languages", which expanded the 1991 lexical lists and added other phonological and grammatical arguments.

Starostin's book was criticized by Stefan Georg in 2004 and 2005, and by Alexander Vovin in 2005.

Other defenses of the theory, in response to the criticisms of Georg and Vovin, were published by Starostin in 2005, Blažek in 2006, Robbeets in 2007, and Dybo and G. Starostin in 2008

In 2010, Lars Johanson echoed Miller's 1996 rebuttal to the critics, and called for a muting of the polemic.

The list below comprises linguists who have worked specifically on the Altaic problem since the publication of the first volume of Ramstedt's "Einführung" in 1952. The dates given are those of works concerning Altaic. For supporters of the theory, the version of Altaic they favor is given at the end of the entry, if other than the prevailing one of Turkic–Mongolic–Tungusic–Korean–Japanese.




The original arguments for grouping the "micro-Altaic" languages within a Uralo-Altaic family were based on such shared features as vowel harmony and agglutination.

According to Roy Miller, the most pressing evidence for the theory is the similarities in verbal morphology.

The "Etymological Dictionary" by Starostin and others (2003) proposes a set of sound change laws that would explain the evolution from Proto-Altaic to the descendant languages. For example, although most of today's Altaic languages have vowel harmony, Proto-Altaic as reconstructed by them lacked it; instead, various vowel assimilations between the first and second syllables of words occurred in Turkic, Mongolic, Tungusic, Korean, and Japonic. They also included a number of grammatical correspondences between the languages.

Starostin claimed in 1991 that the members of the proposed Altaic group shared about 15–20% of apparent cognates within a 110-word Swadesh-Yakhontov list; in particular, Turkic–Mongolic 20%, Turkic–Tungusic 18%, Turkic–Korean 17%, Mongolic–Tungusic 22%, Mongolic–Korean 16%, and Tungusic–Korean 21%. The 2003 "Etymological Dictionary" includes a list of 2,800 proposed cognate sets, as well as a few important changes to the reconstruction of Proto-Altaic. The authors tried hard to distinguish loans between Turkic and Mongolic and between Mongolic and Tungusic from cognates; and suggest words that occur in Turkic and Tungusic but not in Mongolic. All other combinations between the five branches also occur in the book. It lists 144 items of shared basic vocabulary, including words for such items as 'eye', 'ear', 'neck', 'bone', 'blood', 'water', 'stone', 'sun', and 'two'.

According to G. Clauson (1956), G. Doerfer (1963), and A. Shcherbak (1963), many of the typological features of the supposed Altaic languages, such as agglutinative morphology and subject–object–verb (SOV) word order, usually occur together in languages.

Those critics also argued that the words and features shared by Turkic, Mongolic, and Tungusic languages were for the most part borrowings and that the rest could be attributed to chance resemblances. They noted that there was little vocabulary shared by Turkic and Tungusic languages, though more shared with Mongolic languages. They reasoned that, if all three families had a common ancestor, we should expect losses to happen at random, and not only at the geographical margins of the family; and that the observed pattern is consistent with borrowing.

According to C. Schönig (2003), after accounting for areal effects, the shared lexicon that could have a common genetic origin was reduced to a small number of monosyllabic lexical roots, including the personal pronouns and a few other deictic and auxiliary items, whose sharing could be explained in other ways; not the kind of sharing expected in cases of genetic relationship.

Instead of a common genetic origin, Clauson, Doerfer, and Shcherbak proposed (in 1956-1966) that Turkic, Mongolic, and Tungusic languages form a "Sprachbund": a set of languages with similarities due to convergence through intensive borrowing and long contact, rather than common origin.

Asya Pereltsvaig further observed in 2011 that, in general, genetically related languages and families tend to diverge over time: the earlier forms are more similar than modern forms. However, she claims that an analysis of the earliest written records of Mongolic and Turkic languages shows the opposite, suggesting that they do not share a common traceable ancestor, but rather have become more similar through language contact and areal effects.

The prehistory of the peoples speaking the "Altaic" languages is largely unknown. Whereas for certain other language families, such as the speakers of Indo-European, Uralic, and Austronesian, it is possible to frame substantial hypotheses, in the case of the proposed Altaic family much remains to be done.
Some scholars have conjectured a possible Uralic and Altaic homeland in the Central Asian steppes.

According to Juha Janhunen, the ancestral languages of Turkic, Mongolic, Tungusic, Korean, and Japanese were spoken in a relatively small area comprising present-day North Korea, Southern Manchuria, and Southeastern Mongolia. However Janhunen is sceptical about an affiliation of Japanese to Altaic, while András Róna-Tas remarked that a relationship between Altaic and Japanese, if it ever existed, must be more remote than the relationship of any two of the Indo-European languages. Ramsey stated that "the genetic relationship between Korean and Japanese, if it in fact exists, is probably more complex and distant than we can imagine on the basis of our present state of knowledge".

Supporters of the Altaic hypothesis formerly set the date of the Proto-Altaic language at around 4000 BC, but today at around 5000 BC or 6000 BC. This would make Altaic a language family about as old as Indo-European (4000 to 7,000 BC according to several hypotheses) but considerably younger than Afroasiatic (c. 10,000 BC or 11,000 to 16,000 BC according to different sources).




</doc>
<doc id="825" url="https://en.wikipedia.org/wiki?curid=825" title="Austrian German">
Austrian German

Austrian German (), Austrian Standard German (ASG), Standard Austrian German (), or Austrian High German (), is the variety of Standard German written and spoken in Austria. It has the highest sociolinguistic prestige locally, as it is the variation used in the media and for other formal situations. In less formal situations, Austrians tend to use forms closer to or identical with the Bavarian and Alemannic dialects, traditionally spoken – but rarely written – in Austria.

Austrian German has its beginning in the mid-18th century, when empress Maria Theresa and her son Joseph II introduced compulsory schooling (in 1774) and several reforms of administration in their multilingual Habsburg empire. At the time, the written standard was "Oberdeutsche Schreibsprache", which was highly influenced by the Bavarian and Alemannic dialects of Austria. Another option was to create a new standard based on the Southern German dialects, as proposed by the linguist Johann Siegmund Popowitsch. Instead they decided for pragmatic reasons to adopt the already standardized Chancellery language of Saxony ("Sächsische Kanzleisprache" or "Meißner Kanzleideutsch"), which was based on the administrative language of the non-Austrian area of Meißen and Dresden.
Thus Standard Austrian German has the same geographic origin as the German Standard German ("Bundesdeutsches Hochdeutsch") and Swiss High German ("Schweizer Hochdeutsch", not to be confused with the Alemannic Swiss German dialects).

The process of introducing the new written standard was led by Joseph von Sonnenfels.
Since 1951 the standardized form of Austrian German for official texts and schools has been defined by the "Austrian Dictionary" (""), published under the authority of the Austrian Federal Ministry of Education, Arts and Culture.

As German is a pluricentric language, Austrian German is one among several varieties of Standard German. Much like the relationship between British English and American English, the German varieties differ in minor respects (e.g., spelling, word usage and grammar) but are recognizably equivalent and largely mutually intelligible.

The official Austrian dictionary, "das Österreichische Wörterbuch", prescribes grammatical and spelling rules defining the official language. 

Austrian delegates participated in the international working group that drafted the German spelling reform of 1996—several conferences leading up to the reform were hosted in Vienna at the invitation of the Austrian federal government—and adopted it as a signatory, along with Germany, Switzerland, and Liechtenstein, of an international memorandum of understanding (Wiener Absichtserklärung) signed in Vienna in 1996.

The "sharp s" (ß) is used in Austria, as in Germany.

Because of the German language's pluricentric nature, German dialects in Austria should not be confused with the variety of Standard German spoken by most Austrians, which is distinct from that of Germany or Switzerland. 

Distinctions in vocabulary persist, for example, in culinary terms, where communication with Germans is frequently difficult, and administrative and legal language, which is due to Austria's exclusion from the development of a German nation-state in the late 19th century and its manifold particular traditions. A comprehensive collection of Austrian-German legal, administrative and economic terms is offered in "Markhardt, Heidemarie: Wörterbuch der österreichischen Rechts-, Wirtschafts- und Verwaltungsterminologie" (Peter Lang, 2006).

The former standard, used for about 300 years or more in speech in refined language, was the ', a sociolect spoken by the imperial Habsburg family and the nobility of Austria-Hungary. It differed from other dialects in vocabulary and pronunciation; it appears to have been spoken with a slight degree of nasality. This was not a standard in a modern technical sense, as it was just the social standard of upper-class speech.

For many years, Austria had a special form of the language for official government documents. This form is known as "", or "Austrian chancellery language". It is a very traditional form of the language, probably derived from medieval deeds and documents, and has a very complex structure and vocabulary generally reserved for such documents. For most speakers (even native speakers), this form of the language is generally difficult to understand, as it contains many highly specialised terms for diplomatic, internal, official, and military matters. There are no regional variations, because this special written form has mainly been used by a government that has now for centuries been based in Vienna.

' is now used less and less, thanks to various administrative reforms that reduced the number of traditional civil servants ('). As a result, Standard German is replacing it in government and administrative texts.

When Austria became a member of the European Union, 23 food-related terms were listed in its accession agreement as having the same legal status as the equivalent terms used in Germany. Austrian German is the only variety of a pluricentric language recognized under international law or EU primary law.

In Austria, as in the German-speaking parts of Switzerland and in southern Germany, verbs that express a state tend to use "" as the auxiliary verb in the perfect, as well as verbs of movement. Verbs which fall into this category include "sitzen" (to sit), "liegen" (to lie) and, in parts of Carinthia, "schlafen" (to sleep). Therefore, the perfect of these verbs would be "ich bin gesessen", "ich bin gelegen" and "ich bin geschlafen" respectively (note: "ich bin geschlafen" is a rarely used form, more commonly "ich habe geschlafen" is used).

In Germany, the words "stehen" (to stand) and "gestehen" (to confess) are identical in the present perfect: "habe gestanden". The Austrian variant avoids this potential ambiguity ("bin gestanden" from "stehen", "to stand"; and "habe gestanden" from "gestehen", "to confess", e.g. ""der Verbrecher ist vor dem Richter gestanden und hat gestanden"").

In addition, the preterite (simple past) is very rarely used in Austria, especially in the spoken language, with the exception of some modal verbs (i.e. "ich sollte", "ich wollte").

There are many official terms that differ in Austrian German from their usage in most parts of Germany. Words primarily used in Austria are "Jänner" (January) rather than "Januar", "Feber" (February) rather than "Februar", "heuer" (this year) rather than "dieses Jahr", "Stiege" (stairs) instead of "Treppe", "Rauchfang" (chimney) instead of "Schornstein", many administrative, legal and political terms, and many food terms, including the following:

There are, however, some false friends between the two regional varieties:


In addition to the standard variety, in everyday life most Austrians speak one of a number of Upper German dialects.

While strong forms of the various dialects are not fully mutually intelligible to northern Germans, communication is much easier in Bavaria, especially rural areas, where the Bavarian dialect still predominates as the mother tongue. The Central Austro-Bavarian dialects are more intelligible to speakers of Standard German than the Southern Austro-Bavarian dialects of Tyrol.

Viennese, the Austro-Bavarian dialect of Vienna, is seen for many in Germany as quintessentially Austrian. The people of Graz, the capital of Styria, speak yet another dialect which is not very Styrian and more easily understood by people from other parts of Austria than other Styrian dialects, for example from western Styria.

Simple words in the various dialects are very similar, but pronunciation is distinct for each and, after listening to a few spoken words, it may be possible for an Austrian to realise which dialect is being spoken. However, in regard to the dialects of the deeper valleys of the Tirol, other Tyroleans are often unable to understand them. Speakers from the different states of Austria can easily be distinguished from each other by their particular accents (probably more so than Bavarians), those of Carinthia, Styria, Vienna, Upper Austria, and the Tyrol being very characteristic. Speakers from those regions, even those speaking Standard German, can usually be easily identified by their accent, even by an untrained listener.

Several of the dialects have been influenced by contact with non-Germanic linguistic groups, such as the dialect of Carinthia, where in the past many speakers were bilingual with Slovene, and the dialect of Vienna, which has been influenced by immigration during the Austro-Hungarian period, particularly from what is today the Czech Republic. The German dialects of South Tyrol have been influenced by local Romance languages, particularly noticeable with the many loanwords from Italian and Ladin.

The geographic borderlines between the different accents (isoglosses) coincide strongly with the borders of the states and also with the border with Bavaria, with Bavarians having a markedly different rhythm of speech in spite of the linguistic similarities.

Sources




</doc>
<doc id="840" url="https://en.wikipedia.org/wiki?curid=840" title="Axiom of choice">
Axiom of choice

In mathematics, the axiom of choice, or AC, is an axiom of set theory equivalent to the statement that "a Cartesian product of a collection of non-empty sets is non-empty". Informally put, the axiom of choice says that given any collection of bins, each containing at least one object, it is possible to make a selection of exactly one object from each bin, even if the collection is infinite. Formally, it states that for every indexed family formula_1 of nonempty sets there exists an indexed family formula_2 of elements such that formula_3 for every formula_4. The axiom of choice was formulated in 1904 by Ernst Zermelo in order to formalize his proof of the well-ordering theorem.

In many cases, such a selection can be made without invoking the axiom of choice; this is in particular the case if the number of sets is finite, or if a selection rule is available – some distinguishing property that happens to hold for exactly one element in each set. An illustrative example is sets picked from the natural numbers. From such sets, one may always select the smallest number, e.g. in <nowiki></nowiki> the smallest elements are {4, 10, 1}. In this case, "select the smallest number" is a choice function. Even if infinitely many sets were collected from the natural numbers, it will always be possible to choose the smallest element from each set to produce a set. That is, the choice function provides the set of chosen elements. However, no choice function is known for the collection of all non-empty subsets of the real numbers (if there are non-constructible reals). In that case, the axiom of choice must be invoked.

Bertrand Russell coined an analogy: for any (even infinite) collection of pairs of shoes, one can pick out the left shoe from each pair to obtain an appropriate selection; this makes it possible to directly define a choice function. For an "infinite" collection of pairs of socks (assumed to have no distinguishing features), there is no obvious way to make a function that selects one sock from each pair, without invoking the axiom of choice.

Although originally controversial, the axiom of choice is now used without reservation by most mathematicians, and it is included in the standard form of axiomatic set theory, Zermelo–Fraenkel set theory with the axiom of choice (ZFC). One motivation for this use is that a number of generally accepted mathematical results, such as Tychonoff's theorem, require the axiom of choice for their proofs. Contemporary set theorists also study axioms that are not compatible with the axiom of choice, such as the axiom of determinacy. The axiom of choice is avoided in some varieties of constructive mathematics, although there are varieties of constructive mathematics in which the axiom of choice is embraced.

A choice function is a function "f", defined on a collection "X" of nonempty sets, such that for every set "A" in "X", "f"("A") is an element of "A". With this concept, the axiom can be stated:
Formally, this may be expressed as follows:

Thus, the negation of the axiom of choice states that there exists a collection of nonempty sets that has no choice function.

Each choice function on a collection "X" of nonempty sets is an element of the Cartesian product of the sets in "X". This is not the most general situation of a Cartesian product of a family of sets, where a given set can occur more than once as a factor; however, one can focus on elements of such a product that select the same element every time a given set appears as factor, and such elements correspond to an element of the Cartesian product of all "distinct" sets in the family. The axiom of choice asserts the existence of such elements; it is therefore equivalent to:

In this article and other discussions of the Axiom of Choice the following abbreviations are common:

There are many other equivalent statements of the axiom of choice. These are equivalent in the sense that, in the presence of other basic axioms of set theory, they imply the axiom of choice and are implied by it.

One variation avoids the use of choice functions by, in effect, replacing each choice function with its range.
This guarantees for any partition of a set "X" the existence of a subset "C" of "X" containing exactly one element from each part of the partition.

Another equivalent axiom only considers collections "X" that are essentially powersets of other sets:
Authors who use this formulation often speak of the "choice function on A", but be advised that this is a slightly different notion of choice function. Its domain is the powerset of "A" (with the empty set removed), and so makes sense for any set "A", whereas with the definition used elsewhere in this article, the domain of a choice function on a "collection of sets" is that collection, and so only makes sense for sets of sets. With this alternate notion of choice function, the axiom of choice can be compactly stated as
which is equivalent to
The negation of the axiom can thus be expressed as:

The statement of the axiom of choice does not specify whether the collection of nonempty sets is finite or infinite, and thus implies that every finite collection of nonempty sets has a choice function. However, that particular case is a theorem of the Zermelo–Fraenkel set theory without the axiom of choice (ZF); it is easily proved by mathematical induction. In the even simpler case of a collection of "one" set, a choice function just corresponds to an element, so this instance of the axiom of choice says that every nonempty set has an element; this holds trivially. The axiom of choice can be seen as asserting the generalization of this property, already evident for finite collections, to arbitrary collections.

Until the late 19th century, the axiom of choice was often used implicitly, although it had not yet been formally stated. For example, after having established that the set "X" contains only non-empty sets, a mathematician might have said "let "F(s)" be one of the members of "s" for all "s" in "X"" to define a function "F". In general, it is impossible to prove that "F" exists without the axiom of choice, but this seems to have gone unnoticed until Zermelo.

Not every situation requires the axiom of choice. For finite sets "X", the axiom of choice follows from the other axioms of set theory. In that case it is equivalent to saying that if we have several (a finite number of) boxes, each containing at least one item, then we can choose exactly one item from each box. Clearly we can do this: We start at the first box, choose an item; go to the second box, choose an item; and so on. The number of boxes is finite, so eventually our choice procedure comes to an end. The result is an explicit choice function: a function that takes the first box to the first element we chose, the second box to the second element we chose, and so on. (A formal proof for all finite sets would use the principle of mathematical induction to prove "for every natural number "k", every family of "k" nonempty sets has a choice function.") This method cannot, however, be used to show that every countable family of nonempty sets has a choice function, as is asserted by the axiom of countable choice. If the method is applied to an infinite sequence ("X" : "i"∈ω) of nonempty sets, a function is obtained at each finite stage, but there is no stage at which a choice function for the entire family is constructed, and no "limiting" choice function can be constructed, in general, in ZF without the axiom of choice.

The nature of the individual nonempty sets in the collection may make it possible to avoid the axiom of choice even for certain infinite collections. For example, suppose that each member of the collection "X" is a nonempty subset of the natural numbers. Every such subset has a smallest element, so to specify our choice function we can simply say that it maps each set to the least element of that set. This gives us a definite choice of an element from each set, and makes it unnecessary to apply the axiom of choice.

The difficulty appears when there is no natural choice of elements from each set. If we cannot make explicit choices, how do we know that our set exists? For example, suppose that "X" is the set of all non-empty subsets of the real numbers. First we might try to proceed as if "X" were finite. If we try to choose an element from each set, then, because "X" is infinite, our choice procedure will never come to an end, and consequently, we shall never be able to produce a choice function for all of "X". Next we might try specifying the least element from each set. But some subsets of the real numbers do not have least elements. For example, the open interval (0,1) does not have a least element: if "x" is in (0,1), then so is "x"/2, and "x"/2 is always strictly smaller than "x". So this attempt also fails.

Additionally, consider for instance the unit circle "S", and the action on "S" by a group "G" consisting of all rational rotations. Namely, these are rotations by angles which are rational multiples of "π". Here "G" is countable while "S" is uncountable. Hence "S" breaks up into uncountably many orbits under "G". Using the axiom of choice, we could pick a single point from each orbit, obtaining an uncountable subset "X" of "S" with the property that all of its translates by G are disjoint from "X". The set of those translates partitions the circle into a countable collection of disjoint sets, which are all pairwise congruent. Since "X" is not measurable for any rotation-invariant countably additive finite measure on "S", finding an algorithm to select a point in each orbit requires the axiom of choice. See non-measurable set for more details.

The reason that we are able to choose least elements from subsets of the natural numbers is the fact that the natural numbers are well-ordered: every nonempty subset of the natural numbers has a unique least element under the natural ordering. One might say, "Even though the usual ordering of the real numbers does not work, it may be possible to find a different ordering of the real numbers which is a well-ordering. Then our choice function can choose the least element of every set under our unusual ordering." The problem then becomes that of constructing a well-ordering, which turns out to require the axiom of choice for its existence; every set can be well-ordered if and only if the axiom of choice holds.

A proof requiring the axiom of choice may establish the existence of an object without explicitly defining the object in the language of set theory. For example, while the axiom of choice implies that there is a well-ordering of the real numbers, there are models of set theory with the axiom of choice in which no well-ordering of the reals is definable. Similarly, although a subset of the real numbers that is not Lebesgue measurable can be proved to exist using the axiom of choice, it is consistent that no such set is definable.

The axiom of choice proves the existence of these intangibles (objects that are proved to exist, but which cannot be explicitly constructed), which may conflict with some philosophical principles. Because there is no canonical well-ordering of all sets, a construction that relies on a well-ordering may not produce a canonical result, even if a canonical result is desired (as is often the case in category theory). This has been used as an argument against the use of the axiom of choice.

Another argument against the axiom of choice is that it implies the existence of objects that may seem counterintuitive. One example is the Banach–Tarski paradox which says that it is possible to decompose the 3-dimensional solid unit ball into finitely many pieces and, using only rotations and translations, reassemble the pieces into two solid balls each with the same volume as the original. The pieces in this decomposition, constructed using the axiom of choice, are non-measurable sets.

Despite these seemingly paradoxical facts, most mathematicians accept the axiom of choice as a valid principle for proving new results in mathematics. The debate is interesting enough, however, that it is considered of note when a theorem in ZFC (ZF plus AC) is logically equivalent (with just the ZF axioms) to the axiom of choice, and mathematicians look for results that require the axiom of choice to be false, though this type of deduction is less common than the type which requires the axiom of choice to be true.

It is possible to prove many theorems using neither the axiom of choice nor its negation; such statements will be true in any model of ZF, regardless of the truth or falsity of the axiom of choice in that particular model. The restriction to ZF renders any claim that relies on either the axiom of choice or its negation unprovable. For example, the Banach–Tarski paradox is neither provable nor disprovable from ZF alone: it is impossible to construct the required decomposition of the unit ball in ZF, but also impossible to prove there is no such decomposition. Similarly, all the statements listed below which require choice or some weaker version thereof for their proof are unprovable in ZF, but since each is provable in ZF plus the axiom of choice, there are models of ZF in which each statement is true. Statements such as the Banach–Tarski paradox can be rephrased as conditional statements, for example, "If AC holds, then the decomposition in the Banach–Tarski paradox exists." Such conditional statements are provable in ZF when the original statements are provable from ZF and the axiom of choice.

As discussed above, in ZFC, the axiom of choice is able to provide "nonconstructive proofs" in which the existence of an object is proved although no explicit example is constructed. ZFC, however, is still formalized in classical logic. The axiom of choice has also been thoroughly studied in the context of constructive mathematics, where non-classical logic is employed. The status of the axiom of choice varies between different varieties of constructive mathematics.

In Martin-Löf type theory and higher-order Heyting arithmetic, the appropriate statement of the axiom of choice is (depending on approach) included as an axiom or provable as a theorem. Errett Bishop argued that the axiom of choice was constructively acceptable, saying

In constructive set theory, however, Diaconescu's theorem shows that the axiom of choice implies the law of excluded middle (unlike in Martin-Löf type theory, where it does not). Thus the axiom of choice is not generally available in constructive set theory. A cause for this difference is that the axiom of choice in type theory does not have the extensionality properties that the axiom of choice in constructive set theory does.

Some results in constructive set theory use the axiom of countable choice or the axiom of dependent choice, which do not imply the law of the excluded middle in constructive set theory. Although the axiom of countable choice in particular is commonly used in constructive mathematics, its use has also been questioned.

In 1938, Kurt Gödel showed that the "negation" of the axiom of choice is not a theorem of ZF by constructing an inner model (the constructible universe) which satisfies ZFC and thus showing that ZFC is consistent if ZF itself is consistent. In 1963, Paul Cohen employed the technique of forcing, developed for this purpose, to show that, assuming ZF is consistent, the axiom of choice itself is not a theorem of ZF. He did this by constructing a much more complex model which satisfies ZF¬C (ZF with the negation of AC added as axiom) and thus showing that ZF¬C is consistent.

Together these results establish that the axiom of choice is logically independent of ZF. The assumption that ZF is consistent is harmless because adding another axiom to an already inconsistent system cannot make the situation worse. Because of independence, the decision whether to use the axiom of choice (or its negation) in a proof cannot be made by appeal to other axioms of set theory. The decision must be made on other grounds.

One argument given in favor of using the axiom of choice is that it is convenient to use it because it allows one to prove some simplifying propositions that otherwise could not be proved. Many theorems which are provable using choice are of an elegant general character: every ideal in a ring is contained in a maximal ideal, every vector space has a basis, and every product of compact spaces is compact. Without the axiom of choice, these theorems may not hold for mathematical objects of large cardinality.

The proof of the independence result also shows that a wide class of mathematical statements, including all statements that can be phrased in the language of Peano arithmetic, are provable in ZF if and only if they are provable in ZFC. Statements in this class include the statement that P = NP, the Riemann hypothesis, and many other unsolved mathematical problems. When one attempts to solve problems in this class, it makes no difference whether ZF or ZFC is employed if the only question is the existence of a proof. It is possible, however, that there is a shorter proof of a theorem from ZFC than from ZF.

The axiom of choice is not the only significant statement which is independent of ZF. For example, the generalized continuum hypothesis (GCH) is not only independent of ZF, but also independent of ZFC. However, ZF plus GCH implies AC, making GCH a strictly stronger claim than AC, even though they are both independent of ZF.

The axiom of constructibility and the generalized continuum hypothesis each imply the axiom of choice and so are strictly stronger than it. In class theories such as Von Neumann–Bernays–Gödel set theory and Morse–Kelley set theory, there is an axiom called the axiom of global choice that is stronger than the axiom of choice for sets because it also applies to proper classes. The axiom of global choice follows from the axiom of limitation of size.

There are important statements that, assuming the axioms of ZF but neither AC nor ¬AC, are equivalent to the axiom of choice. The most important among them are Zorn's lemma and the well-ordering theorem. In fact, Zermelo initially introduced the axiom of choice in order to formalize his proof of the well-ordering theorem.


There are several results in category theory which invoke the axiom of choice for their proof. These results might be weaker than, equivalent to, or stronger than the axiom of choice, depending on the strength of the technical foundations. For example, if one defines categories in terms of sets, that is, as sets of objects and morphisms (usually called a small category), or even locally small categories, whose hom-objects are sets, then there is no category of all sets, and so it is difficult for a category-theoretic formulation to apply to all sets. On the other hand, other foundational descriptions of category theory are considerably stronger, and an identical category-theoretic statement of choice may be stronger than the standard formulation, à la class theory, mentioned above.

Examples of category-theoretic statements which require choice include:

There are several weaker statements that are not equivalent to the axiom of choice, but are closely related. One example is the axiom of dependent choice (DC). A still weaker example is the axiom of countable choice (AC or CC), which states that a choice function exists for any countable set of nonempty sets. These axioms are sufficient for many proofs in elementary mathematical analysis, and are consistent with some principles, such as the Lebesgue measurability of all sets of reals, that are disprovable from the full axiom of choice.

Other choice axioms weaker than axiom of choice include the Boolean prime ideal theorem and the axiom of uniformization. The former is equivalent in ZF to the existence of an ultrafilter containing each given filter, proved by Tarski in 1930.

One of the most interesting aspects of the axiom of choice is the large number of places in mathematics that it shows up. Here are some statements that require the axiom of choice in the sense that they are not provable from ZF but are provable from ZFC (ZF plus AC). Equivalently, these statements are true in all models of ZFC but false in some models of ZF.


There are several historically important set-theoretic statements implied by AC whose equivalence to AC is open. The partition principle, which was formulated before AC itself, was cited by Zermelo as a justification for believing AC. In 1906 Russell declared PP to be equivalent, but whether the Partition Principle implies AC is still the oldest open problem in set theory, and the equivalences of the other statements are similarly hard old open problems. In every "known" model of ZF where choice fails, these statements fail too, but it is unknown if they can hold without choice.


Now, consider stronger forms of the negation of AC. For example, if we abbreviate by BP the claim that every set of real numbers has the property of Baire, then BP is stronger than ¬AC, which asserts the nonexistence of any choice function on perhaps only a single set of nonempty sets. Strengthened negations may be compatible with weakened forms of AC. For example, ZF + DC + BP is consistent, if ZF is.

It is also consistent with ZF + DC that every set of reals is Lebesgue measurable; however, this consistency result, due to Robert M. Solovay, cannot be proved in ZFC itself, but requires a mild large cardinal assumption (the existence of an inaccessible cardinal). The much stronger axiom of determinacy, or AD, implies that every set of reals is Lebesgue measurable, has the property of Baire, and has the perfect set property (all three of these results are refuted by AC itself). ZF + DC + AD is consistent provided that a sufficiently strong large cardinal axiom is consistent (the existence of infinitely many Woodin cardinals).

Quine's system of axiomatic set theory, "New Foundations" (NF), takes its name from the title (“New Foundations for Mathematical Logic”) of the 1937 article which introduced it. In the NF axiomatic system, the axiom of choice can be disproved.

There are models of Zermelo-Fraenkel set theory in which the axiom of choice is false. We shall abbreviate "Zermelo-Fraenkel set theory plus the negation of the axiom of choice" by ZF¬C. For certain models of ZF¬C, it is possible to prove the negation of some standard facts.
Any model of ZF¬C is also a model of ZF, so for each of the following statements, there exists a model of ZF in which that statement is true. For each of the following statements, there is some model of ZF¬C where it is true:


For proofs, see .


In type theory, a different kind of statement is known as the axiom of choice. This form begins with two types, σ and τ, and a relation "R" between objects of type σ and objects of type τ. The axiom of choice states that if for each "x" of type σ there exists a "y" of type τ such that "R"("x","y"), then there is a function "f" from objects of type σ to objects of type τ such that "R"("x","f"("x")) holds for all "x" of type σ:
Unlike in set theory, the axiom of choice in type theory is typically stated as an axiom scheme, in which "R" varies over all formulas or over all formulas of a particular logical form.

This is a joke: although the three are all mathematically equivalent, many mathematicians find the axiom of choice to be intuitive, the well-ordering principle to be counterintuitive, and Zorn's lemma to be too complex for any intuition.

The observation here is that one can define a function to select from an infinite number of pairs of shoes by stating for example, to choose a left shoe. Without the axiom of choice, one cannot assert that such a function exists for pairs of socks, because left and right socks are (presumably) indistinguishable.
Polish-American mathematician Jan Mycielski relates this anecdote in a 2006 article in the Notices of the AMS.
This quote comes from the famous April Fools' Day article in the "computer recreations" column of the "Scientific American", April 1989.




</doc>
<doc id="841" url="https://en.wikipedia.org/wiki?curid=841" title="Attila">
Attila

Attila (; ), frequently called Attila the Hun, was the ruler of the Huns from 434 until his death in March 453. He was also the leader of a tribal empire consisting of Huns, Ostrogoths, and Alans among others, in Central and Eastern Europe.

During his reign, he was one of the most feared enemies of the Western and Eastern Roman Empires. He crossed the Danube twice and plundered the Balkans, but was unable to take Constantinople. His unsuccessful campaign in Persia was followed in 441 by an invasion of the Eastern Roman (Byzantine) Empire, the success of which emboldened Attila to invade the West. He also attempted to conquer Roman Gaul (modern France), crossing the Rhine in 451 and marching as far as Aurelianum (Orléans) before being defeated at the Battle of the Catalaunian Plains.

He subsequently invaded Italy, devastating the northern provinces, but was unable to take Rome. He planned for further campaigns against the Romans, but died in 453. After Attila's death, his close adviser, Ardaric of the Gepids, led a Germanic revolt against Hunnic rule, after which the Hunnic Empire quickly collapsed.

There is no surviving first-hand account of Attila's appearance, but there is a possible second-hand source provided by Jordanes, who cites a description given by Priscus.

Some scholars have suggested that this description is typically East Asian, because it has all the combined features that fit the physical type of people from Eastern Asia, and Attila's ancestors may have come from there. Other historians also believed that the same descriptions were also evident on some Scythian people.

Many scholars have argued that Attila derives from East Germanic origin; "Attila" is formed from the Gothic or Gepidic noun "atta", "father", by means of the diminutive suffix "-ila", meaning "little father". The Gothic etymology was first proposed by Jacob and Wilhelm Grimm in the early 19th century. Maenchen-Helfen notes that this derivation of the name "offers neither phonetic nor semantic difficulties", and Gerhard Doerfer notes that the name is simply correct Gothic. The name has sometimes been interpreted as a Germanization of a name of Hunnic origin.

Other scholars have argued for a Turkic origin of the name. Omeljan Pritsak considered "Ἀττίλα" (Attíla) a composite title-name which derived from Turkic *"es" (great, old), and *"til" (sea, ocean), and the suffix /a/. The stressed back syllabic "til" assimilated the front member "es", so it became *"as". It is a nominative, in form of "attíl-" (< *"etsíl" < *"es tíl") with the meaning "the oceanic, universal ruler". J. J. Mikkola connected it with Turkic "āt" (name, fame). As another Turkic possibility, H. Althof (1902) considered it was related to Turkish "atli" (horseman, cavalier), or Turkish "at" (horse) and "dil" (tongue). Maenchen-Helfen argues that Pritsak's derivation is "ingenious but for many reasons unacceptable", while dismissing Mikkola's as "too farfetched to be taken seriously". M. Snædal similarly notes that none of these proposals has achieved wide acceptance. Criticizing the proposals of finding Turkic or other etymologies for Attila, Doerfer notes that King George VI of England had a name of Greek origin, and Süleyman the Magnificent had a name of Arabic origin, yet that does not make them Greeks or Arabs: it is therefore plausible that Attila would have a name not of Hunnic origin. Historian Hyun Jin Kim, however, has argued that the Turkic etymology is "more probable".

M. Snædal, in a paper that rejects the Germanic derivation but notes the problems with the existing proposed Turkic etymologies, argues that Attila's name could have originated from Turkic-Mongolian "at, adyy/agta" (gelding, warhorse) and Turkish "atli" (horseman, cavalier), meaning "possessor of geldings, provider of warhorses".

The historiography of Attila is faced with a major challenge, in that the only complete sources are written in Greek and Latin by the enemies of the Huns. Attila's contemporaries left many testimonials of his life, but only fragments of these remain. Priscus was a Byzantine diplomat and historian who wrote in Greek, and he was both a witness to and an actor in the story of Attila, as a member of the embassy of Theodosius II at the Hunnic court in 449. He was obviously biased by his political position, but his writing is a major source for information on the life of Attila, and he is the only person known to have recorded a physical description of him. He wrote a history of the late Roman Empire in eight books covering the period from 430 to 476.

Today we have only fragments of Priscus' work, but it was cited extensively by 6th-century historians Procopius and Jordanes, especially in Jordanes' "The Origin and Deeds of the Goths". It contains numerous references to Priscus's history, and it is also an important source of information about the Hunnic empire and its neighbors. He describes the legacy of Attila and the Hunnic people for a century after Attila's death. Marcellinus Comes, a chancellor of Justinian during the same era, also describes the relations between the Huns and the Eastern Roman Empire.

Numerous ecclesiastical writings contain useful but scattered information, sometimes difficult to authenticate or distorted by years of hand-copying between the 6th and 17th centuries. The Hungarian writers of the 12th century wished to portray the Huns in a positive light as their glorious ancestors, and so repressed certain historical elements and added their own legends.

The literature and knowledge of the Huns themselves was transmitted orally, by means of epics and chanted poems that were handed down from generation to generation. Indirectly, fragments of this oral history have reached us via the literature of the Scandinavians and Germans, neighbors of the Huns who wrote between the 9th and 13th centuries. Attila is a major character in many Medieval epics, such as the Nibelungenlied, as well as various Eddas and sagas.

Archaeological investigation has uncovered some details about the lifestyle, art, and warfare of the Huns. There are a few traces of battles and sieges, but today the tomb of Attila and the location of his capital have not yet been found.

The Huns were a group of Eurasian nomads, appearing from east of the Volga, who migrated further into Western Europe c. 370 and built up an enormous empire there. Their main military techniques were mounted archery and javelin throwing. They were in the process of developing settlements before their arrival in Western Europe, yet the Huns were a society of pastoral warriors whose primary form of nourishment was meat and milk, products of their herds.

The origin and language of the Huns has been the subject of debate for centuries. According to some theories, their leaders at least may have spoken a Turkic language, perhaps closest to the modern Chuvash language. One scholar suggests a relationship to Yeniseian. According to the "Encyclopedia of European Peoples", "the Huns, especially those who migrated to the west, may have been a combination of central Asian Turkic, Mongolic, and Ugric stocks".

Attila's father Mundzuk was the brother of kings Octar and Ruga, who reigned jointly over the Hunnic empire in the early fifth century. This form of diarchy was recurrent with the Huns, but historians are unsure whether it was institutionalized, merely customary, or an occasional occurrence. His family was from a noble lineage, but it is uncertain whether they constituted a royal dynasty. Attila's birthdate is debated; journalist Éric Deschodt and writer Herman Schreiber have proposed a date of 395. However, historian Iaroslav Lebedynsky and archaeologist Katalin Escher prefer an estimate between the 390s and the first decade of the fifth century. Several historians have proposed 406 as the date.

Attila grew up in a rapidly changing world. His people were nomads who had only recently arrived in Europe. They crossed the Volga river during the 370s and annexed the territory of the Alans, then attacked the Gothic kingdom between the Carpathian mountains and the Danube. They were a very mobile people, whose mounted archers had acquired a reputation for invincibility, and the Germanic tribes seemed unable to withstand them. Vast populations fleeing the Huns moved from Germania into the Roman Empire in the west and south, and along the banks of the Rhine and Danube. In 376, the Goths crossed the Danube, initially submitting to the Romans but soon rebelling against Emperor Valens, whom they killed in the Battle of Adrianople in 378. Large numbers of Vandals, Alans, Suebi, and Burgundians crossed the Rhine and invaded Roman Gaul on December 31, 406 to escape the Huns. The Roman Empire had been split in half since 395 and was ruled by two distinct governments, one based in Ravenna in the West, and the other in Constantinople in the East. The Roman Emperors, both East and West, were generally from the Theodosian family in Attila's lifetime (despite several power struggles).

The Huns dominated a vast territory with nebulous borders determined by the will of a constellation of ethnically varied peoples. Some were assimilated to Hunnic nationality, whereas many retained their own identities and rulers but acknowledged the suzerainty of the king of the Huns. The Huns were also the indirect source of many of the Romans' problems, driving various Germanic tribes into Roman territory, yet relations between the two empires were cordial: the Romans used the Huns as mercenaries against the Germans and even in their civil wars. Thus, the usurper Joannes was able to recruit thousands of Huns for his army against Valentinian III in 424. It was Aëtius, later Patrician of the West, who managed this operation. They exchanged ambassadors and hostages, the alliance lasting from 401 to 450 and permitting the Romans numerous military victories. The Huns considered the Romans to be paying them tribute, whereas the Romans preferred to view this as payment for services rendered. The Huns had become a great power by the time that Attila came of age during the reign of his uncle Ruga, to the point that Nestorius, the Patriarch of Constantinople, deplored the situation with these words: "They have become both masters and slaves of the Romans".

The death of Rugila (also known as Rua or Ruga) in 434 left the sons of his brother Mundzuk, Attila and Bleda, in control of the united Hun tribes. At the time of the two brothers' accession, the Hun tribes were bargaining with Eastern Roman Emperor Theodosius II's envoys for the return of several renegades who had taken refuge within the Eastern Roman Empire, possibly Hunnic nobles who disagreed with the brothers' assumption of leadership.

The following year, Attila and Bleda met with the imperial legation at Margus (Požarevac), all seated on horseback in the Hunnic manner, and negotiated an advantageous treaty. The Romans agreed to return the fugitives, to double their previous tribute of 350 Roman pounds (c. 115 kg) of gold, to open their markets to Hunnish traders, and to pay a ransom of eight "solidi" for each Roman taken prisoner by the Huns. The Huns, satisfied with the treaty, decamped from the Roman Empire and returned to their home in the Great Hungarian Plain, perhaps to consolidate and strengthen their empire. Theodosius used this opportunity to strengthen the walls of Constantinople, building the city's first sea wall, and to build up his border defenses along the Danube.

The Huns remained out of Roman sight for the next few years while they invaded the Sassanid Empire. They were defeated in Armenia by the Sassanids, abandoned their invasion, and turned their attentions back to Europe. In 440, they reappeared in force on the borders of the Roman Empire, attacking the merchants at the market on the north bank of the Danube that had been established by the treaty of 435.

Crossing the Danube, they laid waste to the cities of Illyricum and forts on the river, including (according to Priscus) Viminacium, a city of Moesia. Their advance began at Margus, where they demanded that the Romans turn over a bishop who had retained property that Attila regarded as his. While the Romans discussed the bishop's fate, he slipped away secretly to the Huns and betrayed the city to them.

While the Huns attacked city-states along the Danube, the Vandals (led by Geiseric) captured the Western Roman province of Africa and its capital of Carthage. Carthage was the richest province of the Western Empire and a main source of food for Rome. The Sassanid Shah Yazdegerd II invaded Armenia in 441.

The Romans stripped the Balkan area of forces, sending them to Sicily in order to mount an expedition against the Vandals in Africa. This left Attila and Bleda a clear path through Illyricum into the Balkans, which they invaded in 441. The Hunnish army sacked Margus and Viminacium, and then took Singidunum (Belgrade) and Sirmium. During 442, Theodosius recalled his troops from Sicily and ordered a large issue of new coins to finance operations against the Huns. He believed that he could defeat the Huns and refused the Hunnish kings' demands.

Attila responded with a campaign in 443. For the first time (as far as the Romans knew) his forces were equipped with battering rams and rolling siege towers, with which they successfully assaulted the military centers of Ratiara and Naissus (Niš) and massacred the inhabitants. Priscus said "When we arrived at Naissus we found the city deserted, as though it had been sacked; only a few sick persons lay in the churches. We halted at a short distance from the river, in an open space, for all the ground adjacent to the bank was full of the bones of men slain in war."

Advancing along the Nišava River, the Huns next took Serdica (Sofia), Philippopolis (Plovdiv), and Arcadiopolis (Lüleburgaz). They encountered and destroyed a Roman army outside Constantinople but were stopped by the double walls of the Eastern capital. They defeated a second army near Callipolis (Gelibolu).

Theodosius, unable to make effective armed resistance, admitted defeat, sending the "Magister militum per Orientem" Anatolius to negotiate peace terms. The terms were harsher than the previous treaty: the Emperor agreed to hand over 6,000 Roman pounds (c. 2000 kg) of gold as punishment for having disobeyed the terms of the treaty during the invasion; the yearly tribute was tripled, rising to 2,100 Roman pounds (c. 700 kg) in gold; and the ransom for each Roman prisoner rose to 12 "solidi".

Their demands were met for a time, and the Hun kings withdrew into the interior of their empire. Bleda died following the Huns' withdrawal from Byzantium (probably around 445). Attila then took the throne for himself, becoming the sole ruler of the Huns.

In 447, Attila again rode south into the Eastern Roman Empire through Moesia. The Roman army, under Gothic "magister militum" Arnegisclus, met him in the Battle of the Utus and was defeated, though not without inflicting heavy losses. The Huns were left unopposed and rampaged through the Balkans as far as Thermopylae.

Constantinople itself was saved by the Isaurian troops of "magister militum per Orientem" Zeno and protected by the intervention of prefect Constantinus, who organized the reconstruction of the walls that had been previously damaged by earthquakes and, in some places, to construct a new line of fortification in front of the old. Callinicus, in his "Life of Saint Hypatius", wrote:

In 450, Attila proclaimed his intent to attack the Visigoth kingdom of Toulouse by making an alliance with Emperor Valentinian III. He had previously been on good terms with the Western Roman Empire and its influential general Flavius Aëtius. Aëtius had spent a brief exile among the Huns in 433, and the troops that Attila provided against the Goths and Bagaudae had helped earn him the largely honorary title of "magister militum" in the west. The gifts and diplomatic efforts of Geiseric, who opposed and feared the Visigoths, may also have influenced Attila's plans.

However, Valentinian's sister was Honoria, who had sent the Hunnish king a plea for help—and her engagement ring—in order to escape her forced betrothal to a Roman senator in the spring of 450. Honoria may not have intended a proposal of marriage, but Attila chose to interpret her message as such. He accepted, asking for half of the western Empire as dowry.

When Valentinian discovered the plan, only the influence of his mother Galla Placidia convinced him to exile Honoria, rather than killing her. He also wrote to Attila, strenuously denying the legitimacy of the supposed marriage proposal. Attila sent an emissary to Ravenna to proclaim that Honoria was innocent, that the proposal had been legitimate, and that he would come to claim what was rightfully his.

Attila interfered in a succession struggle after the death of a Frankish ruler. Attila supported the elder son, while Aëtius supported the younger. (The location and identity of these kings is not known and subject to conjecture.) Attila gathered his vassals—Gepids, Ostrogoths, Rugians, Scirians, Heruls, Thuringians, Alans, Burgundians, among others–and began his march west. In 451, he arrived in Belgica with an army exaggerated by Jordanes to half a million strong.

On April 7, he captured Metz. Other cities attacked can be determined by the hagiographic "vitae" written to commemorate their bishops: Nicasius was slaughtered before the altar of his church in Rheims; Servatus is alleged to have saved Tongeren with his prayers, as Saint Genevieve is said to have saved Paris. Lupus, bishop of Troyes, is also credited with saving his city by meeting Attila in person.

Aëtius moved to oppose Attila, gathering troops from among the Franks, the Burgundians, and the Celts. A mission by Avitus and Attila's continued westward advance convinced the Visigoth king Theodoric I (Theodorid) to ally with the Romans. The combined armies reached Orléans ahead of Attila, thus checking and turning back the Hunnish advance. Aëtius gave chase and caught the Huns at a place usually assumed to be near Catalaunum (modern Châlons-en-Champagne). Attila decided to fight the Romans on plains where he could use his cavalry.

The two armies clashed in the Battle of the Catalaunian Plains, the outcome of which is commonly considered to be a strategic victory for the Visigothic-Roman alliance. Theodoric was killed in the fighting, and Aëtius failed to press his advantage, according to Edward Gibbon and Edward Creasy, because he feared the consequences of an overwhelming Visigothic triumph as much as he did a defeat. From Aëtius' point of view, the best outcome was what occurred: Theodoric died, Attila was in retreat and disarray, and the Romans had the benefit of appearing victorious.

Attila returned in 452 to renew his marriage claim with Honoria, invading and ravaging Italy along the way. Communities became established in what would later become Venice as a result of these attacks when the residents fled to small islands in the Venetian Lagoon. His army sacked numerous cities and razed Aquileia so completely that it was afterwards hard to recognize its original site. Aëtius lacked the strength to offer battle, but managed to harass and slow Attila's advance with only a shadow force. Attila finally halted at the River Po. By this point, disease and starvation may have taken hold in Attila's camp, thus hindering his war efforts and potentially contributing to the cessation of invasion.

Emperor Valentinian III sent three envoys, the high civilian officers Gennadius Avienus and Trigetius, as well as the Bishop of Rome Leo I, who met Attila at Mincio in the vicinity of Mantua and obtained from him the promise that he would withdraw from Italy and negotiate peace with the Emperor. Prosper of Aquitaine gives a short description of the historic meeting, but gives all the credit to Leo for the successful negotiation. Priscus reports that superstitious fear of the fate of Alaric gave him pause—as Alaric died shortly after sacking Rome in 410.

Italy had suffered from a terrible famine in 451 and her crops were faring little better in 452. Attila's devastating invasion of the plains of northern Italy this year did not improve the harvest. To advance on Rome would have required supplies which were not available in Italy, and taking the city would not have improved Attila's supply situation. Therefore, it was more profitable for Attila to conclude peace and retreat to his homeland.

Furthermore, an East Roman force had crossed the Danube under the command of another officer also named Aetius—who had participated in the Council of Chalcedon the previous year—and proceeded to defeat the Huns who had been left behind by Attila to safeguard their home territories. Attila, hence, faced heavy human and natural pressures to retire "from Italy without ever setting foot south of the Po". As Hydatius writes in his "Chronica Minora":

Marcian was the successor of Theodosius, and he had ceased paying tribute to the Huns in late 450 while Attila was occupied in the west. Multiple invasions by the Huns and others had left the Balkans with little to plunder.
After Attila left Italy and returned to his palace across the Danube, he planned to strike at Constantinople again and reclaim the tribute which Marcian had stopped. However, he died in the early months of 453.

The conventional account from Priscus says that Attila was at a feast celebrating his latest marriage, this time to the beautiful young Ildico (the name suggests Gothic or Ostrogoth origins). In the midst of the revels, however, he suffered a severe nosebleed and choked to death in a stupor. An alternative theory is that he succumbed to internal bleeding after heavy drinking, possibly a condition called esophageal varices, where dilated veins in the lower part of the esophagus rupture leading to death by hemorrhage.

Another account of his death was first recorded 80 years after the events by Roman chronicler Marcellinus Comes. It reports that "Attila, King of the Huns and ravager of the provinces of Europe, was pierced by the hand and blade of his wife". Most scholars reject these accounts as no more than hearsay, preferring instead the account given by Attila's contemporary Priscus. Priscus' version, however, has recently come under renewed scrutiny by Michael A. Babcock. Based on detailed philological analysis, Babcock concludes that the account of natural death given by Priscus was an ecclesiastical "cover story", and that Emperor Marcian (who ruled the Eastern Roman Empire from 450 to 457) was the political force behind Attila's death. Jordanes recounts:

Attila's sons Ellac, Dengizich and Ernak, "in their rash eagerness to rule they all alike destroyed his empire". They "were clamoring that the nations should be divided among them equally and that warlike kings with their peoples should be apportioned to them by lot like a family estate". Against the treatment as "slaves of the basest condition" a Germanic alliance led by the Gepid ruler Ardaric (who was noted for great loyalty to Attila) revolted and fought with the Huns in Pannonia in the Battle of Nedao 454 AD. Attila's eldest son Ellac was killed in that battle. Attila's sons "regarding the Goths as deserters from their rule, came against them as though they were seeking fugitive slaves", attacked Ostrogothic co-ruler Valamir (who also fought alongside Ardaric and Attila at the Catalaunian Plains), but were repelled, and some group of Huns moved to Scythia (probably those of Ernak). His brother Dengizich attempted a renewed invasion across the Danube in 468 AD, but was defeated at the Battle of Bassianae by the Ostrogoths. Dengizich was killed by Roman-Gothic general Anagast the following year, after which the Hunnic dominion ended.

Attila's many children and relatives are known by name and some even by deeds, but soon valid genealogical sources all but dried up, and there seems to be no verifiable way to trace Attila's descendants. This has not stopped many genealogists from attempting to reconstruct a valid line of descent for various medieval rulers. One of the most credible claims has been that of the "Nominalia of the Bulgarian khans" for mythological Avitohol and Irnik from the Dulo clan of the Bulgars.

Attila himself is said to have claimed the titles "Descendant of the Great Nimrod", and "King of the Huns, the Goths, the Danes, and the Medes"—the last two peoples being mentioned to show the extent of his control over subject nations even on the peripheries of his domain.

Jordanes embellished the report of Priscus, reporting that Attila had possessed the "Holy War Sword of the Scythians", which was given to him by Mars and made him a "prince of the entire world".

By the end of the 12th century the royal court of Hungary proclaimed their descent from Attila. Lampert of Hersfeld's contemporary chronicles report that shortly before the year 1071, the Sword of Attila had been presented to Otto of Nordheim by the exiled queen of Hungary, Anastasia of Kiev. This sword, a cavalry sabre now in the Kunsthistorisches Museum in Vienna, appears to be the work of Hungarian goldsmiths of the ninth or tenth century.

An anonymous chronicler of the medieval period represented the meeting of Pope Leo and Atilla as attended also by Saint Peter and Saint Paul, "a miraculous tale calculated to meet the taste of the time" This apotheosis was later portrayed artistically by the Renaissance artist Raphael and sculptor Algardi, whom eighteenth-century historian Edward Gibbon praised for establishing "one of the noblest legends of ecclesiastical tradition".

According to a version of this narrative related in the Chronicon Pictum, a mediaeval Hungarian chronicle, the Pope promised Attila that if he left Rome in peace, one of his successors would receive a holy crown (which has been understood as referring to the Holy Crown of Hungary).

Some histories and chronicles describe him as a great and noble king, and he plays major roles in three Norse sagas: "Atlakviða", "Volsunga saga", and "Atlamál". The "Polish Chronicle" represents Attila's name as "Aquila".

Frutolf of Michelsberg and Otto of Freising pointed out that some songs as "vulgar fables" made Theoderic the Great, Attila and Ermanaric contemporaries, when any reader of Jordanes knew that this was not the case. This refers to the so-called historical poems about Dietrich von Bern (Theoderic), in which Etzel (Attila) is Dietrich's refuge in exile from his wicked uncle Ermenrich (Ermanaric). Etzel is most prominent in the poems "Dietrichs Flucht" and the "Rabenschlacht". Etzel also appears as Kriemhild's second noble husband in the "Nibelungenlied", in which Kriemhild causes the destruction of both the Hunnish kingdom and that of her Burgundian relatives.

In 1812, Ludwig van Beethoven conceived the idea of writing an opera about Attila and approached August von Kotzebue to write the libretto. It was, however, never written. In 1846, Giuseppe Verdi wrote the opera, loosely based on episodes in Attila's invasion of Italy.

In World War I, Allied propaganda referred to Germans as the "Huns", based on a 1900 speech by Emperor Wilhelm II praising Attila the Hun's military prowess, according to Jawaharlal Nehru's "Glimpses of World History". "Der Spiegel" commented on November 6, 1948, that the Sword of Attila was hanging menacingly over Austria.

American writer Cecelia Holland wrote "The Death of Attila" (1973), a historical novel in which Attila appears as a powerful background figure whose life and death deeply impact the protagonists, a young Hunnic warrior and a Germanic one.

The name has many variants in several languages: Atli and Atle in Old Norse; Etzel in Middle High German (Nibelungenlied); Ætla in Old English; Attila, Atilla, and Etele in Hungarian (Attila is the most popular); Attila, Atilla, Atilay, or Atila in Turkish; and Adil and Edil in Kazakh or Adil ("same/similar") or Edil ("to use") in Mongolian.

In modern Hungary and in Turkey, "Attila" and its Turkish variation "Atilla" are commonly used as a male first name. In Hungary, several public places are named after Attila; for instance, in Budapest there are 10 Attila Streets, one of which is an important street behind the Buda Castle. When the Turkish Armed Forces invaded Cyprus in 1974, the operations were named after Attila ("The Attila Plan").

The 1954 Universal International film "Sign of the Pagan" starred Jack Palance as Attila.




</doc>
<doc id="842" url="https://en.wikipedia.org/wiki?curid=842" title="Aegean Sea">
Aegean Sea

The Aegean Sea is an elongated embayment of the Mediterranean Sea located between the Greek and Anatolian peninsulas. The sea has an area of some 215,000 square kilometres. In the north, the Aegean is connected to the Marmara Sea and the Black Sea by the straits of the Dardanelles and Bosphorus. The Aegean Islands are located within the sea and some bound it on its southern periphery, including Crete and Rhodes. The sea reaches a maximum depth of 3,544 meters, to the east of Crete.

The Aegean Islands can be divided into several island groups, including Dodecanese, the Cyclades, the Sporades, the Saronic islands and the North Aegean Islands, as well as Crete and its surrounding islands. The Dodecanese, located to the southeast, includes the islands of Rhodes, Kos, and Patmos; the islands of Delos and Naxos are within the Cyclades to the south of the sea. Lesbos is part of the North Aegean Islands. Euboea, the second largest island in Greece, is located in the Aegean, despite being administered as part of Central Greece. Nine out of twelve of the Administrative regions of Greece border the sea, along with the Turkish provinces of Edirne, Canakkale, Balıkesir, Izmir, Aydın and Muğla to the east of the sea. Various Turkish islands in the sea are Imbros, Tenedos, Cunda Island, and the Foça Islands.

The Aegean Sea has been historically important, especially in regards to the civilization of Ancient Greece, who inhabited the area around the coast of the Aegean and the Aegean islands. The Aegean islands facilitated contact between the people of the area and between Europe and Asia. Along with the Greeks, Thracians lived among the northern coast. The Romans conquered the area under the Roman Empire, and later the Byzantine Empire held it against advances by the First Bulgarian Empire. The Fourth Crusade weakened Byzantine control of the area, and it was eventually conquered by the Ottoman Empire, with the exception of Crete, which was a Venetian colony until 1669. The Greek War of Independence allowed a Greek state on the coast of the Aegean from 1829 onwards. The Ottoman Empire held a presence over the sea for over 500 years until their dissolution, when it was replaced by modern Turkey.

The sea was traditionally known as "the Archipelago" (in Ancient Greek, , meaning "chief sea"), but in English the meaning of "archipelago" has changed to refer to the Aegean Islands and, generally, to any island group. The rocks making up the floor of the Aegean are mainly limestone, though often greatly altered by volcanic activity that has convulsed the region in relatively recent geologic times. Of particular interest are the richly coloured sediments in the region of the islands of Santorini and Milos, in the south Aegean. Notable cities on the Aegean coastline include Thessaloniki, Kavala and Heraklion in Greece, and İzmir and Bodrum in Turkey.

A number of issues concerning sovereignty within the Aegean Sea are disputed between Greece and Turkey. The Aegean dispute has had a large effect on Greek-Turkish relations since the 1970s. Issues include the delimitation of territorial waters, national airspace, exclusive economic zones and flight information regions.

It is generally believed that the Greek name "Aegean" is linked to the mythological figure Aegeus (Greek: Αἰγεύς), who was the father of Theseus, the mythical king and founder-hero of Athens. Aegeus had told Theseus to put up white sails when returning if he was successful in killing the Minotaur. When Theseus returned, he forgot these instructions, and Aegeus subsequently drowned himself in the sea when he thought his son had died.

The sea was known in Latin as "Aegaeum mare" under the control of the Roman Empire. The Venetians, who ruled many Greek islands in the High and Late Middle Ages, popularized the name "Archipelago" (Greek: αρχιπέλαγος, meaning "main sea" or "chief sea"), a name that held on in many European countries until the early modern period. In some South Slavic languages, the Aegean is often called "White Sea" (Bulgarian: /; Macedonian: /; Serbo-Croatian: /). The Turkish name for the sea is "Ege Denizi," most likely a phonetic transliteration of the Greek name.

The Aegean Sea is an elongated embayment of the Mediterranean Sea, and covers about in area, measuring about longitudinally and latitudinal. The sea's maximum depth is , located at a point east of Crete. The Aegean Islands are found within its waters, with the following islands delimiting the sea on the south, generally from west to east: Kythera, Antikythera, Crete, Kasos, Karpathos and Rhodes. The Anatolian peninsula marks the eastern boundary of the sea, while the Greek mainland marks the west. Several seas are contained within the Aegean Sea; the Thracian Sea is a section of the Aegean located to the north, the Icarian Sea to the east, the Myrtoan Sea to the west, while the Sea of Crete is the southern section.

The Greek regions that border the sea, in alphabetical order, are Attica, Central Greece, Central Macedonia, Crete, Eastern Macedonia and Thrace, North Aegean, Peloponnese, South Aegean, and Thessaly. The historical region of Macedonia also borders the sea, to the north.

The Aegean Islands, which almost all belong to Greece, can be divided into seven groups:

The word "archipelago" was originally applied specifically to the Aegean Sea and its islands. Many of the Aegean Islands, or chains of islands, are actually extensions of the mountains on the mainland. One chain extends across the sea to Chios, another extends across Euboea to Samos, and a third extends across the Peloponnese and Crete to Rhodes, dividing the Aegean from the Mediterranean.

The bays and gulfs of the Aegean beginning at the South and moving clockwise include on Crete, the Mirabello, Almyros, Souda and Chania bays or gulfs, on the mainland the Myrtoan Sea to the west with the Argolic Gulf, the Saronic Gulf northwestward, the Petalies Gulf which connects with the South Euboic Sea, the Pagasetic Gulf which connects with the North Euboic Sea, the Thermian Gulf northwestward, the Chalkidiki Peninsula including the Cassandra and the Singitic Gulfs, northward the Strymonian Gulf and the Gulf of Kavala and the rest are in Turkey; Saros Gulf, Edremit Gulf, Dikili Gulf, Gulf of Çandarlı, Gulf of İzmir, Gulf of Kuşadası, Gulf of Gökova, Güllük Gulf.

The Aegean sea is connected to the Sea of Marmara by the Dardanelles, also known from Classical Antiquity as the Hellespont. The Dardanelles are located to the northeast of the sea. It ultimately connects with the Black Sea through the Bosphoros strait, upon which lies the city of Istanbul. The Dardanelles and the Bosphoros are known as the Turkish Straits.

According to the International Hydrographic Organization, the limits of the Aegean Sea as follows:


Aegean surface water circulates in a counterclockwise gyre, with hypersaline Mediterranean water moving northward along the west coast of Turkey, before being displaced by less dense Black Sea outflow. The dense Mediterranean water sinks below the Black Sea inflow to a depth of , then flows through the Dardanelles Strait and into the Sea of Marmara at velocities of . The Black Sea outflow moves westward along the northern Aegean Sea, then flows southwards along the east coast of Greece.

The physical oceanography of the Aegean Sea is controlled mainly by the regional climate, the fresh water discharge from major rivers draining southeastern Europe, and the seasonal variations in the Black Sea surface water outflow through the Dardanelles Strait.

Analysis of the Aegean during 1991 and 1992 revealed three distinct water masses:


The Climate of the Aegean Sea largely reflects the climate of Greece and Western Turkey, which is to say, predominately Mediterranean. According to the Köppen climate classification, most of the Aegean is classified as Hot-summer Mediterranean ("Csa"), with hotter and drier summers along with milder and wetter winters. However, high temperatures during summers are generally not quite as high as those in arid or semiarid climates due to the presence of a large body of water. This is most predominant in the west and east coasts of the Aegean, and within the Aegean islands. In the north of the Aegean Sea, the climate is instead classified as Cold semi-arid "(BSk)", which feature cooler summers that Hot-summer Mediterranean climates.
The Etesian winds are a dominant weather influence in the Aegean Basin.

The below table lists climate conditions of some major Aegean cities:
Numerous Greek and Turkish settlements are located along their mainland coast, as well as on towns on the Aegean islands. The largest cities are Athens and Thessaloniki in Greece and İzmir in Turkey. The most populated of the Aegean islands is Crete, followed by Euboea and Rhodes. 

Greece has established several marine protected areas along its coasts. According to the Network of Managers of Marine Protected Areas in the Mediterranean (MedPAN), four Greek MPAs are participating in the Network. These include Alonnisos Marine Park, while the Missolonghi–Aitoliko Lagoons and the island of Zakynthosare not on the Aegean.

The current coastline dates back to about 4000 BC. Before that time, at the peak of the last ice age (about 18,000 years ago) sea levels everywhere were 130 metres lower, and there were large well-watered coastal plains instead of much of the northern Aegean. When they were first occupied, the present-day islands including Milos with its important obsidian production were probably still connected to the mainland. The present coastal arrangement appeared around 9,000 years ago, with post-ice age sea levels continuing to rise for another 3,000 years after that.

The subsequent Bronze Age civilizations of Greece and the Aegean Sea have given rise to the general term "Aegean civilization". In ancient times, the sea was the birthplace of two ancient civilizations – the Minoans of Crete and the Myceneans of the Peloponnese.

The Minoan civilization was a Bronze Age civilization on the island of Crete and other Aegean Islands, flourishing from around 2700 to 1450 BC before a period of decline, finally ending at around 1100 BC. It represented the first advanced civilization in Europe, leaving behind massive building complexes, tools, stunning artwork, writing systems, and a massive network of trade. The Minoan period saw extensive trade between Crete, Aegean, and Mediterranean settlements, particularly the Near East. The most notable Minoan palace is that of Knossos, followed by that of Phaistos.

After the decline of the Minoan civilization, the Mycenaean Greeks arose, becoming the first advanced civilization in mainland Greece, and lasted from approximately 1600 to 1100 BC. It is believed that the site of Mycenae, which sits close to the Aegean coast, was the center of Mycenaean civilization. The Mycenaeans introduced several innovations in the fields of engineering, architecture and military infrastructure, while trade over vast areas of the Mediterranean, including the Aegean, was essential for the Mycenaean economy. Their syllabic script, the Linear B, offers the first written records of the Greek language and their religion already included several deities that can also be found in the Olympic Pantheon. Mycenaean Greece was dominated by a warrior elite society and consisted of a network of palace-centered states that developed rigid hierarchical, political, social and economic systems. At the head of this society was the king, known as "wanax".

The civilization of Mycenaean Greeks perished with the collapse of Bronze Age culture in the eastern Mediterranean, to be followed by the so-called Greek Dark Ages. It is undetermined what cause the collapse of the Mycenaeans. During the Greek Dark Ages, writing in the Linear B script ceased, vital trade links were lost, and towns and villages were abandoned.

The Archaic period followed the Greek Dark Ages in the 8th century BC. Greece became divided into small self-governing communities, and adopted the Phoenician alphabet, modifying it to create the Greek alphabet. By the 6th century BC several cities had emerged as dominant in Greek affairs: Athens, Sparta, Corinth, and Thebes, of which Athens, Sparta, and Corinth were closest to the Aegean Sea. Each of them had brought the surrounding rural areas and smaller towns under their control, and Athens and Corinth had become major maritime and mercantile powers as well. In the 8th and 7th centuries BC many Greeks emigrated to form colonies in Magna Graecia (Southern Italy and Sicily), Asia Minor and further afield.

In the second half of the 6th century BC, Athens fell under the tyranny of Peisistratos and then of his sons Hippias and Hipparchos. However, in 510 BC, at the instigation of the Athenian aristocrat Cleisthenes, the Spartan king Cleomenes I helped the Athenians overthrow the tyranny. Afterwards, Sparta and Athens promptly turned on each other, at which point Cleomenes I installed Isagoras as a pro-Spartan archon. Eager to prevent Athens from becoming a Spartan puppet, Cleisthenes responded by proposing to his fellow citizens that Athens undergo a revolution: that all citizens share in political power, regardless of status: that Athens become a "democracy". So enthusiastically did the Athenians take to this idea that, having overthrown Isagoras and implemented Cleisthenes's reforms, they were easily able to repel a Spartan-led three-pronged invasion aimed at restoring Isagoras. The advent of the democracy cured many of the ills of Athens and led to a 'golden age' for the Athenians. Plato described the Greeks living round the Aegean "like frogs around a pond".

The Aegean Sea would later come to be under the control, albeit briefly, of the Kingdom of Macedonia. Philip II and his son Alexander the Great led a series of conquests that led not only to the unification of the Greek mainland and the control of the Aegean Sea under his rule, but also the destruction of the Achaemenid Empire. After Alexander the Great's death, his empire was divided among his generals. Cassander became king of the Hellenistic kingdom of Macedon, which held territory along the western coast of the Aegean, roughly corresponding to modern-day Greece. The Kingdom of Lysimachus had control over the sea's eastern coast. Greece had entered the Hellenistic period.

The Macedonian Wars were a series of conflicts fought by the Roman Republic and its Greek allies in the eastern Mediterranean against several different major Greek kingdoms. They resulted in Roman control or influence over the eastern Mediterranean basin, including the Aegean, in addition to their hegemony in the western Mediterranean after the Punic Wars. During Roman rule, the land around the Aegean Sea fell under the provinces of Achaea, Macedonia, Thracia, Asia and Creta et Cyrenica (island of Crete)

The fall of the Western Roman Empire allowed its successor state, the Byzantine Empire, to continue Roman control over the Aegean Sea. However, their territory would later be threatened by the Early Muslim conquests initiated by Muhammad in the 7th century. Although the Rashidun Caliphate did not manage to obtain land along the cost of the Aegean sea, its conquest of the Eastern Anatolian peninsula as well as Egypt, the Levant, and North Africa left the Byzantine Empire weakened. The Umayyad Caliphate expanded the territorial gains of the Rashidun Caliphate, conquering much of North Africa, and threatened the Byzantine Empire's control of Western Anatolia, where it meets the Aegean Sea.

During the 820s, Crete was conquered by a group of Berbers Andalusians exiles led by Abu Hafs Umar al-Iqritishi, and it became an independent Islamic state. The Byzantine Empire launched a campaign that took most of the island back in 842 and 843 under Theoktistos, but the reconquest was not completed and was soon reversed. Later attempts by the Byzantine Empire to recover the island were without success. For the approximately 135 years of its existence, the emirate of Crete was one of the major foes of Byzantium. Crete commanded the sea lanes of the Eastern Mediterranean and functioned as a forward base and haven for Muslim corsair fleets that ravaged the Byzantine-controlled shores of the Aegean Sea. Crete returned to Byzantine rule under Nikephoros Phokas, who launched a huge campaign against the Emirate of Crete in 960 to 961.

Meanwhile, the Bulgarian Empire threatened Byzantine control of Northern Greece and the Aegean coast to the south. Under Presian I and his successor Boris I, the Bulgarian Empire managed to obtain a small portion of the northern Aegean coast. Simeon I of Bulgaria led Bulgaria to its greatest territorial expansion, and managed to conqueror much of the northern and western coasts of the Aegean. The Byzantines later regained control. The Second Bulgarian Empire achieved similar success along, again, the northern and western coasts, under Ivan Asen II of Bulgaria.

The Seljuq Turks, under the Seljuk Empire, invaded the Byzantine Empire in 1068, from which they annexed almost all of Anatolia, including the east coast of the Aegean Sea, during the reign of Alp Arslan, the second Sultan of the Seljuk Empire. After the death of his successor, Malik Shah I, the empire was divided, and Malik Shah was succeeded in Anatolia by Kilij Arslan I, who founded the Sultanate of Rum. The Byzantines yet again recaptured the eastern coast of the Aegean.

After Constantinople was occupied by Western European and Venetian forces during the Fourth Crusade, the area around the Aegean sea was fragmented into multiple entities, including the Latin Empire, the Kingdom of Thessalonica, the Empire of Nicaea, the Principality of Achaea, and the Duchy of Athens. The Venetians created the maritime state of the Duchy of the Archipelago, which included all the Cyclades except Mykonos and Tinos. The Empire of Nicaea, a Byzantine rump state, managed to effect the Recapture of Constantinople from the Latins in 1261 and defeat Epirus. Byzantine successes were not to last; the Ottomans would conquer the area around the Aegean coast, but before their expansion the Byzantine Empire had already been weakened from internal conflict. By the late 14th century the Byzantine Empire had lost all control of the coast of the Aegean Sea and could exercise power around their capital, Constantinople. The Ottoman Empire then gained control of all the Aegean coast with the exception of Crete, which was a Venetian colony until 1669.

The Greek War of Independence allowed a Greek state on the coast of the Aegean from 1829 onward. The Ottoman Empire held a presence over the sea for over 500 years until their dissolution following World War I, when it was replaced by modern Turkey. During the war, Greece gained control over the area around the northern coast of the Aegean. By the 1930s, Greece and Turkey had about resumed their present-day borders.

In the Italo-Turkish War of 1912, Italy captured the Dodecanese islands, and had occupied them since, reneging on the 1919 Venizelos–Tittoni agreement to cede them to Greece. The Greco-Italian War took place from October 1940 to April 1941 as part of the Balkans Campaign of World War II. The Italian war aim was to establish a Greek puppet state, which would permit the Italian annexation of the Sporades and the Cyclades islands in the Aegean Sea, to be administered as a part of the Italian Aegean Islands. The German invasion resulted in the Axis occupation of Greece. The German troops evacuated Athens on 12 October 1944, and by the end of the month, they had withdrawn from mainland Greece. Greece was then liberated by Allied troops.

Many of the islands in the Aegean have safe harbours and bays. In ancient times, navigation through the sea was easier than travelling across the rough terrain of the Greek mainland, and to some extent, the coastal areas of Anatolia. Many of the islands are volcanic, and marble and iron are mined on other islands. The larger islands have some fertile valleys and plains.

Of the main islands in the Aegean Sea, two belong to Turkey – Bozcaada (Tenedos) and Gökçeada (Imbros); the rest belong to Greece. Between the two countries, there are political disputes over several aspects of political control over the Aegean space, including the size of territorial waters, air control and the delimitation of economic rights to the continental shelf. These issues are known as the Aegean dispute.

Multiple ports are located along the Greek and Turkish coasts of the Aegean Sea. The port of Piraeus in Athens is the chief port in Greece, the largest passenger port in Europe and the third largest in the world, servicing about 20 million passengers annually. With a throughput of 1.4 million TEUs, Piraeus is placed among the top ten ports in container traffic in Europe and the top container port in the Eastern Mediterranean. Piraeus is also the commercial hub of Greek shipping. Piraeus bi-annually acts as the focus for a major shipping convention, known as Posidonia, which attracts maritime industry professionals from all over the world. Piraeus is currently Greece's third-busiest port in terms of tons of goods transported, behind Aghioi Theodoroi and Thessaloniki. The central port serves ferry routes to almost every island in the eastern portion of Greece, the island of Crete, the Cyclades, the Dodecanese, and much of the northern and the eastern Aegean Sea, while the western part of the port is used for cargo services.

The Port of Thessaloniki is the second-largest container port in Greece after the port of Piraeus. In 2007, the Port of Thessaloniki handled 14,373,245 tonnes of cargo and 222,824 TEU's, making it one of the busiest cargo ports in Greece and the second largest container port in the country. Paloukia, on the island of Salamis, is a major passenger port.

Fishing is Greece's second largest agricultural export, and contains Europe's largest fishing fleet. Fish captured include sardines, mackerel, grouper, grey mullets, sea bass, and seabream. There is a considerable difference between fish catches between the pelagic and demersal zones; with respect to pelagic fisheries, the catches from the northern, central and southern Aegean area groupings are dominated, respectively, by anchovy, horse mackerels, and boops. For demersal fisheries, the catches from the northern and southern Aegean area groupings are dominated by grey mullets and pickerel ("Spicara smaris") respectively.

The industry has been impacted by the Great Recession. Overfishing and habitat destruction is also a concern, threatening grouper, and seabream populations, resulting in perhaps a 50% decline of fish catch. To address these concerns, Greek fishermen have been offered a compensation by the government. Although some species are defined as protected or threatened under EU legislation, several illegal species such as the molluscs "Pinna nobilis", "Charonia tritonis" and "Lithophaga lithophaga", can be bought in restaurants and fish markets around Greece.

The Aegean islands within the Aegean Sea are significant tourist destinations. Tourism to the Aegean islands contribute a significant portion of tourism in Greece, especially since the second half of the 20th century. A total of five UNESCO World Heritage sites are located the Aegean Islands; these include the Monastery of Saint John the Theologian and the Cave of the Apocalypse on Patmos, the Pythagoreion and Heraion of Samos in Samos, the Nea Moni of Chios, the island of Delos, and the Medieval City of Rhodes.

Greece is one of the most visited countries in Europe and the world with over 33 million visitors in 2018, and the tourism industry around a quarter of Greece's Gross Domestic Product. The islands of Santorini, Crete, Lesbos, Delos, and Mykonos are common tourist destinations. An estimated 2 million tourists visit Santorini annually. However, concerns relating to overtourism have arisen in recent years, such as issues of inadequate infrastructure and overcrowding. Alongside Greece, Turkey has also been successful in developing resort areas and attracting large number of tourists, contributing to tourism in Turkey. The phrase "Blue Cruise" refers to recreational voyages along the Turkish Riviera, including across the Aegean. The ancient city of Troy, a World Heritage Site, is on the Turkish coast of the Aegean.

Greece and Turkey both take part in the Blue Flag beach certification programme of the Foundation for Environmental Education. The certification is awarded for beaches and marinas meeting strict quality standards including environmental protection, water quality, safety and services criteria. As of 2015, the Blue Flag has been awarded to 395 beaches and 9 marinas in Greece. Southern Aegean beaches on the Turkish coast include Muğla, with 102 beaches awarded with the blue flag, along with İzmir and Aydın, who have 49 and 30 beaches awarded respectively.

• The Aegean sea is where one of the White Star Line's ships, H.M.H.S. Britannic, sank after hitting a mine.



</doc>
<doc id="843" url="https://en.wikipedia.org/wiki?curid=843" title="A Clockwork Orange (novel)">
A Clockwork Orange (novel)

A Clockwork Orange is a dystopian satirical black comedy novel by English writer Anthony Burgess, published in 1962. It is set in a near-future society that has a youth subculture of extreme violence. The teenage protagonist, Alex, narrates his violent exploits and his experiences with state authorities intent on reforming him. The book is partially written in a Russian-influenced argot called "Nadsat", which takes its name from the Russian suffix that is equivalent to '-teen' in English. According to Burgess, it was a "jeu d'esprit" written in just three weeks.

In 2005, "A Clockwork Orange" was included on "Time" magazine's list of the 100 best English-language novels written since 1923, and it was named by Modern Library and its readers as one of the 100 best English-language novels of the 20th century. The original manuscript of the book has been located at McMaster University's William Ready Division of Archives and Research Collections in Hamilton, Ontario, Canada since the institution purchased the documents in 1971.

Alex is a 15-year-old living in a near-future dystopian city who leads his gang on a night of opportunistic, random "ultra-violence". Alex's friends ("droogs" in the novel's Anglo-Russian slang, "Nadsat") are Dim, a slow-witted bruiser, who is the gang's muscle; Georgie, an ambitious second-in-command; and Pete, who mostly plays along as the droogs indulge their taste for ultra-violence. Characterised as a sociopath and hardened juvenile delinquent, Alex also displays intelligence, quick wit, and a predilection for classical music; he is particularly fond of Beethoven, referred to as "Lovely Ludwig Van".

The novella begins with the droogs sitting in their favourite hangout, the Korova Milk Bar, and drinking "milk-plus" – a beverage consisting of milk laced with the customer's drug of choice – to prepare for a night of mayhem. They assault a scholar walking home from the public library; rob a store, leaving the owner and his wife bloodied and unconscious; beat up a beggar; then scuffle with a rival gang. Joyriding through the countryside in a stolen car, they break into an isolated cottage and terrorise the young couple living there, beating the husband and raping his wife. In a metafictional touch, the husband is a writer working on a manuscript called ""A Clockwork Orange"", and Alex contemptuously reads out a paragraph that states the novel's main theme before shredding the manuscript. Back at the Korova, Alex strikes Dim for his crude response to a woman's singing of an operatic passage, and strains within the gang become apparent. At home in his parents' futuristic flat, Alex plays classical music at top volume, which he describes as giving him orgasmic bliss before falling asleep.

Alex coyly feigns illness to his parents to stay out of school the next day. Following an unexpected visit from P.R. Deltoid, his "post-corrective adviser", Alex visits a record store, where he meets two pre-teen girls. He invites them back to the flat, where he drugs and rapes them. That night after a nap, Alex finds his droogs in a mutinous mood, waiting downstairs in the torn-up and graffitied lobby. Georgie challenges Alex for leadership of the gang, demanding that they pull a "man-sized" job. Alex quells the rebellion by slashing Dim's hand and fighting with Georgie. Then, in a show of generosity, he takes them to a bar, where Alex insists on following through on Georgie's idea to burgle the home of a wealthy elderly woman. Alex breaks in and knocks the woman unconscious; but, when he opens the door to let the others in, Dim strikes him in payback for the earlier fight. The gang abandons Alex on the front step to be arrested by the police; while in custody, he learns that the woman has died from her injuries.

Alex is convicted of murder and sentenced to 14 years in Wandsworth Prison. His parents visit one day to inform him that Georgie has been killed in a botched robbery. Two years into his term, he has obtained a job in one of the prison chapels, playing Christian music on the stereo to accompany the Sunday Christian services. The chaplain mistakes Alex's Bible studies for stirrings of faith; in reality, Alex is only reading Scripture for the violent passages. After his fellow cellmates blame him for beating a troublesome cellmate to death, he is chosen to undergo an experimental behaviour modification treatment called the Ludovico Technique in exchange for having the remainder of his sentence commuted. The technique is a form of aversion therapy, in which Alex is injected with nausea-inducing drugs while watching graphically violent films, eventually conditioning him to become severely ill at the mere thought of violence. As an unintended consequence, the soundtrack to one of the films, Beethoven's Ninth Symphony, renders Alex unable to enjoy his beloved classical music as before.

The effectiveness of the technique is demonstrated to a group of VIPs, who watch as Alex collapses before a bully and abases himself before a scantily clad young woman whose presence has aroused his predatory sexual inclinations. Although the prison chaplain accuses the state of stripping Alex of free will, the government officials on the scene are pleased with the results and Alex is released from prison.

Alex returns to his parents' flat, only to find that they are letting his room to a lodger. Now homeless, he wanders the streets and enters a public library, hoping to learn of a painless method for committing suicide. The old scholar whom Alex had assaulted in Part 1 finds him and beats him, with the help of several friends. Two policemen come to Alex's rescue, but they turn out to be Dim and Billyboy, a former rival gang leader. They take Alex outside of town, brutalise him, and abandon him there. Alex collapses at the door of an isolated cottage, realising too late that it is the one he and his droogs invaded in Part 1. The writer, F. Alexander, still lives here, but his wife has since died of injuries she sustained in the gang rape. He does not recognise Alex but gives him shelter and questions him about the conditioning he has undergone. Alexander and his colleagues, all highly critical of the government, plan to use Alex as a symbol of state brutality and thus prevent the incumbent government from being re-elected. Alex inadvertently reveals that he was the ringleader of the home invasion; he is removed from the cottage and locked in an upper-story bedroom as a relentless barrage of classical music plays over speakers. He attempts suicide by leaping from the window.

Alex wakes up in a hospital, where he is courted by government officials anxious to counter the bad publicity created by his suicide attempt. Placed in a mental institution, Alex is offered a well-paying job if he agrees to side with the government. A round of tests reveals that his old violent impulses have returned, indicating that the hospital doctors have undone the effects of his conditioning. As photographers snap pictures, Alex daydreams of orgiastic violence and reflects, "I was cured all right."

In the final chapter, Alex finds himself halfheartedly preparing for yet another night of crime with a new gang (Lenn, Rick, Bully). After a chance encounter with Pete, who has reformed and married, Alex finds himself taking less and less pleasure in acts of senseless violence. He begins contemplating giving up crime himself to become a productive member of society and start a family of his own, while reflecting on the notion that his own children could possibly end up being just as destructive as he has been, if not more so.

The book has three parts, each with seven chapters. Burgess has stated that the total of 21 chapters was an intentional nod to the age of 21 being recognised as a milestone in human maturation. The 21st chapter was omitted from the editions published in the United States prior to 1986. In the introduction to the updated American text (these newer editions include the missing 21st chapter), Burgess explains that when he first brought the book to an American publisher, he was told that U.S. audiences would never go for the final chapter, in which Alex sees the error of his ways, decides he has lost all energy for and thrill from violence and resolves to turn his life around (a moment of metanoia).

At the American publisher's insistence, Burgess allowed their editors to cut the redeeming final chapter from the U.S. version, so that the tale would end on a darker note, with Alex succumbing to his violent, reckless nature – an ending which the publisher insisted would be "more realistic" and appealing to a US audience. The film adaptation, directed by Stanley Kubrick, is based on the American edition of the book (which Burgess considered to be "badly flawed"). Kubrick called Chapter 21 "an extra chapter" and claimed that he had not read the original version until he had virtually finished the screenplay, and that he had never given serious consideration to using it. In Kubrick's opinion – as in the opinion of other readers, including the original American editor – the final chapter was unconvincing and inconsistent with the book.


"A Clockwork Orange" was written in Hove, then a senescent seaside town. Burgess had arrived back in Britain after his stint abroad to see that much had changed. A youth culture had grown, including coffee bars, pop music and teenage gangs. England was gripped by fears over juvenile delinquency. Burgess stated that the novel's inspiration was his first wife Lynne's beating by a gang of drunk American servicemen stationed in England during World War II. She subsequently miscarried. In its investigation of free will, the book's target is ostensibly the concept of behaviourism, pioneered by such figures as B. F. Skinner.

Burgess later stated that he wrote the book in three weeks.

Burgess has offered several clarifications about the meaning and origin of its title:

The saying "as queer as..." followed by an improbable object: "...a clockwork orange", or "...a four speed walking stick" or "...a left handed corkscrew" etc. predates Burgess' novel. An early example, "as queer as Dick's hatband", appeared in 1796, and was alluded to in 1757.


This title alludes to the protagonist's negative emotional responses to feelings of evil which prevent the exercise of his free will subsequent to the administration of the Ludovico Technique. To induce this conditioning, Alex is forced to watch scenes of violence on a screen that are systematically paired with negative physical stimulation. The negative physical stimulation takes the form of nausea and "feelings of terror," which are caused by an emetic medicine administered just before the presentation of the films.

The book, narrated by Alex, contains many words in a slang argot which Burgess invented for the book, called Nadsat. It is a mix of modified Slavic words, rhyming slang and derived Russian (like "baboochka"). For instance, these terms have the following meanings in Nadsat: "droog" = friend; "moloko" = milk; "gulliver" ("golova") = head; "malchick" or "malchickiwick" = boy; "soomka" = sack or bag; "Bog" = God; "horrorshow" ("khorosho") = good; "prestoopnick" = criminal; "rooker" ("rooka") = hand; "cal" = crap; "veck" ("chelloveck") = man or guy; "litso" = face; "malenky" = little; and so on. Some words Burgess invented himself or just adapted from pre-existing languages. Compare Polari.

One of Alex's doctors explains the language to a colleague as "odd bits of old rhyming slang; a bit of gypsy talk, too. But most of the roots are Slav propaganda. Subliminal penetration." Some words are not derived from anything, but merely easy to guess, e.g. "in-out, in-out" or "the old in-out" means sexual intercourse. "Cutter", however, means "money", because "cutter" rhymes with "bread-and-butter"; this is rhyming slang, which is intended to be impenetrable to outsiders (especially eavesdropping policemen). Additionally, slang like "appypolly loggy" ("apology") seems to derive from school boy slang. This reflects Alex's age of 15.

In the first edition of the book, no key was provided, and the reader was left to interpret the meaning from the context. In his appendix to the restored edition, Burgess explained that the slang would keep the book from seeming dated, and served to muffle "the raw response of pornography" from the acts of violence.

The term "ultraviolence", referring to excessive or unjustified violence, was coined by Burgess in the book, which includes the phrase "do the ultra-violent". The term's association with aesthetic violence has led to its use in the media.

In 1976, "A Clockwork Orange" was removed from an Aurora, Colorado high school because of "objectionable language". A year later in 1977 it was removed from high school classrooms in Westport, Massachusetts over similar concerns with "objectionable" language. In 1982, it was removed from two Anniston, Alabama libraries, later to be reinstated on a restricted basis. Also, in 1973 a bookseller was arrested for selling the novel. The charges were later dropped. However, each of these instances came after the release of Stanley Kubrick's popular 1971 film adaptation of "A Clockwork Orange", itself the subject of much controversy.

"The Sunday Telegraph" review was positive, and described the book as "entertaining ... even profound". "The Sunday Times" review was negative, and described the book as "a very ordinary, brutal and psychologically shallow story". "The Times" also reviewed the book negatively, describing it as "a somewhat clumsy experiment with science fiction [with] clumsy cliches about juvenile delinquency". The violence was criticised as "unconvincing in detail".

Burgess dismissed "A Clockwork Orange" as "too didactic to be artistic". He claimed that the violent content of the novel "nauseated" him.

In 1985, Burgess published "Flame into Being: The Life and Work of D. H. Lawrence" and while discussing "Lady Chatterley's Lover" in his biography, Burgess compared that novel's notoriety with "A Clockwork Orange": "We all suffer from the popular desire to make the known notorious. The book I am best known for, or only known for, is a novel I am prepared to repudiate: written a quarter of a century ago, a "jeu d'esprit" knocked off for money in three weeks, it became known as the raw material for a film which seemed to glorify sex and violence. The film made it easy for readers of the book to misunderstand what it was about, and the misunderstanding will pursue me until I die. I should not have written the book because of this danger of misinterpretation, and the same may be said of Lawrence and "Lady Chatterley's Lover"."


"A Clockwork Orange" was chosen by "Time" magazine as one of the 100 best English-language books from 1923 to 2005.

A 1965 film by Andy Warhol entitled "Vinyl" was an adaptation of Burgess's novel.

The best known adaptation of the novella to other forms is the 1971 Oscar-nominated film "A Clockwork Orange" by Stanley Kubrick, featuring Malcolm McDowell as Alex. In 1987 Burgess published a stage play titled "A Clockwork Orange: A Play with Music". The play includes songs, written by Burgess, which are inspired by Beethoven and Nadsat slang.

In 1988, a German adaptation of "A Clockwork Orange" at the intimate theatre of Bad Godesberg featured a musical score by the German punk rock band Die Toten Hosen which, combined with orchestral clips of Beethoven's Ninth Symphony and "other dirty melodies" (so stated by the subtitle), was released on the album "Ein kleines bisschen Horrorschau". The track "Hier kommt Alex" became one of the band's signature songs.
In February 1990, another musical version was produced at the Barbican Theatre in London by the Royal Shakespeare Company. Titled "A Clockwork Orange: 2004", it received mostly negative reviews, with John Peter of "The Sunday Times" of London calling it "only an intellectual "Rocky Horror Show"", and John Gross of "The Sunday Telegraph" calling it "a clockwork lemon". Even Burgess himself, who wrote the script based on his novel, was disappointed. According to "The Evening Standard", he called the score, written by Bono and The Edge of the rock group U2, "neo-wallpaper." Burgess had originally worked alongside the director of the production, Ron Daniels, and envisioned a musical score that was entirely classical. Unhappy with the decision to abandon that score, he heavily criticised the band's experimental mix of hip hop, liturgical and gothic music. Lise Hand of "The Irish Independent" reported The Edge as saying that Burgess's original conception was "a score written by a novelist rather than a songwriter". Calling it "meaningless glitz", Jane Edwardes of "20/20 Magazine" said that watching this production was "like being invited to an expensive French Restaurant – and being served with a Big Mac."

In 1994, Chicago's Steppenwolf Theater put on a production of "A Clockwork Orange" directed by Terry Kinney. The American premiere of novelist Anthony Burgess's own adaptation of his "A Clockwork Orange" starred K. Todd Freeman as Alex. In 2001, UNI Theatre (Mississauga, Ontario) presented the Canadian premiere of the play under the direction of Terry Costa.

In 2002, Godlight Theatre Company presented the New York Premiere adaptation of "A Clockwork Orange" at Manhattan Theatre Source. The production went on to play at the SoHo Playhouse (2002), Ensemble Studio Theatre (2004), 59E59 Theaters (2005) and the Edinburgh Festival Fringe (2005). While at Edinburgh, the production received rave reviews from the press while playing to sold-out audiences. The production was directed by Godlight's Artistic Director, Joe Tantalo.

In 2003, Los Angeles director Brad Mays and the ARK Theatre Company staged a multi-media adaptation of "A Clockwork Orange", which was named "Pick Of The Week" by the "LA Weekly" and nominated for three of the 2004 LA Weekly Theater Awards: Direction, Revival Production (of a 20th-century work), and Leading Female Performance. Vanessa Claire Smith won Best Actress for her gender-bending portrayal of Alex, the music-loving teenage sociopath. This production utilised three separate video streams outputted to seven onstage video monitors – six 19-inch and one 40-inch. In order to preserve the first-person narrative of the book, a pre-recorded video stream of Alex, "your humble narrator", was projected onto the 40-inch monitor, thereby freeing the onstage character during passages which would have been awkward or impossible to sustain in the breaking of the fourth wall.

An adaptation of the work, based on the original novel, the film and Burgess's own stage version, was performed by The SiLo Theatre in Auckland, New Zealand in early 2007.







</doc>
<doc id="844" url="https://en.wikipedia.org/wiki?curid=844" title="Amsterdam">
Amsterdam

Amsterdam (, ; ) is the capital and most populous city of the Netherlands with a population of 872,680 within the city proper, 1,380,872 in the urban area and 2,410,960 in the metropolitan area. Found within the province of North Holland, Amsterdam is colloquially referred to as the "Venice of the North", attributed by the large number of canals which form a UNESCO World Heritage Site.

Amsterdam's name derives from "Amstelredamme", indicative of the city's origin around a dam in the river Amstel. Originating as a small fishing village in the late 12th century, Amsterdam became one of the most important ports in the world in the Dutch Golden Age of the 17th century and became the leading centre for finance and trade. In the 19th and 20th centuries, the city expanded, and many new neighbourhoods and suburbs were planned and built. The 17th-century canals of Amsterdam and the 19–20th century Defence Line of Amsterdam are on the UNESCO World Heritage List. Sloten, annexed in 1921 by the municipality of Amsterdam, is the oldest part of the city, dating to the 9th century.

As the commercial capital of the Netherlands and one of the top financial centres in Europe, Amsterdam is considered an alpha-world city by the Globalization and World Cities (GaWC) study group. The city is also the cultural capital of the Netherlands. Many large Dutch institutions have their headquarters there, including Philips, AkzoNobel, TomTom and ING. Also, many of the world's largest companies are based in Amsterdam or have established their European headquarters in the city, such as leading technology companies Uber, Netflix and Tesla. In 2012, Amsterdam was ranked the second best city to live in by the Economist Intelligence Unit (EIU) and 12th globally on quality of living for environment and infrastructure by Mercer. The city was ranked 4th place globally as top tech hub in the Savills Tech Cities 2019 report (2nd in Europe), and 3rd in innovation by Australian innovation agency 2thinknow in their Innovation Cities Index 2009. The Port of Amsterdam is the fifth largest in Europe. Amsterdam Airport Schiphol is the busiest airport in the Netherlands, and the third busiest in Europe. Famous Amsterdam residents include the diarist Anne Frank, artists Rembrandt and Van Gogh, and philosopher Baruch Spinoza.

The Amsterdam Stock Exchange is the oldest stock exchange in the world. Amsterdam's main attractions include its historic canals, the Rijksmuseum, the Van Gogh Museum, the Stedelijk Museum, Hermitage Amsterdam, the Concertgebouw, the Anne Frank House, the Scheepvaartmuseum, the Amsterdam Museum, the Heineken Experience, the Royal Palace of Amsterdam, Natura Artis Magistra, Hortus Botanicus Amsterdam, NEMO, the red-light district and many cannabis coffee shops. They draw more than 5 million international visitors annually. The city is also well known for its nightlife and festival activity; several of its nightclubs (Melkweg, Paradiso) are among the world's most famous. It is also one of the world's most multicultural cities, with at least 177 nationalities represented.

After the floods of 1170 and 1173, locals near the river Amstel built a bridge over the river and a dam across it, giving its name to the village: "Aemstelredamme". The earliest recorded use of that name is in a document dated 27 October 1275, which exempted inhabitants of the village from paying bridge tolls to Count Floris V. This allowed the inhabitants of the village of Aemstelredamme to travel freely through the County of Holland, paying no tolls at bridges, locks and dams. The certificate describes the inhabitants as "homines manentes apud Amestelledamme" (people residing near Amestelledamme). By 1327, the name had developed into "Aemsterdam".

Amsterdam is much younger than Dutch cities such as Nijmegen, Rotterdam, and Utrecht. In October 2008, historical geographer Chris de Bont suggested that the land around Amsterdam was being reclaimed as early as the late 10th century. This does not necessarily mean that there was already a settlement then, since reclamation of land may not have been for farming—it may have been for peat, for use as fuel.

Amsterdam was granted city rights in either 1300 or 1306. From the 14th century on, Amsterdam flourished, largely from trade with the Hanseatic League. In 1345, an alleged Eucharistic miracle in the Kalverstraat rendered the city an important place of pilgrimage until the adoption of the Protestant faith. The Miracle devotion went underground but was kept alive. In the 19th century, especially after the jubilee of 1845, the devotion was revitalized and became an important national point of reference for Dutch Catholics. The "Stille Omgang"—a silent walk or procession in civil attire—is the expression of the pilgrimage within the Protestant Netherlands since the late 19th century. In the heyday of the Silent Walk, up to 90,000 pilgrims came to Amsterdam. In the 21st century this has reduced to about 5000.

In the 16th century, the Dutch rebelled against Philip II of Spain and his successors. The main reasons for the uprising were the imposition of new taxes, the tenth penny, and the religious persecution of Protestants by the newly introduced Inquisition. The revolt escalated into the Eighty Years' War, which ultimately led to Dutch independence. Strongly pushed by Dutch Revolt leader William the Silent, the Dutch Republic became known for its relative religious tolerance. Jews from the Iberian Peninsula, Huguenots from France, prosperous merchants and printers from Flanders, and economic and religious refugees from the Spanish-controlled parts of the Low Countries found safety in Amsterdam. The influx of Flemish printers and the city's intellectual tolerance made Amsterdam a centre for the European free press.

The 17th century is considered Amsterdam's "Golden Age", during which it became the wealthiest city in the western world. Ships sailed from Amsterdam to the Baltic Sea, North America, and Africa, as well as present-day Indonesia, India, Sri Lanka, and Brazil, forming the basis of a worldwide trading network. Amsterdam's merchants had the largest share in both the Dutch East India Company and the Dutch West India Company. These companies acquired overseas possessions that later became Dutch colonies.

Amsterdam was Europe's most important point for the shipment of goods and was the leading Financial centre of the western world. In 1602, the Amsterdam office of the international trading Dutch East India Company became the world's first stock exchange by trading in its own shares. The Bank of Amsterdam started operations in 1609, acting as a full service bank for Dutch merchant bankers and as a reserve bank.

Amsterdam's prosperity declined during the 18th and early 19th centuries. The wars of the Dutch Republic with England and France took their toll on Amsterdam. During the Napoleonic Wars, Amsterdam's significance reached its lowest point, with Holland being absorbed into the French Empire. However, the later establishment of the United Kingdom of the Netherlands in 1815 marked a turning point.
The end of the 19th century is sometimes called Amsterdam's second Golden Age. New museums, a railway station, and the Concertgebouw were built; in this same time, the Industrial Revolution reached the city. The Amsterdam–Rhine Canal was dug to give Amsterdam a direct connection to the Rhine, and the North Sea Canal was dug to give the port a shorter connection to the North Sea. Both projects dramatically improved commerce with the rest of Europe and the world. In 1906, Joseph Conrad gave a brief description of Amsterdam as seen from the seaside, in "The Mirror of the Sea".

Shortly before the First World War, the city started to expand again, and new suburbs were built. Even though the Netherlands remained neutral in this war, Amsterdam suffered a food shortage, and heating fuel became scarce. The shortages sparked riots in which several people were killed. These riots are known as the "Aardappeloproer" (Potato rebellion). People started looting stores and warehouses in order to get supplies, mainly food.

On 1 January 1921, after a flood in 1916, the depleted municipalities of Durgerdam, Holysloot, Zunderdorp and Schellingwoude, all lying north of Amsterdam, were, at their own request, annexed to the city. Between the wars, the city continued to expand, most notably to the west of the Jordaan district in the Frederik Hendrikbuurt and surrounding neighbourhoods.

Nazi Germany invaded the Netherlands on 10 May 1940 and took control of the country. Some Amsterdam citizens sheltered Jews, thereby exposing themselves and their families to a high risk of being imprisoned or sent to concentration camps. More than 100,000 Dutch Jews were deported to Nazi concentration camps, of whom some 60,000 lived in Amsterdam. In response, the Dutch Communist Party organised the February strike attended by 300,000 people to protest against the raids. Perhaps the most famous deportee was the young Jewish girl Anne Frank, who died in the Bergen-Belsen concentration camp. At the end of the Second World War, communication with the rest of the country broke down, and food and fuel became scarce. Many citizens travelled to the countryside to forage. Dogs, cats, raw sugar beets, and Tulip bulbs—cooked to a pulp—were consumed to stay alive. Most of the trees in Amsterdam were cut down for fuel, and all the wood was taken from the apartments of deported Jews.
Many new suburbs, such as Osdorp, Slotervaart, Slotermeer and Geuzenveld, were built in the years after the Second World War.
These suburbs contained many public parks and wide open spaces, and the new buildings provided improved housing conditions with larger and brighter rooms, gardens, and balconies. Because of the war and other events of the 20th century, almost the entire city centre had fallen into disrepair. As society was changing, politicians and other influential figures made plans to redesign large parts of it. There was an increasing demand for office buildings, and also for new roads, as the automobile became available to most people. A metro started operating in 1977 between the new suburb of Bijlmermeer in the city's Zuidoost (southeast) exclave and the centre of Amsterdam. Further plans were to build a new highway above the metro to connect Amsterdam Centraal and city centre with other parts of the city.

The required large-scale demolitions began in Amsterdam's former Jewish neighbourhood. Smaller streets, such as the Jodenbreestraat, were widened and almost all of their houses were demolished. At the peak of the demolition, the "Nieuwmarktrellen" (Nieuwmarkt Riots) broke out; the rioters expressed their fury about the demolition caused by the restructuring of the city.

As a result, the demolition was stopped and the highway was never built; only the metro was completed. Only a few streets remained widened. The new city hall was built on the almost completely demolished Waterlooplein. Meanwhile, large private organisations, such as "Stadsherstel Amsterdam", were founded with the aim of restoring the entire city centre. Although the success of this struggle is visible today, efforts for further restoration are still ongoing. The entire city centre has reattained its former splendour and, as a whole, is now a protected area. Many of its buildings have become monuments, and in July 2010 the Grachtengordel (the three concentric canals: Herengracht, Keizersgracht, and Prinsengracht) was added to the UNESCO World Heritage List.
In the early years of the 21st century, the Amsterdam city centre has attracted large numbers of tourists: between 2012 and 2015, the annual number of visitors rose from 10 million to 17 million. Real estate prices have surged, and local shops are making way for tourist-oriented ones, making the centre unaffordable for the city's inhabitants. These developments have evoked comparisons with Venice, a city thought to be overwhelmed by the tourist influx.

Construction of a metro line connecting the part of the city north of the river (or lake) IJ to the centre was started in 2003. The project was controversial because its cost had exceeded its budget by a factor three by 2008, because of fears of damage to buildings in the centre, and because construction had to be halted and restarted multiple times. The metro line was completed in 2018.

Since 2014, renewed focus has been given to urban regeneration and renewal, especially in areas directly bordering the city centre, such as Frederik Hendrikbuurt. This urban renewal and expansion of the traditional centre of the city—with the construction on artificial islands of the new eastern IJburg neighbourhood—is part of the Structural Vision Amsterdam 2040 initiative.

Amsterdam is located in the Western Netherlands, in the province of North Holland, although it is not its capital which is Haarlem. The river Amstel ends in the city centre and connects to a large number of canals that eventually terminate in the IJ. Amsterdam is about below sea level. The surrounding land is flat as it is formed of large polders. A man-made forest, Amsterdamse Bos, is in the southwest. Amsterdam is connected to the North Sea through the long North Sea Canal.

Amsterdam is intensely urbanised, as is the Amsterdam metropolitan area surrounding the city. Comprising of land, the city proper has 4,457 inhabitants per km and 2,275 houses per km. Parks and nature reserves make up 12% of Amsterdam's land area.

Amsterdam has more than of canals, most of which are navigable by boat. The city's three main canals are the Prinsengracht, Herengracht, and Keizersgracht.

In the Middle Ages, Amsterdam was surrounded by a moat, called the Singel, which now forms the innermost ring in the city, and makes the city centre a horseshoe shape. The city is also served by a seaport. It has been compared with Venice, due to its division into about 90 islands, which are linked by more than 1,200 bridges.

Amsterdam has an oceanic climate (Köppen "Cfb") strongly influenced by its proximity to the North Sea to the west, with prevailing westerly winds. While winters and summers are generally mild, temperatures can vary year by year. There can occasionally be cold snowy winters and hot humid summers.

Amsterdam, as well as most of the North Holland province, lies in USDA Hardiness zone 8b. Frosts mainly occur during spells of easterly or northeasterly winds from the inner European continent. Even then, because Amsterdam is surrounded on three sides by large bodies of water, as well as having a significant heat-island effect, nights rarely fall below , while it could easily be in Hilversum, southeast.

Summers are moderately warm with a number of hot days every month. The average daily high in August is , and or higher is only measured on average on 2.5 days, placing Amsterdam in AHS Heat Zone 2. The record extremes range from to .
Days with more than of precipitation are common, on average 133 days per year.

Amsterdam's average annual precipitation is . A large part of this precipitation falls as light rain or brief showers. Cloudy and damp days are common during the cooler months of October through March.

In 1300, Amsterdam's population was around 1,000 people. While many towns in Holland experienced population decline during the 15th and 16th centuries, Amsterdam's population grew, mainly due to the rise of the profitable Baltic maritime trade after the Burgundian victory in the Dutch–Hanseatic War. Still, the population of Amsterdam was only modest compared to the towns and cities of Flanders and Brabant, which comprised the most urbanised area of the Low Countries.

This changed when, during the Dutch Revolt, many people from the Southern Netherlands fled to the North, especially after Antwerp fell to Spanish forces in 1585. Jewish people from Spain, Portugal and Eastern Europe similarly settled in Amsterdam, as did Germans and Scandinavians. In thirty years, Amsterdam's population more than doubled from 1585 to 1610. By 1600, its population was around 50,000. During the 1660s, Amsterdam's population reached 200,000. The city's growth levelled off and the population stabilised around 240,000 for most of the 18th century.

In 1750, Amsterdam was the fourth largest city in western Europe, behind London (676,000), Paris (560,000) and Naples (324,000). This was all the more remarkable as Amsterdam was neither the capital city nor the seat of government of the Dutch Republic, which itself was a much smaller state than England, France or the Ottoman Empire. In contrast to those other metropolises, Amsterdam was also surrounded by large towns such as Leiden (about 67,000), Rotterdam (45,000), Haarlem (38,000), and Utrecht (30,000).

The city's population declined in the early 19th century, dipping under 200,000 in 1820. By the second half of the 19th century, industrialisation spurred renewed growth. Amsterdam's population hit an all-time high of 872,000 in 1959, before declining in the following decades due to government-sponsored suburbanisation to so-called "groeikernen" (growth centres) such as Purmerend and Almere. Between 1970 and 1980, Amsterdam experienced its sharp population decline, peaking at a net loss of 25,000 people in 1973. By 1985 the city had only 675,570 residents. This was soon followed by reurbanisation and gentrification, leading to renewed population growth in the 2010s. Also in the 2010s, much of Amsterdam's population growth was due to immigration to the city. Amsterdam's population is expected to top its previous high in 2019, reaching 873,000.

In the 16th and 17th century non-Dutch immigrants to Amsterdam were mostly Huguenots, Flemings, Sephardi Jews and Westphalians. Huguenots came after the Edict of Fontainebleau in 1685, while the Flemish Protestants came during the Eighty Years' War. The Westphalians came to Amsterdam mostly for economic reasons – their influx continued through the 18th and 19th centuries. Before the Second World War, 10% of the city population was Jewish. Just twenty per cent of them survived the Shoah.

The first mass immigration in the 20th century were by people from Indonesia, who came to Amsterdam after the independence of the Dutch East Indies in the 1940s and 1950s. In the 1960s guest workers from Turkey, Morocco, Italy and Spain emigrated to Amsterdam. After the independence of Suriname in 1975, a large wave of Surinamese settled in Amsterdam, mostly in the Bijlmer area. Other immigrants, including refugees asylum seekers and illegal immigrants, came from Europe, America, Asia, and Africa. In the 1970s and 1980s, many 'old' Amsterdammers moved to 'new' cities like Almere and Purmerend, prompted by the third planological bill of the Dutch government. This bill promoted suburbanisation and arranged for new developments in so-called "groeikernen", literally "cores of growth". Young professionals and artists moved into neighbourhoods de Pijp and the Jordaan abandoned by these Amsterdammers. The non-Western immigrants settled mostly in the social housing projects in Amsterdam-West and the Bijlmer. Today, people of non-Western origin make up approximately one-third of the population of Amsterdam, and more than 50% of the city'
s children. Ethnic Dutch (as defined by the Dutch census) now make up a minority of the total population, although by far the largest one. Only one in three inhabitants under 15 is an "autochtoon", or a person who has two parents of Dutch origin. Segregation along ethnic lines is clearly visible, with people of non-Western origin, considered a separate group by Statistics Netherlands, concentrating in specific neighbourhoods especially in Nieuw-West, Zeeburg, Bijlmer and in certain areas of Amsterdam-Noord.

In 2000, Christians formed the largest religious group in the city (17% of the population). The next largest religion was Islam (14%), most of whose followers were Sunni.

In 1578, the largely Roman Catholic city of Amsterdam joined the revolt against Spanish rule, late in comparison to other major northern Dutch cities. Roman Catholic priests were driven out of the city. Following the Dutch takeover, all churches were converted to Protestant worship. Calvinism was declared the main religion, and although Catholicism was not forbidden and priests allowed to serve, the Catholic hierarchy was prohibited. This led to the establishment of "schuilkerken", covert religious buildings that were hidden in pre-existing buildings. Catholics, some Jewish and dissenting Protestants worshiped in such buildings. A large influx of foreigners of many religions came to 17th-century Amsterdam, in particular Sefardic Jews from Spain and Portugal, Huguenots from France, Lutherans, Mennonites, and Protestants from across the Netherlands. This led to the establishment of many non-Dutch-speaking churches. In 1603, the Jewish received permission to practice their religion. In 1639, the first synagogue was consecrated. The Jews came to call the town Jerusalem of the West.

As they became established in the city, other Christian denominations used converted Catholic chapels to conduct their own services. The oldest English-language church congregation in the world outside the United Kingdom is found at the Begijnhof. Regular services there are still offered in English under the auspices of the Church of Scotland. Being Calvinists, the Huguenots soon integrated into the Dutch Reformed Church, though often retaining their own congregations. Some, commonly referred by the moniker 'Walloon', are recognisable today as they offer occasional services in French.

In the second half of the 17th century, Amsterdam experienced an influx of Ashkenazim, Jews from Central and Eastern Europe. Jews often fled the pogroms in those areas. The first Ashkenazi who arrived in Amsterdam were refugees from the Khmelnytsky Uprising in eastern Poland and the Thirty Years' War. They not only founded their own synagogues, but had a strong influence on the 'Amsterdam dialect' adding a large Yiddish local vocabulary.

Despite an absence of an official Jewish ghetto, most Jews preferred to live in the eastern part of the old medieval heart of the city. The main street of this Jewish neighbourhood was the "Jodenbreestraat". The neighbourhood comprised the "Waterlooplein" and the Nieuwmarkt. Buildings in this neighbourhood fell into disrepair after the Second World War, and a large section of the neighbourhood was demolished during the construction of the subway. This led to riots, and as a result the original plans for large-scale reconstruction were abandoned. The neighbourhood was rebuilt with smaller-scale residence buildings on the basis of its original layout.

Catholic churches in Amsterdam have been constructed since the restoration of the episcopal hierarchy in 1853. One of the principal architects behind the city's Catholic churches, Cuypers, was also responsible for the Amsterdam Central station and the Rijksmuseum.

In 1924, the Roman Catholic Church of the Netherlands hosted the International Eucharistic Congress in Amsterdam, and numerous Catholic prelates visited the city, where festivities were held in churches and stadiums. Catholic processions on the public streets, however, were still forbidden under law at the time. Only in the 20th century was Amsterdam's relation to Catholicism normalised, but despite its far larger population size, the episcopal see of the city was placed in the provincial town of Haarlem.

In recent times, religious demographics in Amsterdam have been changed by immigration from former colonies. Hinduism has been introduced from the Hindu diaspora from Suriname and several distinct branches of Islam have been brought from various parts of the world. Islam is now the largest non-Christian religion in Amsterdam. The large community of Ghanaian immigrants have established African churches, often in parking garages in the Bijlmer area.

Amsterdam experienced an influx of religions and cultures after the Second World War. With 180 different nationalities, Amsterdam is home to one of the widest varieties of nationalities of any city in the world. The proportion of the population of immigrant origin in the city proper is about 50% and 88% of the population are Dutch citizens.

Amsterdam has been one of the municipalities in the Netherlands which provided immigrants with extensive and free Dutch-language courses, which have benefited many immigrants.

Amsterdam fans out south from the Amsterdam Centraal railway station and Damrak, the main street off the station. The oldest area of the town is known as De Wallen (English: "The Quays"). It lies to the east of Damrak and contains the city's famous red light district. To the south of De Wallen is the old Jewish quarter of Waterlooplein.

The medieval and colonial age canals of Amsterdam, known as "grachten", embraces the heart of the city where homes have interesting gables. Beyond the Grachtengordel are the former working class areas of Jordaan and de Pijp. The Museumplein with the city's major museums, the Vondelpark, a 19th-century park named after the Dutch writer Joost van den Vondel, and the Plantage neighbourhood, with the zoo, are also located outside the Grachtengordel.

Several parts of the city and the surrounding urban area are polders. This can be recognised by the suffix "-meer" which means "lake", as in Aalsmeer, Bijlmermeer, Haarlemmermeer, and Watergraafsmeer.

The Amsterdam canal system is the result of conscious city planning. In the early 17th century, when immigration was at a peak, a comprehensive plan was developed that was based on four concentric half-circles of canals with their ends emerging at the IJ bay. Known as the Grachtengordel, three of the canals were mostly for residential development: the Herengracht (where "Heren" refers to "Heren Regeerders van de stad Amsterdam" (ruling lords of Amsterdam), and "gracht" means canal, so the name can be roughly translated as "Canal of the Lords"), Keizersgracht (Emperor's Canal), and Prinsengracht (Prince's Canal). The fourth and outermost canal is the Singelgracht, which is often not mentioned on maps, because it is a collective name for all canals in the outer ring. The Singelgracht should not be confused with the oldest and innermost canal, the Singel.

The canals served for defence, water management and transport. The defences took the form of a moat and earthen dikes, with gates at transit points, but otherwise no masonry superstructures. The original plans have been lost, so historians, such as Ed Taverne, need to speculate on the original intentions: it is thought that the considerations of the layout were purely practical and defensive rather than ornamental.

Construction started in 1613 and proceeded from west to east, across the breadth of the layout, like a gigantic windshield wiper as the historian Geert Mak calls it – and not from the centre outwards, as a popular myth has it. The canal construction in the southern sector was completed by 1656. Subsequently, the construction of residential buildings proceeded slowly. The eastern part of the concentric canal plan, covering the area between the Amstel river and the IJ bay, has never been implemented. In the following centuries, the land was used for parks, senior citizens' homes, theatres, other public facilities, and waterways without much planning. Over the years, several canals have been filled in, becoming streets or squares, such as the Nieuwezijds Voorburgwal and the Spui.

After the development of Amsterdam's canals in the 17th century, the city did not grow beyond its borders for two centuries. During the 19th century, Samuel Sarphati devised a plan based on the grandeur of Paris and London at that time. The plan envisaged the construction of new houses, public buildings and streets just outside the Grachtengordel. The main aim of the plan, however, was to improve public health. Although the plan did not expand the city, it did produce some of the largest public buildings to date, like the "Paleis voor Volksvlijt".

Following Sarphati, civil engineers Jacobus van Niftrik and Jan Kalff designed an entire ring of 19th-century neighbourhoods surrounding the city's centre, with the city preserving the ownership of all land outside the 17th-century limit, thus firmly controlling development. Most of these neighbourhoods became home to the working class.

In response to overcrowding, two plans were designed at the beginning of the 20th century which were very different from anything Amsterdam had ever seen before: "Plan Zuid", designed by the architect Berlage, and "West". These plans involved the development of new neighbourhoods consisting of housing blocks for all social classes.

After the Second World War, large new neighbourhoods were built in the western, southeastern, and northern parts of the city. These new neighbourhoods were built to relieve the city's shortage of living space and give people affordable houses with modern conveniences. The neighbourhoods consisted mainly of large housing blocks situated among green spaces, connected to wide roads, making the neighbourhoods easily accessible by motor car. The western suburbs which were built in that period are collectively called the Westelijke Tuinsteden. The area to the southeast of the city built during the same period is known as the Bijlmer.

Amsterdam has a rich architectural history. The oldest building in Amsterdam is the Oude Kerk (English: Old Church), at the heart of the Wallen, consecrated in 1306. The oldest wooden building is "Het Houten Huys" at the Begijnhof. It was constructed around 1425 and is one of only two existing wooden buildings. It is also one of the few examples of Gothic architecture in Amsterdam. The oldest stone building of the Netherlands, The Moriaan is build in 's-Hertogenbosch.

In the 16th century, wooden buildings were razed and replaced with brick ones. During this period, many buildings were constructed in the architectural style of the Renaissance. Buildings of this period are very recognisable with their stepped gable façades, which is the common Dutch Renaissance style. Amsterdam quickly developed its own Renaissance architecture. These buildings were built according to the principles of the architect Hendrick de Keyser. One of the most striking buildings designed by Hendrick de Keyer is the Westerkerk. In the 17th century baroque architecture became very popular, as it was elsewhere in Europe. This roughly coincided with Amsterdam's Golden Age. The leading architects of this style in Amsterdam were Jacob van Campen, Philips Vingboons and Daniel Stalpaert.
Philip Vingboons designed splendid merchants' houses throughout the city. A famous building in baroque style in Amsterdam is the Royal Palace on Dam Square. Throughout the 18th century, Amsterdam was heavily influenced by French culture. This is reflected in the architecture of that period. Around 1815, architects broke with the baroque style and started building in different neo-styles. Most Gothic style buildings date from that era and are therefore said to be built in a neo-gothic style. At the end of the 19th century, the Jugendstil or Art Nouveau style became popular and many new buildings were constructed in this architectural style. Since Amsterdam expanded rapidly during this period, new buildings adjacent to the city centre were also built in this style. The houses in the vicinity of the Museum Square in Amsterdam Oud-Zuid are an example of Jugendstil. The last style that was popular in Amsterdam before the modern era was Art Deco. Amsterdam had its own version of the style, which was called the Amsterdamse School. Whole districts were built this style, such as the "Rivierenbuurt". A notable feature of the façades of buildings designed in Amsterdamse School is that they are highly decorated and ornate, with oddly shaped windows and doors.

The old city centre is the focal point of all the architectural styles before the end of the 19th century.
Jugendstil and Georgian are mostly found outside the city's centre in the neighbourhoods built in the early
20th century, although there are also some striking examples of these styles in the city centre.
Most historic buildings in the city centre and nearby are houses, such as the famous merchants' houses lining the canals.

Amsterdam has many parks, open spaces, and squares throughout the city. The Vondelpark, the largest park in the city, is located in the Oud-Zuid neighbourhood and is named after the 17th-century Amsterdam author Joost van den Vondel. Yearly, the park has around 10 million visitors. In the park is an open-air theatre, a playground and several horeca facilities. In the Zuid borough, is the Beatrixpark, named after Queen Beatrix. Between Amsterdam and Amstelveen is the Amsterdamse Bos ("Amsterdam Forest"), the largest recreational area in Amsterdam. Annually, almost 4.5 million people visit the park, which has a size of 1.000 hectares and is approximately three times the size of Central Park. The Amstelpark in the Zuid borough houses the Rieker windmill, which dates to 1636. Other parks include the Sarphatipark in the De Pijp neighbourhood, the Oosterpark in the Oost borough and the Westerpark in the Westerpark neighbourhood. The city has three beaches: Nemo Beach, Citybeach "Het stenen hoofd" (Silodam) and Blijburg, all located in the Centrum borough.

The city has many open squares ("plein" in Dutch). The namesake of the city as the site of the original dam, Dam Square, is the main city square and has the Royal Palace and National Monument. Museumplein hosts various museums, including the Rijksmuseum, Van Gogh Museum, and Stedelijk Museum. Other squares include Rembrandtplein, Muntplein, Nieuwmarkt, Leidseplein, Spui, and Waterlooplein. Also, near to Amsterdam is the Nekkeveld estate conservation project.

Amsterdam is the financial and business capital of the Netherlands.
Amsterdam is ranked fifth best of European cities in which to locate an international business, surpassed by London, Paris, Frankfurt and Barcelona. Many large corporations and banks have their headquarters in Amsterdam, including AkzoNobel, Heineken International, ING Group, ABN AMRO, TomTom, Delta Lloyd Group, Booking.com and Philips. KPMG International's global headquarters is located in nearby Amstelveen, where many non-Dutch companies have settled as well, because surrounding communities allow full land ownership, contrary to Amsterdam's land-lease system.

Though many small offices are still located on the old canals, companies are increasingly relocating outside the city centre. The Zuidas (English: South Axis) has become the new financial and legal hub. The five largest law firms of the Netherlands, a number of Dutch subsidiaries of large consulting firms like Boston Consulting Group and Accenture, and the World Trade Center Amsterdam are also located in Zuidas.

There are three other smaller financial districts in Amsterdam. The first is the area surrounding Amsterdam Sloterdijk railway station, where several newspapers like "De Telegraaf" have their offices.

Also, Deloitte, the Gemeentelijk Vervoerbedrijf (municipal public transport company) and the Dutch tax offices ("Belastingdienst") are located there. The second Financial District is the area surrounding the Johan Cruyff Arena. The third is the area surrounding Amsterdam Amstel railway station. The tallest building in Amsterdam, the Rembrandt Tower, is situated there, as is the headquarters of Philips.

The Port of Amsterdam is the fourth largest port in Europe, the 38th largest port in the world and the second largest port in the Netherlands by metric tons of cargo. In 2014 the Port of Amsterdam had a cargo throughput of 97,4 million tons of cargo, which was mostly bulk cargo.
Amsterdam has the biggest cruise port in the Netherlands with more than 150 cruise ships every year.
In 2019 the new lock in IJmuiden will open; the port will then be able to grow to 125 million tonnes in capacity.

The Amsterdam Stock Exchange (AEX), now part of Euronext, is the world's oldest stock exchange and is one of Europe's largest bourses. It is near Dam Square in the city centre.

Together with Eindhoven (Brainport) and Rotterdam (Seaport), Amsterdam (Airport) forms the foundation of the Dutch economy.

Amsterdam is one of the most popular tourist destinations in Europe, receiving more than 4.63 million international visitors annually, this is excluding the 16 million day-trippers visiting the city every year. The number of visitors has been growing steadily over the past decade. This can be attributed to an increasing number of European visitors. Two-thirds of the hotels are located in the city's centre. Hotels with 4 or 5 stars contribute 42% of the total beds available and 41% of the overnight stays in Amsterdam. The room occupation rate was 78% in 2006, up from 70% in 2005. The majority of tourists (74%) originate from Europe. The largest group of non-European visitors come from the United States, accounting for 14% of the total. Certain years have a theme in Amsterdam to attract extra tourists. For example, the year 2006 was designated "Rembrandt 400", to celebrate the 400th birthday of Rembrandt van Rijn. Some hotels offer special arrangements or activities during these years. The average number of guests per year staying at the four campsites around the city range from 12,000 to 65,000.

De Wallen, also known as Walletjes or Rosse Buurt, is a designated area for legalised prostitution and is Amsterdam's largest and most well known red-light district. This neighbourhood has become a famous attraction for tourists. It consists of a network of roads and alleys containing several hundred small, one-room apartments rented by sex workers who offer their services from behind a window or glass door, typically illuminated with red lights.

Shops in Amsterdam range from large high end department stores such as De Bijenkorf founded in 1870 to small specialty shops. Amsterdam's high-end shops are found in the streets P.C. Hooftstraat and "Cornelis Schuytstraat", which are located in the vicinity of the Vondelpark. One of Amsterdam's busiest high streets is the narrow, medieval Kalverstraat in the heart of the city. Other shopping areas include the "Negen Straatjes" and Haarlemmerdijk and Haarlemmerstraat. "Negen Straatjes" are nine narrow streets within the "Grachtengordel", the concentric canal system of Amsterdam. The Negen Straatjes differ from other shopping districts with the presence of a large diversity of privately owned shops. The Haarlemmerstraat and Haarlemmerdijk were voted best shopping street in the Netherlands in 2011. These streets have as the "Negen Straatjes" a large diversity of privately owned shops. But as the "Negen Straatjes" are dominated by fashion stores the Haarlemmerstraat and Haarlemmerdijk offer a very wide variety of all kinds of stores, just to name some specialties: candy and other food related stores, lingerie, sneakers, wedding clothing, interior shops, books, Italian deli's, racing and mountain bikes, skatewear, etc.

The city also features a large number of open-air markets such as the Albert Cuyp Market, Westerstraat-markt, Ten Katemarkt, and Dappermarkt. Some of these markets are held on a daily basis, like the Albert Cuypmarkt and the Dappermarkt. Others, like the Westerstraatmarkt, are held on a weekly basis.

Several fashion brands and designers are based in Amsterdam. Brands include G-star, 10 feet and Warmenhoven & Venderbos, and fashion designers include Iris van Herpen, Mart Visser, Viktor & Rolf, Marlies Dekkers and Frans Molenaar. Modelling agencies Elite Models, Touche models and Tony Jones have opened branches in Amsterdam. Fashion models like Yfke Sturm, Doutzen Kroes and Kim Noorda started their careers in Amsterdam. Amsterdam has its garment centre in the World Fashion Center. Buildings which formerly housed brothels in the red light district have been converted to ateliers for young fashion designers, AKA eagle fuel. Fashion photographers Inez van Lamsweerde and Vinoodh Matadin were born in Amsterdam.

During the later part of the 16th-century, Amsterdam's Rederijkerskamer (Chamber of rhetoric) organised contests between different Chambers in the reading of poetry and drama. In 1637, Schouwburg, the first theatre in Amsterdam was built, opening on 3 January 1638. The first ballet performances in the Netherlands were given in Schouwburg in 1642 with the "Ballet of the Five Senses". In the 18th century, French theatre became popular. While Amsterdam was under the influence of German music in the 19th century there were few national opera productions; the Hollandse Opera of Amsterdam was built in 1888 for the specific purpose of promoting Dutch opera. In the 19th century, popular culture was centred on the Nes area in Amsterdam (mainly vaudeville and music-hall). An improved metronome was invented in 1812 by Dietrich Nikolaus Winkel. The Rijksmuseum (1885) and Stedelijk Museum (1895) were built and opened. In 1888, the Concertgebouworkest orchestra was established. With the 20th century came cinema, radio and television. Though most studios are located in Hilversum and Aalsmeer, Amsterdam's influence on programming is very strong. Many people who work in the television industry live in Amsterdam. Also, the headquarters of the Dutch SBS Broadcasting Group is located in Amsterdam.

The most important museums of Amsterdam are located on the Museumplein (Museum Square), located at the southwestern side of the Rijksmuseum. It was created in the last quarter of the 19th century on the grounds of the former World's fair. The northeastern part of the square is bordered by the very large Rijksmuseum. In front of the Rijksmuseum on the square itself is a long, rectangular pond. This is transformed into an ice rink in winter. The northwestern part of the square is bordered by the Van Gogh Museum, Stedelijk Museum, House of Bols Cocktail & Genever Experience and Coster Diamonds. The southwestern border of the Museum Square is the Van Baerlestraat, which is a major thoroughfare in this part of Amsterdam. The Concertgebouw is situated across this street from the square. To the southeast of the square are situated a number of large houses, one of which contains the American consulate. A parking garage can be found underneath the square, as well as a supermarket. The Museumplein is covered almost entirely with a lawn, except for the northeastern part of the square which is covered with gravel. The current appearance of the square was realised in 1999, when the square was remodelled. The square itself is the most prominent site in Amsterdam for festivals and outdoor concerts, especially in the summer. Plans were made in 2008 to remodel the square again, because many inhabitants of Amsterdam are not happy with its current appearance.
The Rijksmuseum possesses the largest and most important collection of classical Dutch art.
It opened in 1885. Its collection consists of nearly one million objects. The artist most associated with Amsterdam is Rembrandt, whose work, and the work of his pupils, is displayed in the Rijksmuseum. Rembrandt's masterpiece "The Night Watch" is one of top pieces of art of the museum. It also houses paintings from artists like Van der Helst, Vermeer, Frans Hals, Ferdinand Bol, Albert Cuyp, Jacob van Ruisdael and Paulus Potter. Aside from paintings, the collection consists of a large variety of decorative art. This ranges from Delftware to giant doll-houses from the 17th century. The architect of the gothic revival building was P.J.H. Cuypers. The museum underwent a 10-year, 375 million euro renovation starting in 2003. The full collection was reopened to the public on 13 April 2013 and the Rijksmuseum has remained the most visited museum in Amsterdam with 2.2 million visitors in 2016 and 2.16 million in 2017.

Van Gogh lived in Amsterdam for a short while and there is a museum dedicated to his work. The museum is housed in one of the few modern buildings in this area of Amsterdam. The building was designed by Gerrit Rietveld. This building is where the permanent collection is displayed. A new building was added to the museum in 1999. This building, known as the performance wing, was designed by Japanese architect Kisho Kurokawa. Its purpose is to house temporary exhibitions of the museum. Some of Van Gogh's most famous paintings, like "The Potato Eaters" and "Sunflowers", are in the collection. The Van Gogh museum is the second most visited museum in Amsterdam, not far behind the Rijksmuseum in terms of the number of visits, being approximately 2.1 million in 2016, for example.

Next to the Van Gogh museum stands the Stedelijk Museum. This is Amsterdam's most important museum of modern art. The museum is as old as the square it borders and was opened in 1895. The permanent collection consists of works of art from artists like Piet Mondriaan, Karel Appel, and Kazimir Malevich. After renovations lasting several years the museum opened in September 2012 with a new composite extension that has been called 'The Bathtub' due to its resemblance to one.

Amsterdam contains many other museums throughout the city. They range from small museums such as the Verzetsmuseum (Resistance Museum), the Anne Frank House, and the Rembrandt House Museum, to the very large, like the Tropenmuseum (Museum of the Tropics), Amsterdam Museum (formerly known as Amsterdam Historical Museum), Hermitage Amsterdam (a dependency of the Hermitage Museum in Saint Petersburg) and the Joods Historisch Museum (Jewish Historical Museum). The modern-styled Nemo is dedicated to child-friendly science exhibitions.

Amsterdam's musical culture includes a large collection of songs which treat the city nostalgically and lovingly. The 1949 song "Aan de Amsterdamse grachten" ("On the canals of Amsterdam") was performed and recorded by many artists, including John Kraaijkamp Sr.; the best-known version is probably that by Wim Sonneveld (1962). In the 1950s Johnny Jordaan rose to fame with "Geef mij maar Amsterdam" ("I prefer Amsterdam"), which praises the city above all others (explicitly Paris); Jordaan sang especially about his own neighbourhood, the Jordaan ("Bij ons in de Jordaan"). Colleagues and contemporaries of Johnny include Tante Leen and Manke Nelis. Another notable Amsterdam song is "Amsterdam" by Jacques Brel (1964). A 2011 poll by Amsterdam newspaper "Het Parool" that Trio Bier's "Oude Wolf" was voted "Amsterdams lijflied". Notable Amsterdam bands from the modern era include the Osdorp Posse and The Ex.

AFAS Live (formerly known as the Heineken Music Hall) is a concert hall located near the Johan Cruyff Arena (known as the Amsterdam Arena until 2018). Its main purpose is to serve as a podium for pop concerts for big audiences. Many famous international artists have performed there. Two other notable venues, Paradiso and the Melkweg are located near the Leidseplein. Both focus on broad programming, ranging from indie rock to hip hop, R&B, and other popular genres. Other more subcultural music venues are OCCII, OT301, De Nieuwe Anita, Winston Kingdom and Zaal 100. Jazz has a strong following in Amsterdam, with the Bimhuis being the premier venue. In 2012, Ziggo Dome was opened, also near Amsterdam Arena, a state-of-the-art indoor music arena.

AFAS Live is also host to many electronic dance music festivals, alongside many other venues. Armin van Buuren and Tiesto, some of the world's leading Trance DJ's hail from the Netherlands and perform frequently in Amsterdam. Each year in October, the city hosts the Amsterdam Dance Event (ADE) which is one of the leading electronic music conferences and one of the biggest club festivals for electronic music in the world, attracting over 350,000 visitors each year. Another popular dance festival is 5daysoff, which takes place in the venues Paradiso and Melkweg. In summer time there are several big outdoor dance parties in or nearby Amsterdam, such as Awakenings, Dance Valley, Mystery Land, Loveland, A Day at the Park, Welcome to the Future, and Valtifest.
Amsterdam has a world-class symphony orchestra, the Royal Concertgebouw Orchestra. Their home is the Concertgebouw, which is across the Van Baerlestraat from the Museum Square. It is considered by critics to be a concert hall with some of the best acoustics in the world. The building contains three halls, Grote Zaal, Kleine Zaal, and Spiegelzaal. Some nine hundred concerts and other events per year take place in the Concertgebouw, for a public of over 700,000, making it one of the most-visited concert halls in the world. The opera house of Amsterdam is situated adjacent to the city hall. Therefore, the two buildings combined are often called the Stopera, (a word originally coined by protesters against it very construction: "Stop the Opera[-house]"). This huge modern complex, opened in 1986, lies in the former Jewish neighbourhood at "Waterlooplein" next to the river Amstel. The "Stopera" is the homebase of Dutch National Opera, Dutch National Ballet and the Holland Symfonia. Muziekgebouw aan 't IJ is a concert hall, which is situated in the IJ near the central station. Its concerts perform mostly modern classical music. Located adjacent to it, is the "Bimhuis", a concert hall for improvised and Jazz music.

Amsterdam has three main theatre buildings.

The Stadsschouwburg at the Leidseplein is the home base of Toneelgroep Amsterdam. The current building dates from 1894. Most plays are performed in the Grote Zaal (Great Hall). The normal programme of events encompasses all sorts of theatrical forms. The Stadsschouwburg is currently being renovated and expanded. The third theatre space, to be operated jointly with next door Melkweg, will open in late 2009 or early 2010.

The Dutch National Opera and Ballet (formerly known as "Het Muziektheater"), dating from 1986, is the principal opera house and home to Dutch National Opera and Dutch National Ballet. Royal Theatre Carré was built as a permanent circus theatre in 1887 and is currently mainly used for musicals, cabaret performances and pop concerts.

The recently re-opened DeLaMar Theater houses the more commercial plays and musicals. A new theatre has also moved into Amsterdam scene in 2014, joining other established venues: Theater Amsterdam is situated in the west part of Amsterdam, on the Danzigerkade. It is housed in a modern building with a panoramic view over the harbour. The theatre is the first ever purpose-built venue to showcase a single play entitled ANNE, the play based on Anne Frank's life.

On the east side of town, there is a small theatre in a converted bath house, the Badhuistheater. The theatre often has English programming.

The Netherlands has a tradition of cabaret or "kleinkunst", which combines music, storytelling, commentary, theatre and comedy. Cabaret dates back to the 1930s and artists like Wim Kan, Wim Sonneveld and Toon Hermans were pioneers of this form of art in the Netherlands. In Amsterdam is the Kleinkunstacademie (English: Cabaret Academy). Contemporary popular artists are Youp van 't Hek, Freek de Jonge, Herman Finkers, Hans Teeuwen, Theo Maassen, Herman van Veen, Najib Amhali, Raoul Heertje, Jörgen Raymann, Brigitte Kaandorp and Comedytrain. The English spoken comedy scene was established with the founding of Boom Chicago in 1993. They have their own theatre at Leidseplein.

Amsterdam is famous for its vibrant and diverse nightlife. Amsterdam has many "cafés" (bars). They range from large and modern to small and cozy. The typical "Bruine Kroeg" (brown "café") breathe a more old fashioned atmosphere with dimmed lights, candles, and somewhat older clientele. These brown cafés mostly offer a wide range of local and international artesanal beers. Most "cafés" have terraces in summertime. A common sight on the Leidseplein during summer is a square full of terraces packed with people drinking beer or wine. Many restaurants can be found in Amsterdam as well. Since Amsterdam is a multicultural city, a lot of different ethnic restaurants can be found. Restaurants range from being rather luxurious and expensive to being ordinary and affordable. Amsterdam also possesses many discothèques. The two main nightlife areas for tourists are the Leidseplein and the Rembrandtplein. The Paradiso, Melkweg and Sugar Factory are cultural centres, which turn into discothèques on some nights. Examples of discothèques near the Rembrandtplein are the Escape, Air, John Doe and Club Abe. Also noteworthy are Panama, Hotel Arena (East), TrouwAmsterdam and Studio 80. In recent years '24-hour' clubs opened their doors, most notably Radion and De School. Bimhuis located near the Central Station, with its rich programming hosting the best in the field is considered one of the best jazz clubs in the world. The Reguliersdwarsstraat is the main street for the LGBT community and nightlife.

In 2008, there were 140 festivals and events in Amsterdam.

Famous festivals and events in Amsterdam include: "Koningsdag" (which was named "Koninginnedag" until the crowning of King Willem-Alexander in 2013) (King's Day – Queen's Day); the Holland Festival for the performing arts; the yearly Prinsengrachtconcert (classical concerto on the Prinsen canal) in August; the 'Stille Omgang' (a silent Roman Catholic evening procession held every March); Amsterdam Gay Pride; The Cannabis Cup; and the Uitmarkt. On Koningsdag—that is held each year on 27 April—hundreds of thousands of people travel to Amsterdam to celebrate with the city's residents. The entire city becomes overcrowded with people buying products from the "freemarket," or visiting one of the many music concerts.
The yearly Holland Festival attracts international artists and visitors from all over Europe. Amsterdam Gay Pride is a yearly local LGBT parade of boats in Amsterdam's canals, held on the first Saturday in August. The annual Uitmarkt is a three-day cultural event at the start of the cultural season in late August. It offers previews of many different artists, such as musicians and poets, who perform on podia.

Amsterdam is home of the "Eredivisie" football club AFC Ajax. The stadium Johan Cruyff Arena is the home of Ajax. It is located in the south-east of the city next to the new Amsterdam Bijlmer ArenA railway station. Before moving to their current location in 1996, Ajax played their regular matches in De Meer Stadion.
In 1928, Amsterdam hosted the Summer Olympics. The Olympic Stadium built for the occasion has been completely restored and is now used for cultural and sporting events, such as the Amsterdam Marathon. In 1920, Amsterdam assisted in hosting some of the sailing events for the Summer Olympics held in neighbouring Antwerp, Belgium by hosting events at Buiten Y.

The city holds the Dam to Dam Run, a race from Amsterdam to Zaandam, as well as the Amsterdam Marathon. The ice hockey team Amstel Tijgers play in the Jaap Eden ice rink. The team competes in the Dutch ice hockey premier league. Speed skating championships have been held on the 400-metre lane of this ice rink.

Amsterdam holds two American football franchises: the Amsterdam Crusaders and the Amsterdam Panthers. The Amsterdam Pirates baseball team competes in the Dutch Major League. There are three field hockey teams: Amsterdam, Pinoké and Hurley, who play their matches around the Wagener Stadium in the nearby city of Amstelveen. The basketball team MyGuide Amsterdam competes in the Dutch premier division and play their games in the Sporthallen Zuid.

There is one rugby club in Amsterdam, which also hosts sports training classes such as RTC (Rugby Talenten Centrum or Rugby Talent Centre) and the National Rugby stadium.

Since 1999, the city of Amsterdam honours the best sportsmen and women at the Amsterdam Sports Awards. Boxer Raymond Joval and field hockey midfielder Carole Thate were the first to receive the awards, in 1999.

Amsterdam hosted the World Gymnaestrada in 1991 and will do so again in 2023.

The city of Amsterdam is a municipality under the Dutch Municipalities Act. It is governed by a directly elected municipal council, a municipal executive board and a mayor. Since 1981, the municipality of Amsterdam has gradually been divided into semi-autonomous boroughs, called "stadsdelen" or 'districts'. Over time, a total of 15 boroughs were created. In May 2010, under a major reform, the number of Amsterdam boroughs was reduced to eight: Amsterdam-Centrum covering the city centre including the canal belt, Amsterdam-Noord consisting of the neighbourhoods north of the IJ lake, Amsterdam-Oost in the east, Amsterdam-Zuid in the south, Amsterdam-West in the west, Amsterdam Nieuw-West in the far west, Amsterdam Zuidoost in the southeast, and Westpoort covering the Port of Amsterdam area.

As with all Dutch municipalities, Amsterdam is governed by a directly elected municipal council, a municipal executive board and a government appointed mayor ("burgemeester"). The mayor is a member of the municipal executive board, but also has individual responsibilies in maintaining public order. On 27 June 2018, Femke Halsema (former member of House of Representatives for GroenLinks from 1998 to 2011) was appointed as the first woman to be Mayor of Amsterdam by the King's Commissioner of North Holland for a six-year term after being nominated by the Amsterdam municipal council and began serving a six-year term on 12 July 2018. She replaces Eberhard van der Laan (Labour Party) who was the Mayor of Amsterdam from 2010 until his death in October 2017. After the 2014 municipal council elections, a governing majority of D66, VVD and SP was formed – the first coalition without the Labour Party since World War II. Next to the Mayor, the municipal executive board consists of eight "wethouders" ('alderpersons') appointed by the municipal council: four D66 alderpersons, two VVD alderpersons and two SP alderpersons.

On 18 September 2017, it was announced by Eberhard van der Laan in an open letter to Amsterdam citizens that Kajsa Ollongren would take up his office as acting Mayor of Amsterdam with immediate effect due to ill health. Ollongren was succeeded as acting Mayor by Eric van der Burg on 26 October 2017 and by Jozias van Aartsen on 4 December 2017.
Unlike most other Dutch municipalities, Amsterdam is subdivided into eight boroughs, called "stadsdelen" or 'districts', a system that was implemented gradually in the 1980s to improve local governance. The boroughs are responsible for many activities that had previously been run by the central city. In 2010, the number of Amsterdam boroughs reached fifteen. Fourteen of those had their own district council ("deelraad"), elected by a popular vote. The fifteenth, Westpoort, covers the harbour of Amsterdam and had very few residents. Therefore, it was governed by the central municipal council.

Under the borough system, municipal decisions are made at borough level, except for those affairs pertaining to the whole city such as major infrastructure projects, which are the jurisdiction of the central municipal authorities. In 2010, the borough system was restructured, in which many smaller boroughs merged into larger boroughs. In 2014, under a reform of the Dutch Municipalities Act, the Amsterdam boroughs lost much of their autonomous status, as their district councils were abolished.

The municipal council of Amsterdam voted to maintain the borough system by replacing the district councils with smaller, but still directly elected district committees ("bestuurscommissies"). Under a municipal ordinance, the new district committees were granted responsibilities through delegation of regulatory and executive powers by the central municipal council.

"Amsterdam" is usually understood to refer to the municipality of Amsterdam. Colloquially, some areas within the municipality, such as the town of Durgerdam, may not be considered part of Amsterdam.

Statistics Netherlands uses three other definitions of Amsterdam: metropolitan agglomeration Amsterdam ("Grootstedelijke Agglomeratie Amsterdam", not to be confused with "Grootstedelijk Gebied Amsterdam", a synonym of "Groot Amsterdam"), Greater Amsterdam ("Groot Amsterdam", a COROP region) and the urban region Amsterdam ("Stadsgewest Amsterdam"). The Amsterdam Department for Research and Statistics uses a fourth conurbation, namely the "Stadsregio Amsterdam" ('City Region of Amsterdam'). The city region is similar to Greater Amsterdam but includes the municipalities of Zaanstad and Wormerland. It excludes Graft-De Rijp.

The smallest of these areas is the municipality of Amsterdam with a population of 802,938 in 2013. The conurbation had a population of 1,096,042 in 2013. It includes the municipalities of Zaanstad, Wormerland, Oostzaan, Diemen and Amstelveen only, as well as the municipality of Amsterdam. Greater Amsterdam includes 15 municipalities, and had a population of 1,293,208 in 2013. Though much larger in area, the population of this area is only slightly larger, because the definition excludes the relatively populous municipality of Zaanstad. The largest area by population, the Amsterdam Metropolitan Area (Dutch: Metropoolregio Amsterdam), has a population of 2,33 million. It includes for instance Zaanstad, Wormerland, Muiden, Abcoude, Haarlem, Almere and Lelystad but excludes Graft-De Rijp. Amsterdam is part of the conglomerate metropolitan area Randstad, with a total population of 6,659,300 inhabitants.

Of these various metropolitan area configurations, only the "Stadsregio Amsterdam" (City Region of Amsterdam) has a formal governmental status. Its responsibities include regional spatial planning and the metropolitan public transport concessions.

Under the Dutch Constitution, Amsterdam is the capital of the Netherlands. Since the 1983 constitutional revision, the constitution mentions "Amsterdam" and "capital" in chapter 2, article 32: The king's confirmation by oath and his coronation take place in "the capital Amsterdam" (""de hoofdstad Amsterdam""). Previous versions of the constitution only mentioned "the city of Amsterdam" (""de stad Amsterdam""). For a royal investiture, therefore, the States General of the Netherlands (the Dutch Parliament) meets for a ceremonial joint session in Amsterdam. The ceremony traditionally takes place at the Nieuwe Kerk on Dam Square, immediately after the former monarch has signed the act of abdication at the nearby Royal Palace of Amsterdam. Normally, however, the Parliament sits in The Hague, the city which has historically been the seat of the Dutch government, the Dutch monarchy, and the Dutch supreme court. Foreign embassies are also located in The Hague.

The coat of arms of Amsterdam is composed of several historical elements. First and centre are three St Andrew's crosses, aligned in a vertical band on the city's shield (although Amsterdam's patron saint was Saint Nicholas). These St Andrew's crosses can also be found on the cityshields of neighbours Amstelveen and Ouder-Amstel. This part of the coat of arms is the basis of the flag of Amsterdam, flown by the city government, but also as civil ensign for ships registered in Amsterdam. Second is the Imperial Crown of Austria. In 1489, out of gratitude for services and loans, Maximilian I awarded Amsterdam the right to adorn its coat of arms with the king's crown. Then, in 1508, this was replaced with Maximilian's imperial crown when he was crowned Holy Roman Emperor. In the early years of the 17th century, Maximilian's crown in Amsterdam's coat of arms was again replaced, this time with the crown of Emperor Rudolph II, a crown that became the Imperial Crown of Austria. The lions date from the late 16th century, when city and province became part of the Republic of the Seven United Netherlands. Last came the city's official motto: "Heldhaftig, Vastberaden, Barmhartig" ("Heroic, Determined, Merciful"), bestowed on the city in 1947 by Queen Wilhelmina, in recognition of the city's bravery during the Second World War.

Currently, there are sixteen tram routes and five metro routes. All are operated by municipal public transport operator Gemeentelijk Vervoerbedrijf (GVB), which also runs the city bus network.

Four fare-free GVB ferries carry pedestrians and cyclists across the IJ lake to the borough of Amsterdam-Noord, and two fare-charging ferries run east and west along the harbour. There are also privately operated water taxis, a water bus, a boat sharing operation, electric rental boats and canal cruises, that transport people along Amsterdam's waterways.

Regional buses, and some suburban buses, are operated by Connexxion and EBS. International coach services are provided by Eurolines from Amsterdam Amstel railway station, IDBUS from Amsterdam Sloterdijk railway station, and Megabus from the Zuiderzeeweg in the east of the city.

In order to facilitate easier transport to the center of Amsterdam, the city has various P+R Locations where people can park their car at an affordable price and transfer to one of the numerous public transport lines.

Amsterdam was intended in 1932 to be the hub, a kind of Kilometre Zero, of the highway system of the Netherlands, with freeways numbered One to Eight planned to originate from the city. The outbreak of the Second World War and shifting priorities led to the current situation, where only roads A1, A2, and A4 originate from Amsterdam according to the original plan. The A3 to Rotterdam was cancelled in 1970 in order to conserve the Groene Hart. Road A8, leading north to Zaandam and the A10 Ringroad were opened between 1968 and 1974. Besides the A1, A2, A4 and A8, several freeways, such as the A7 and A6, carry traffic mainly bound for Amsterdam.

The A10 ringroad surrounding the city connects Amsterdam with the Dutch national network of freeways. Interchanges on the A10 allow cars to enter the city by transferring to one of the 18 "city roads", numbered S101 through to S118. These city roads are regional roads without grade separation, and sometimes without a central reservation. Most are accessible by cyclists. The S100 "Centrumring" is a smaller ringroad circumnavigating the city's centre.

In the city centre, driving a car is discouraged. Parking fees are expensive, and many streets are closed to cars or are one-way. The local government sponsors carsharing and carpooling initiatives such as "Autodelen" and "Meerijden.nu".

Amsterdam is served by ten stations of the Nederlandse Spoorwegen (Dutch Railways). Five are intercity stops: Sloterdijk, Zuid, Amstel, Bijlmer ArenA and Amsterdam Centraal. The stations for local services are: Lelylaan, RAI, Holendrecht, Muiderpoort and Science Park. Amsterdam Centraal is also an international railway station. From the station there are regular services to destinations such as Austria, Belarus, Belgium, the Czech Republic, Denmark, France, Germany, Hungary, Poland, Russia, Switzerland and the United Kingdom. Among these trains are international trains of the Nederlandse Spoorwegen (Amsterdam-Berlin), the Eurostar (Amsterdam-Brussels-London), Thalys (Amsterdam-Brussels-Paris/Lille), and Intercity-Express (Amsterdam–Cologne–Frankfurt).

Amsterdam Airport Schiphol is less than 20 minutes by train from Amsterdam Centraal station and is served by domestic and international intercity trains, such as Thalys, Eurostar and Intercity Brussel. Schiphol is the largest airport in the Netherlands, the third largest in Europe, and the 14th-largest in the world in terms of passengers. It handles over 68 million passengers per year and is the home base of four airlines, KLM, Transavia, Martinair and Arkefly. , Schiphol was the fifth busiest airport in the world measured by international passenger numbers. This airport is 4 meters below sea level.

Amsterdam is one of the most bicycle-friendly large cities in the world and is a centre of bicycle culture with good facilities for cyclists such as bike paths and bike racks, and several guarded bike storage garages ("fietsenstalling") which can be used.

According to the most recent figures published by Central Bureau of Statistics (CBS), in 2015 the 442.693 households (850.000 residents) in Amsterdam together owned 847.000 bicycles – 1.91 bicycle per household. Previously, wildly different figures were arrived at using a Wisdom of the crowd approach. Theft is widespreadin 2011, about 83,000 bicycles were stolen in Amsterdam. Bicycles are used by all socio-economic groups because of their convenience, Amsterdam's small size, the of bike paths, the flat terrain, and the inconvenience of driving an automobile.

Amsterdam has two universities: the University of Amsterdam ("Universiteit van Amsterdam", UvA), and the "Vrije Universiteit Amsterdam" (VU). Other institutions for higher education include an art school – Gerrit Rietveld Academie, a university of applied sciences – the Hogeschool van Amsterdam, and the Amsterdamse Hogeschool voor de Kunsten. Amsterdam's International Institute of Social History is one of the world's largest documentary and research institutions concerning social history, and especially the history of the labour movement. Amsterdam's Hortus Botanicus, founded in the early 17th century, is one of the oldest botanical gardens in the world, with many old and rare specimens, among them the coffee plant that served as the parent for the entire coffee culture in Central and South America.

There are over 200 primary schools in Amsterdam. Some of these primary schools base their teachings on particular pedagogic theories like the various Montessori schools. The biggest Montessori high school in Amsterdam is the Montessori Lyceum Amsterdam. Many schools, however, are based on religion. This used to be primarily Roman Catholicism and various Protestant denominations, but with the influx of Muslim immigrants there has been a rise in the number of Islamic schools. Jewish schools can be found in the southern suburbs of Amsterdam.

Amsterdam is noted for having five independent grammar schools (Dutch: gymnasia), the Vossius Gymnasium, Barlaeus Gymnasium, St. Ignatius Gymnasium, Het 4e Gymnasium and the Cygnus Gymnasium where a classical curriculum including Latin and classical Greek is taught. Though believed until recently by many to be an anachronistic and elitist concept that would soon die out, the gymnasia have recently experienced a revival, leading to the formation of a fourth and fifth grammar school in which the three aforementioned schools participate. Most secondary schools in Amsterdam offer a variety of different levels of education in the same school. The city also has various colleges ranging from art and design to politics and economics which are mostly also available for students coming from other countries.

Schools for foreign nationals in Amsterdam include the Amsterdam International Community School, British School of Amsterdam, Albert Einstein International School Amsterdam, Lycée Vincent van Gogh La Haye-Amsterdam primary campus (French school), International School of Amsterdam, and the Japanese School of Amsterdam.

Amsterdam is a prominent centre for national and international media. Some locally based newspapers include "Het Parool", a national daily paper; "De Telegraaf", the largest Dutch daily newspaper; the daily newspapers "Trouw", "de Volkskrant" and "NRC Handelsblad"; "De Groene Amsterdammer", a weekly newspaper; the free newspapers "Metro" and "The Holland Times" (printed in English).

Amsterdam is home to the second-largest Dutch commercial TV group SBS Broadcasting Group, consisting of TV-stations SBS 6, Net 5 and Veronica. However, Amsterdam is not considered 'the media city of the Netherlands'. The town of Hilversum, south-east of Amsterdam, has been crowned with this unofficial title. Hilversum is the principal centre for radio and television broadcasting in the Netherlands. Radio Netherlands, heard worldwide via shortwave radio since the 1920s, is also based there. Hilversum is home to an extensive complex of audio and television studios belonging to the national broadcast production company NOS, as well as to the studios and offices of all the Dutch public broadcasting organisations and many commercial TV production companies.

In 2012, the music video of Far East Movement, 'Live My Life', was filmed in various parts of Amsterdam.

Also, several movies were filmed in Amsterdam, such as James Bond's Diamonds Are Forever, Ocean's Twelve, Girl with a Pearl Earring and The Hitman's Bodyguard. Amsterdam is also featured in John Green's book "The Fault in Our Stars", which has been made into a film as well that partly takes place in Amsterdam.

The housing market is heavily regulated. In Amsterdam, 55% of existing housing and 30% of new housing is owned by Housing Associations, which are Government sponsored entities.

From the late 1960s onwards many buildings in Amsterdam have been squatted both for housing and for using as social centres. A number of these squats have legalised and become well known, such as OCCII, OT301, Paradiso and Vrankrijk.





</doc>
<doc id="846" url="https://en.wikipedia.org/wiki?curid=846" title="Museum of Work">
Museum of Work

The Museum of Work ("Arbetetsmuseum") is a museum located in Norrköping, Sweden. The museum is located in the "Styrkjärn", a former weaving mill in the old industrial area on the Motala ström river in the city centre of Norrköping. The former textile factory Holmens Bruks och Fabriks operated in the building from 1917-1962.

The museum documents work and everyday life by collecting personal stories about people's professional lives from both the past and the present. The museum's archive contain material from memory collections and documentation projects.

Since 2009, the museum also houses the EWK - Center for Political Illustration Art, which is based on work of the satirist Ewert Karlsson (1918–2004). For decades he was frequently published in the Swedish tabloid, "Aftonbladet".


</doc>
<doc id="848" url="https://en.wikipedia.org/wiki?curid=848" title="Audi">
Audi

Audi AG () is a German automobile manufacturer that designs, engineers, produces, markets and distributes luxury vehicles. Audi is a member of the Volkswagen Group and has its roots at Ingolstadt, Bavaria, Germany. Audi-branded vehicles are produced in nine production facilities worldwide.

The origins of the company are complex, going back to the early 20th century and the initial enterprises (Horch and the "Audiwerke") founded by engineer August Horch; and two other manufacturers (DKW and Wanderer), leading to the foundation of Auto Union in 1932. The modern era of Audi essentially began in the 1960s when Auto Union was acquired by Volkswagen from Daimler-Benz. After relaunching the Audi brand with the 1965 introduction of the Audi F103 series, Volkswagen merged Auto Union with NSU Motorenwerke in 1969, thus creating the present day form of the company.

The company name is based on the Latin translation of the surname of the founder, August Horch. "Horch", meaning "listen" in German, becomes "audi" in Latin. The four rings of the Audi logo each represent one of four car companies that banded together to create Audi's predecessor company, Auto Union. Audi's slogan is "Vorsprung durch Technik", meaning "Being Ahead through Technology". However, Audi USA had used the slogan "Truth in Engineering" from 2007 to 2016, and have not used the slogan since 2016. Audi, along with fellow German marques BMW and Mercedes-Benz, is among the best-selling luxury automobile brands in the world.

Automobile company Wanderer was originally established in 1885, later becoming a branch of Audi AG. Another company, NSU, which also later merged into Audi, was founded during this time, and later supplied the chassis for Gottlieb Daimler's four-wheeler.

On 14 November, 1899, August Horch (1868–1951) established the company A. Horch & Cie. in the Ehrenfeld district of Cologne. In 1902, he moved with his company to Reichenbach im Vogtland. On 10 May, 1904, he founded the August Horch & Cie. Motorwagenwerke AG, a joint-stock company in Zwickau (State of Saxony).

After troubles with Horch chief financial officer, August Horch left Motorwagenwerke and founded in Zwickau on 16 July, 1909, his second company, the August Horch Automobilwerke GmbH. His former partners sued him for trademark infringement. The German Reichsgericht (Supreme Court) in Leipzig, eventually determined that the Horch brand belonged to his former company.

Since August Horch was prohibited from using "Horch" as a trade name in his new car business, he called a meeting with close business friends, Paul and Franz Fikentscher from Zwickau. At the apartment of Franz Fikentscher, they discussed how to come up with a new name for the company. During this meeting, Franz's son was quietly studying Latin in a corner of the room. Several times he looked like he was on the verge of saying something but would just swallow his words and continue working, until he finally blurted out, "Father – "audiatur et altera pars"... wouldn't it be a good idea to call it "audi" instead of "horch"?" "Horch!" in German means "Hark!" or "hear", which is "Audi" in the singular imperative form of "audire" – "to listen" – in Latin. The idea was enthusiastically accepted by everyone attending the meeting. On 25 April 1910 the Audi Automobilwerke GmbH Zwickau (from 1915 on Audiwerke AG Zwickau) was entered in the company's register of Zwickau registration court.

The first Audi automobile, the Audi Type A 10/ Sport-Phaeton, was produced in the same year, followed by the successor Type B 10/28PS in the same year.

Audi started with a 2,612 cc inline-four engine model Type A, followed by a 3,564 cc model, as well as 4,680 cc and 5,720 cc models. These cars were successful even in sporting events. The first six-cylinder model Type M, 4,655 cc appeared in 1924.

August Horch left the "Audiwerke" in 1920 for a high position at the ministry of transport, but he was still involved with Audi as a member of the board of trustees. In September 1921, Audi became the first German car manufacturer to present a production car, the Audi Type K, with left-handed drive. Left-hand drive spread and established dominance during the 1920s because it provided a better view of oncoming traffic, making overtaking safer when driving on the right.

In August 1928, Jørgen Rasmussen, the owner of Dampf-Kraft-Wagen (DKW), acquired the majority of shares in Audiwerke AG. In the same year, Rasmussen bought the remains of the U.S. automobile manufacturer Rickenbacker, including the manufacturing equipment for 8-cylinder engines. These engines were used in "Audi Zwickau" and "Audi Dresden" models that were launched in 1929. At the same time, 6-cylinder and 4-cylinder (the "four" with a Peugeot engine) models were manufactured. Audi cars of that era were luxurious cars equipped with special bodywork.

In 1932, Audi merged with Horch, DKW, and Wanderer, to form Auto Union AG, Chemnitz. It was during this period that the company offered the Audi Front that became the first European car to combine a six-cylinder engine with front-wheel drive. It used a powertrain shared with the Wanderer, but turned 180-degrees, so that the drive shaft faced the front.

Before World War II, Auto Union used the four interlinked rings that make up the Audi badge today, representing these four brands. However, this badge was used only on Auto Union racing cars in that period while the member companies used their own names and emblems. The technological development became more and more concentrated and some Audi models were propelled by Horch or Wanderer built engines.

Reflecting the economic pressures of the time, Auto Union concentrated increasingly on smaller cars through the 1930s, so that by 1938 the company's DKW brand accounted for 17.9% of the German car market, while Audi held only 0.1%. After the final few Audis were delivered in 1939 the "Audi" name disappeared completely from the new car market for more than two decades.

Like most German manufacturing, at the onset of World War II the Auto Union plants were retooled for military production, and were a target for allied bombing during the war which left them damaged.

Overrun by the Soviet Army in 1945, on the orders of the Soviet Union military administration the factories were dismantled as part of war reparations. Following this, the company's entire assets were expropriated without compensation. On 17 August 1948, Auto Union AG of Chemnitz was deleted from the commercial register. These actions had the effect of liquidating Germany's Auto Union AG. The remains of the Audi plant of Zwickau became the VEB (for "People Owned Enterprise") or AWZ (in English: Automobile Works Zwickau).

With no prospect of continuing production in Soviet-controlled East Germany, Auto Union executives began the process of relocating what was left of the company to West Germany. A site was chosen in Ingolstadt, Bavaria, to start a spare parts operation in late 1945, which would eventually serve as the headquarters of the reformed Auto Union in 1949.

The former Audi factory in Zwickau restarted assembly of the pre-war-models in 1949. These DKW models were renamed to IFA F8 and IFA F9 and were similar to the West German versions. West and East German models were equipped with the traditional and renowned DKW two-stroke engines. The Zwickau plant manufactured the infamous Trabant until 1991, when it came under Volkswagen control—effectively bringing it under the same umbrella as Audi since 1945.

A new West German headquartered Auto Union was launched in Ingolstadt with loans from the Bavarian state government and Marshall Plan aid. The reformed company was launched 3 September 1949 and continued DKW's tradition of producing front-wheel drive vehicles with two-stroke engines. This included production of a small but sturdy 125 cc motorcycle and a DKW delivery van, the DKW F89 L at Ingolstadt. The Ingolstadt site was large, consisting of an extensive complex of formerly military buildings which was suitable for administration as well as vehicle warehousing and distribution, but at this stage there was at Ingolstadt no dedicated plant suitable for mass production of automobiles: for manufacturing the company's first post-war mass-market passenger car plant capacity in Düsseldorf was rented from Rheinmetall-Borsig. It was only ten years later, after the company had attracted an investor, when funds became available for construction of major car plant at the Ingolstadt head office site.

In 1958, in response to pressure from Friedrich Flick, then the company's largest single shareholder, Daimler-Benz took an 87% holding in the Auto Union company, and this was increased to a 100% holding in 1959. However, small two-stroke cars were not the focus of Daimler-Benz's interests, and while the early 1960s saw major investment in new Mercedes models and in a state of the art factory for Auto Union's, the company's aging model range at this time did not benefit from the economic boom of the early 1960s to the same extent as competitor manufacturers such as Volkswagen and Opel. The decision to dispose of the Auto Union business was based on its lack of profitability. Ironically, by the time they sold the business, it also included a large new factory and near production-ready modern four-stroke engine, which would enable the Auto Union business, under a new owner, to embark on a period of profitable growth, now producing not Auto Unions or DKWs, but using the "Audi" name, resurrected in 1965 after a 25-year gap.

In 1964, Volkswagen acquired a 50% holding in the business, which included the new factory in Ingolstadt, the DKW and Audi brands along with the rights to the new engine design which had been funded by Daimler-Benz, who in return retained the dormant Horch trademark and the Düsseldorf factory which became a Mercedes-Benz van assembly plant. Eighteen months later, Volkswagen bought complete control of Ingolstadt, and by 1966 were using the spare capacity of the Ingolstadt plant to assemble an additional 60,000 Volkswagen Beetles per year. Two-stroke engines became less popular during the 1960s as customers were more attracted to the smoother four-stroke engines. In September 1965, the DKW F102 was fitted with a four-stroke engine and a facelift for the car's front and rear. Volkswagen dumped the DKW brand because of its associations with two-stroke technology, and having classified the model internally as the F103, sold it simply as the "Audi". Later developments of the model were named after their horsepower ratings and sold as the Audi 60, 75, 80, and Super 90, selling until 1972. Initially, Volkswagen was hostile to the idea of Auto Union as a standalone entity producing its own models having acquired the company merely to boost its own production capacity through the Ingolstadt assembly plant – to the point where Volkswagen executives ordered that the Auto Union name and flags bearing the four rings were removed from the factory buildings. Then VW chief Heinz Nordhoff explicitly forbade Auto Union from any further product development. Fearing that the Volkswagen had no long term ambition for the Audi brand, Auto Union engineers under the leadership of Ludwig Kraus developed the first Audi 100 in secret, without Nordhoff's knowledge. When presented with a finished prototype, Nordhoff was so impressed he authorised the car for production, which when launched in 1968, went on to be a huge success. With this, the resurrection of the Audi brand was now complete, this being followed by the first generation Audi 80 in 1972, which would in turn provide a template for VW's new front-wheel-drive water-cooled range which debuted from the mid-1970s onward.
In 1969, Auto Union merged with NSU, based in Neckarsulm, near Stuttgart. In the 1950s, NSU had been the world's largest manufacturer of motorcycles, but had moved on to produce small cars like the NSU Prinz, the TT and TTS versions of which are still popular as vintage race cars. NSU then focused on new rotary engines based on the ideas of Felix Wankel. In 1967, the new NSU Ro 80 was a car well ahead of its time in technical details such as aerodynamics, light weight, and safety. However, teething problems with the rotary engines put an end to the independence of NSU. The Neckarsulm plant is now used to produce the larger Audi models A6 and A8. The Neckarsulm factory is also home of the "quattro GmbH" (from November 2016 "Audi Sport GmbH"), a subsidiary responsible for development and production of Audi high-performance models: the R8 and the "RS" model range.

The new merged company was incorporated on 1 January 1969 and was known as Audi NSU Auto Union AG, with its headquarters at NSU's Neckarsulm plant, and saw the emergence of Audi as a separate brand for the first time since the pre-war era. Volkswagen introduced the Audi brand to the United States for the 1970 model year. That same year, the mid-sized car that NSU had been working on, the K70, originally intended to slot between the rear-engined Prinz models and the futuristic NSU Ro 80, was instead launched as a Volkswagen.

After the launch of the Audi 100 of 1968, the Audi 80/Fox (which formed the basis for the 1973 Volkswagen Passat) followed in 1972 and the Audi 50 (later rebadged as the Volkswagen Polo) in 1974. The Audi 50 was a seminal design because it was the first incarnation of the Golf/Polo concept, one that led to a hugely successful world car. Ultimately, the Audi 80 and 100 (progenitors of the A4 and A6, respectively) became the company's biggest sellers, whilst little investment was made in the fading NSU range; the Prinz models were dropped in 1973 whilst the fatally flawed NSU Ro80 went out of production in 1977, spelling the effective end of the NSU brand. Production of the Audi 100 had been steadily moved from Ingolstadt to Neckarsulm as the 1970s had progressed, and by the appearance of the second generation C2 version in 1976, all production was now at the former NSU plant. Neckarsulm from that point onward would produce Audi's higher end models.

The Audi image at this time was a conservative one, and so, a proposal from chassis engineer Jörg Bensinger was accepted to develop the four-wheel drive technology in Volkswagen's Iltis military vehicle for an Audi performance car and rally racing car. The performance car, introduced in 1980, was named the "Audi Quattro", a turbocharged coupé which was also the first German large-scale production vehicle to feature permanent all-wheel drive through a centre differential. Commonly referred to as the "Ur-Quattro" (the "Ur-" prefix is a German augmentative used, in this case, to mean "original" and is also applied to the first generation of Audi's S4 and S6 Sport Saloons, as in "UrS4" and "UrS6"), few of these vehicles were produced (all hand-built by a single team), but the model was a great success in rallying. Prominent wins proved the viability of all-wheel drive racecars, and the Audi name became associated with advances in automotive technology.

In 1985, with the Auto Union and NSU brands effectively dead, the company's official name was now shortened to simply Audi AG. At the same time the company's headquarters moved back to Ingolstadt and two new wholly owned subsidiaries; "Auto Union GmbH" and "NSU GmbH", were formed to own and manage the historical trademarks and intellectual property of the original constituent companies (the exception being Horch, which had been retained by Daimler-Benz after the VW takeover), and to operate Audi's heritage operations.
In 1986, as the Passat-based Audi 80 was beginning to develop a kind of "grandfather's car" image, the "type 89" was introduced. This completely new development sold extremely well. However, its modern and dynamic exterior belied the low performance of its base engine, and its base package was quite spartan (even the passenger-side mirror was an option.) In 1987, Audi put forward a new and very elegant Audi 90, which had a much superior set of standard features. In the early 1990s, sales began to slump for the Audi 80 series, and some basic construction problems started to surface.

In the early part of the 21st century, Audi set forth on a German racetrack to claim and maintain several world records, such as top speed endurance. This effort was in-line with the company's heritage from the 1930s racing era Silver Arrows.

Through the early 1990s, Audi began to shift its target market upscale to compete against German automakers Mercedes-Benz and BMW. This began with the release of the Audi V8 in 1990. It was essentially a new engine fitted to the Audi 100/200, but with noticeable bodywork differences. Most obvious was the new grille that was now incorporated in the bonnet.

By 1991, Audi had the four-cylinder Audi 80, the 5-cylinder Audi 90 and Audi 100, the turbocharged Audi 200 and the Audi V8. There was also a coupé version of the 80/90 with both four- and five-cylinder engines.

Although the five-cylinder engine was a successful and robust powerplant, it was still a little too different for the target market. With the introduction of an all-new Audi 100 in 1992, Audi introduced a 2.8L V6 engine. This engine was also fitted to a face-lifted Audi 80 (all 80 and 90 models were now badged 80 except for the USA), giving this model a choice of four-, five-, and six-cylinder engines, in saloon, coupé and convertible body styles.

The five-cylinder was soon dropped as a major engine choice; however, a turbocharged version remained. The engine, initially fitted to the 200 quattro 20V of 1991, was a derivative of the engine fitted to the Sport Quattro. It was fitted to the Audi Coupé, named the S2, and also to the Audi 100 body, and named the S4. These two models were the beginning of the mass-produced S series of performance cars.

Sales in the United States fell after a series of recalls from 1982 to 1987 of Audi 5000 models associated with reported incidents of sudden unintended acceleration linked to six deaths and 700 accidents. At the time, NHTSA was investigating 50 car models from 20 manufacturers for sudden surges of power.

A "60 Minutes" report aired 23 November 1986, featuring interviews with six people who had sued Audi after reporting unintended acceleration, showing an Audi 5000 ostensibly suffering a problem when the brake pedal was pushed. Subsequent investigation revealed that "60 Minutes" had engineered the failure – fitting a canister of compressed air on the passenger-side floor, linked via a hose to a hole drilled into the transmission.

Audi contended, prior to findings by outside investigators, that the problems were caused by driver error, specifically pedal misapplication. Subsequently, the National Highway Traffic Safety Administration (NHTSA) concluded that the majority of unintended acceleration cases, including all the ones that prompted the "60 Minutes" report, were caused by driver error such as confusion of pedals. CBS did not acknowledge the test results of involved government agencies, but did acknowledge the similar results of another study.

In a review study published in 2012, NHTSA summarized its past findings about the Audi unintended acceleration problems: "Once an unintended acceleration had begun, in the Audi 5000, due to a failure in the idle-stabilizer system (producing an initial acceleration of 0.3g), pedal misapplication resulting from panic, confusion, or unfamiliarity with the Audi 5000 contributed to the severity of the incident."

This summary is consistent with the conclusions of NHTSA's most technical analysis at the time: "Audi idle-stabilization systems were prone to defects which resulted in excessive idle speeds and brief unanticipated accelerations of up to 0.3g [which is similar in magnitude to an emergency stop in a subway car]. These accelerations could not be the sole cause of [(long-duration) sudden acceleration incidents (SAI)], but might have triggered some SAIs by startling the driver. The defective idle-stabilization system performed a type of electronic throttle control. Significantly: multiple "intermittent malfunctions of the electronic control unit were observed and recorded ... and [were also observed and] reported by Transport Canada."

With a series of recall campaigns, Audi made several modifications; the first adjusted the distance between the brake and accelerator pedal on automatic-transmission models. Later repairs, of 250,000 cars dating back to 1978, added a device requiring the driver to press the brake pedal before shifting out of park. A legacy of the Audi 5000 and other reported cases of sudden unintended acceleration are intricate gear stick patterns and brake interlock mechanisms to prevent inadvertent shifting into forward or reverse. It is unclear how the defects in the idle-stabilization system were addressed.

Audi's U.S. sales, which had reached 74,061 in 1985, dropped to 12,283 in 1991 and remained level for three years. – with resale values falling dramatically. Audi subsequently offered increased warranty protection and renamed the affected models – with the "5000" becoming the "100" and "200" in 1989 – and reached the same sales levels again only by model year 2000.

A 2010 "BusinessWeek" article – outlining possible parallels between Audi's experience and 2009–2010 Toyota vehicle recalls – noted a class-action lawsuit filed in 1987 by about 7,500 Audi 5000-model owners remains unsettled and remains contested in Chicago's Cook County after appeals at the Illinois state and U.S. federal levels.

In the mid-to-late 1990s, Audi introduced new technologies including the use of aluminium construction. Produced from 1999 to 2005, the Audi A2 was a futuristic super mini, born from the Al2 concept, with many features that helped regain consumer confidence, like the aluminium space frame, which was a first in production car design. In the A2 Audi further expanded their TDI technology through the use of frugal three-cylinder engines. The A2 was extremely aerodynamic and was designed around a wind tunnel. The Audi A2 was criticised for its high price and was never really a sales success but it planted Audi as a cutting-edge manufacturer. The model, a Mercedes-Benz A-Class competitor, sold relatively well in Europe. However, the A2 was discontinued in 2005 and Audi decided not to develop an immediate replacement.

The next major model change came in 1995 when the Audi A4 replaced the Audi 80. The new nomenclature scheme was applied to the Audi 100 to become the Audi A6 (with a minor facelift). This also meant the S4 became the S6 and a new S4 was introduced in the A4 body. The S2 was discontinued. The Audi Cabriolet continued on (based on the Audi 80 platform) until 1999, gaining the engine upgrades along the way. A new A3 hatchback model (sharing the Volkswagen Golf Mk4's platform) was introduced to the range in 1996, and the radical Audi TT coupé and roadster were debuted in 1998 based on the same underpinnings.

The engines available throughout the range were now a 1.4 L, 1.6 L and 1.8 L four-cylinder, 1.8 L four-cylinder turbo, 2.6 L and 2.8 L V6, 2.2 L turbo-charged five-cylinder and the 4.2 L V8 engine. The V6s were replaced by new 2.4 L and 2.8 L 30V V6s in 1998, with marked improvement in power, torque and smoothness. Further engines were added along the way, including a 3.7 L V8 and 6.0 L W12 engine for the A8.

Audi's sales grew strongly in the 2000s, with deliveries to customers increasing from 653,000 in 2000 to 1,003,000 in 2008. The largest sales increases came from Eastern Europe (+19.3%), Africa (+17.2%) and the Middle East (+58.5%). China in particular has become a key market, representing 108,000 out of 705,000 cars delivered in the first three quarters of 2009. One factor for its popularity in China is that Audis have become the car of choice for purchase by the Chinese government for officials, and purchases by the government are responsible for 20% of its sales in China. As of late 2009, Audi's operating profit of €1.17-billion ($1.85-billion) made it the biggest contributor to parent Volkswagen Group's nine-month operating profit of €1.5-billion, while the other marques in Group such as Bentley and SEAT had suffered considerable losses. May 2011 saw record sales for Audi of America with the new Audi A7 and Audi A3 TDI Clean Diesel. In May 2012, Audi reported a 10% increase in its sales—from 408 units to 480 in the last year alone.

Audi manufactures vehicles in seven plants around the world, some of which are shared with other VW Group marques although many sub-assemblies such as engines and transmissions are manufactured within other Volkswagen Group plants.

Audi's two principal assembly plants are:


Outside of Germany, Audi produces vehicles at:


In September 2012, Audi announced the construction of its first North American manufacturing plant in Puebla, Mexico. This plant became operative in 2016 and produces the second generation Q5.

From 2002 up to 2003, Audi headed the Audi Brand Group, a subdivision of the Volkswagen Group's Automotive Division consisting of Audi, Lamborghini and SEAT, that was focused on sporty values, with the marques' product vehicles and performance being under the higher responsibility of the Audi brand.

On January 2014, Audi, along with the Wireless Power Consortium, operated a booth which demonstrated a phone compartment using the Qi open interface standard at the Consumer Electronics Show (CES). In May, most of the Audi dealers in UK falsely claimed that the Audi A7, A8, and R8 were Euro NCAP safety tested, all achieving five out of five stars. In fact none were tested.

In 2015, Audi admitted that at least 2.1 million Audi cars had been involved in the Volkswagen emissions testing scandal in which software installed in the cars manipulated emissions data to fool regulators and allow the cars to pollute at higher than government-mandated levels. The A1, A3, A4, A5, A6, TT, Q3 and Q5 models were implicated in the scandal. Audi promised to quickly find a technical solution and upgrade the cars so they can function within emissions regulations. Ulrich Hackenberg, the head of research and development at Audi, was suspended in relation to the scandal. Despite widespread media coverage about the scandal through the month of September, Audi reported that U.S. sales for the month had increased by 16.2%. Audi's parent company Volkswagen announced on 18 June 2018 that Audi chief executive Rupert Stadler had been arrested.

In November 2015, the U.S. Environmental Protection Agency implicated the 3-liter diesel engine versions of the 2016 Audi A6 Quattro, A7 Quattro, A8, A8L and the Q5 as further models that had emissions regulation defeat-device software installed. Thus, these models emitted nitrogen oxide at up to nine times the legal limit when the car detected that it was not hooked up to emissions testing equipment.

In November 2016, Audi expressed an intention to establish an assembly factory in Pakistan, with the company's local partner acquiring land for a plant in Korangi Creek Industrial Park in Karachi. Approval of the plan would lead to an investment of $30 million in the new plant. Audi planned to cut 9,500 jobs in Germany starting from 2020 till 2025 to fund electric vehicles and digital working.

Audi AI is a driver assist feature offered by Audi. The company's stated intent is to offer fully autonomous driving at a future time, acknowledging that legal, regulatory and technical hurdles must be overcome to achieve this goal. On June 4, 2017, Audi stated that its new A8 will be fully self-driving for speeds up to 60 km/h using its Audi AI. Contrary to other cars, the driver will not have to do safety checks such as touching the steering wheel every 15 seconds to use this feature. The Audi A8 will therefore be the first production car to reach level 3 autonomous driving, meaning that the driver can safely turn their attention away from driving tasks, e.g. the driver can text or watch a movie. Audi will also be the first manufacturer to use a 3D LIDAR system in addition to cameras and ultrasonic sensors for their AI.

Audi produces 100% galvanised cars to prevent corrosion, and was the first mass-market vehicle to do so, following introduction of the process by Porsche, c. 1975. Along with other precautionary measures, the full-body zinc coating has proved to be very effective in preventing rust. The body's resulting durability even surpassed Audi's own expectations, causing the manufacturer to extend its original 10-year warranty against corrosion perforation to currently 12 years (except for aluminium bodies which do not rust).

Audi introduced a new series of vehicles in the mid-1990s and continues to pursue new technology and high performance. An all-aluminium car was brought forward by Audi, and in 1994 the Audi A8 was launched, which introduced aluminium space frame technology (called "Audi Space Frame" or ASF) which saves weight and improves torsion rigidity compared to a conventional steel frame. Prior to that effort, Audi used examples of the Type 44 chassis fabricated out of aluminium as test-beds for the technique. The disadvantage of the aluminium frame is that it is very expensive to repair and requires a specialized aluminium bodyshop. The weight reduction is somewhat offset by the quattro four-wheel drive system which is standard in most markets. Nonetheless, the A8 is usually the lightest all-wheel drive car in the full-size luxury segment, also having best-in-class fuel economy. The Audi A2, Audi TT and Audi R8 also use Audi Space Frame designs.

For most of its lineup (excluding the A3, A1, and TT models), Audi has not adopted the transverse engine layout which is typically found in economy cars (such as Peugeot and Citroën), since that would limit the type and power of engines that can be installed. To be able to mount powerful engines (such as a V8 engine in the Audi S4 and Audi RS4, as well as the W12 engine in the Audi A8L W12), Audi has usually engineered its more expensive cars with a longitudinally front-mounted engine, in an "overhung" position, over the front wheels in front of the axle line - this layout dates back to the DKW and Auto Union saloons from the 1950s. But while this allows for the easy adoption of all-wheel drive, it goes against the ideal 50:50 weight distribution.

In all its post Volkswagen-era models, Audi has firmly refused to adopt the traditional rear-wheel drive layout favored by its two archrivals Mercedes-Benz and BMW, favoring either front-wheel drive or all-wheel drive. The majority of Audi's lineup in the United States features all-wheel drive standard on most of its expensive vehicles (only the entry-level trims of the A4 and A6 are available with front-wheel drive), in contrast to Mercedes-Benz and BMW whose lineup treats all-wheel drive as an option. BMW did not offer all-wheel drive on its V8-powered cars (as opposed to crossover SUVs) until the 2010 BMW 7 Series and 2011 BMW 5 Series, while the Audi A8 has had all-wheel drive available/standard since the 1990s. Regarding high-performance variants, Audi S and RS models have always had all-wheel drive, unlike their direct rivals from BMW M and Mercedes-AMG whose cars are rear-wheel drive only (although their performance crossover SUVs are all-wheel drive).

Audi has recently applied the "quattro" badge to models such as the A3 and TT which do not use the Torsen-based system as in prior years with a mechanical center differential, but with the Haldex Traction electro-mechanical clutch AWD system.

Prior to the introduction of the Audi 80 and Audi 50 in 1972 and 1974, respectively, Audi had led the development of the "EA111" and "EA827" inline-four engine families. These new power units underpinned the water-cooled revival of parent company Volkswagen (in the Polo, Golf, Passat and Scirocco), whilst the many derivatives and descendants of these two basic engine designs have appeared in every generation of VW Group vehicles right up to the present day.

In the 1980s, Audi, along with Volvo, was the champion of the inline-five cylinder, 2.1/2.2 L engine as a longer-lasting alternative to more traditional six-cylinder engines. This engine was used not only in production cars but also in their race cars. The 2.1 L inline five-cylinder engine was used as a base for the rally cars in the 1980s, providing well over after modification. Before 1990, there were engines produced with a displacement between 2.0 L and 2.3 L. This range of engine capacity allowed for both fuel economy and power.

For the ultra-luxury version of its Audi A8 fullsize luxury flagship sedan, the Audi A8L W12, Audi uses the Volkswagen Group W12 engine instead of the conventional V12 engine favored by rivals Mercedes-Benz and BMW. The W12 engine configuration (also known as a "WR12") is created by forming two imaginary narrow-angle 15° VR6 engines at an angle of 72°, and the narrow angle of each set of cylinders allows just two overhead camshafts to drive each pair of banks, so just four are needed in total. The advantage of the W12 engine is its compact packaging, allowing Audi to build a 12-cylinder sedan with all-wheel drive, whereas a conventional V12 engine could have only a rear-wheel drive configuration as it would have no space in the engine bay for a differential and other components required to power the front wheels. In fact, the 6.0 L W12 in the Audi A8L W12 is smaller in overall dimensions than the 4.2 L V8 that powers the Audi A8 4.2 variants. The 2011 Audi A8 debuted a revised 6.3-litre version of the W12 (WR12) engine with .

New models of the A3, A4, A6 and A8 have been introduced, with the ageing 1.8-litre engine now having been replaced by new Fuel Stratified Injection (FSI) engines. Nearly every petroleum burning model in the range now incorporates this fuel-saving technology.

In 2003 Volkswagen introduced the Direct-Shift Gearbox (DSG), a type of dual clutch transmission. It is an automated semi-automatic transmission, drivable like a conventional automatic transmission. Based on the gearbox found in the Group B S1, the system includes dual electrohydraulically controlled clutches instead of a torque converter. This is implemented in some VW Golfs, Audi A3, Audi A4 and TT models where DSG is called S-tronic.

Beginning in 2005, Audi has implemented white LED technology as daytime running lights (DRL) in their products. The distinctive shape of the DRLs has become a trademark of sorts. LEDs were first introduced on the Audi A8 W12, the world's first production car to have LED DRLs, and have since spread throughout the entire model range. The LEDs are present on some Audi billboards.

Since 2010, Audi has also offered the LED technology in low- and high-beam headlights.

Starting with the 2003 Audi A8, Audi has used a centralised control interface for its on-board infotainment systems, called Multi Media Interface (MMI). It is essentially a rotating control knob and 'segment' buttons – designed to control all in-car entertainment devices (radio, CD changer, iPod, TV tuner), satellite navigation, heating and ventilation, and other car controls with a screen.

The availability of MMI has gradually filtered down the Audi lineup, and following its introduction on the third generation A3 in 2011, MMI is now available across the entire range. It has been generally well received, as it requires less menu-surfing with its segment buttons around a central knob, along with 'main function' direct access buttons – with shortcuts to the radio or phone functions. The colour screen is mounted on the upright dashboard, and on the A4 (new), A5, A6, A8, and Q7, the controls are mounted horizontally.

Audi has assisted with technology to produce synthetic diesel from water and carbon dioxide. Audi calls the synthetic diesel E-diesel. It is also working on synthetic gasoline (which it calls E-gasoline).

Audi uses scanning gloves for parts registration during assembly, and automatic robots to transfer cars from factory to rail cars.

The following tables list Audi production vehicles that are sold as of 2018:

Audi is planning an alliance with the Japanese electronics giant Sanyo to develop a pilot hybrid electric project for the Volkswagen Group. The alliance could result in Sanyo batteries and other electronic components being used in future models of the Volkswagen Group. Concept electric vehicles unveiled to date include the Audi A1 Sportback Concept, Audi A4 TDI Concept E, and the fully electric Audi e-tron Concept Supercar.

In December 2018, Audi announced to invest 14 billion Euro ($15.9 billion) in e-mobility, self-driving cars.


Audi has competed in various forms of motorsports. Audi's tradition in motorsport began with their former company Auto Union in the 1930s. In the 1990s, Audi found success in the Touring and Super Touring categories of motor racing after success in circuit racing in North America.

In 1980, Audi released the Quattro, a four-wheel drive (4WD) turbocharged car that went on to win rallies and races worldwide. It is considered one of the most significant rally cars of all time, because it was one of the first to take advantage of the then-recently changed rules which allowed the use of four-wheel drive in competition racing. Many critics doubted the viability of four-wheel drive racers, thinking them to be too heavy and complex, yet the Quattro was to become a successful car. Leading its first rally it went off the road, however the rally world had been served notice 4WD was the future. The Quattro went on to achieve much success in the World Rally Championship. It won the 1983 (Hannu Mikkola) and the 1984 (Stig Blomqvist) drivers' titles, and brought Audi the manufacturers' title in 1982 and 1984.
In 1984, Audi launched the short-wheelbase Sport Quattro which dominated rally races in Monte Carlo and Sweden, with Audi taking all podium places, but succumbed to problems further into WRC contention. In 1985, after another season mired in mediocre finishes, Walter Röhrl finished the season in his Sport Quattro S1, and helped place Audi second in the manufacturers' points. Audi also received rally honours in the Hong Kong to Beijing rally in that same year. Michèle Mouton, the only female driver to win a round of the World Rally Championship and a driver for Audi, took the Sport Quattro S1, now simply called the "S1", and raced in the Pikes Peak International Hill Climb. The climb race pits a driver and car to drive to the summit of the Pikes Peak mountain in Colorado, and in 1985, Michèle Mouton set a new record of 11:25.39, and being the first woman to set a Pikes Peak record. In 1986, Audi formally left international rally racing following an accident in Portugal involving driver Joaquim Santos in his Ford RS200. Santos swerved to avoid hitting spectators in the road, and left the track into the crowd of spectators on the side, killing three and injuring 30. Bobby Unser used an Audi in that same year to claim a new record for the Pikes Peak Hill Climb at 11:09.22.

In 1987, Walter Röhrl claimed the title for Audi setting a new Pikes Peak International Hill Climb record of 10:47.85 in his Audi S1, which he had retired from the WRC two years earlier. The Audi S1 employed Audi's time-tested inline-five-cylinder turbocharged engine, with the final version generating . The engine was mated to a six-speed gearbox and ran on Audi's famous four-wheel drive system. All of Audi's top drivers drove this car; Hannu Mikkola, Stig Blomqvist, Walter Röhrl and Michèle Mouton. This Audi S1 started the range of Audi 'S' cars, which now represents an increased level of sports-performance equipment within the mainstream Audi model range.

As Audi moved away from rallying and into circuit racing, they chose to move first into America with the Trans-Am in 1988.

In 1989, Audi moved to International Motor Sports Association (IMSA) GTO with the Audi 90, however as they avoided the two major endurance events (Daytona and Sebring) despite winning on a regular basis, they would lose out on the title.

In 1990, having completed their objective to market cars in North America, Audi returned to Europe, turning first to the Deutsche Tourenwagen Meisterschaft (DTM) series with the Audi V8, and then in 1993, being unwilling to build cars for the new formula, they turned their attention to the fast-growing Super Touring series, which are a series of national championships. Audi first entered in the French Supertourisme and Italian Superturismo. In the following year, Audi would switch to the German Super Tourenwagen Cup (known as STW), and then to British Touring Car Championship (BTCC) the year after that.

The Fédération Internationale de l'Automobile (FIA), having difficulty regulating the quattro four-wheel drive system, and the impact it had on the competitors, would eventually ban all four-wheel drive cars from competing in 1998, but by then, Audi switched all their works efforts to sports car racing.

By 2000, Audi would still compete in the US with their RS4 for the SCCA Speed World GT Challenge, through dealer/team Champion Racing competing against Corvettes, Vipers, and smaller BMWs (where it is one of the few series to permit 4WD cars). In 2003, Champion Racing entered an RS6. Once again, the quattro four-wheel drive was superior, and Champion Audi won the championship. They returned in 2004 to defend their title, but a newcomer, Cadillac with the new Omega Chassis CTS-V, gave them a run for their money. After four victories in a row, the Audis were sanctioned with several negative changes that deeply affected the car's performance. Namely, added ballast weights, and Champion Audi deciding to go with different tyres, and reducing the boost pressure of the turbocharger.

In 2004, after years of competing with the TT-R in the revitalised DTM series, with privateer team Abt Racing/Christian Abt taking the 2002 title with Laurent Aïello, Audi returned as a full factory effort to touring car racing by entering two factory supported Joest Racing A4 DTM cars.

Audi began racing prototype sportscars in 1999, debuting at the Le Mans 24 hour. Two car concepts were developed and raced in their first season - the Audi R8R (open-cockpit 'roadster' prototype) and the Audi R8C (closed-cockpit 'coupé' GT-prototype). The R8R scored a credible podium on its racing debut at Le Mans and was the concept which Audi continued to develop into the 2000 season due to favourable rules for open-cockpit prototypes.

However, most of the competitors (such as BMW, Toyota, Mercedes and Nissan) retired at the end of 1999.
The factory-supported Joest Racing team won at Le Mans three times in a row with the Audi R8 (2000–2002), as well as winning every race in the American Le Mans Series in its first year. Audi also sold the car to customer teams such as Champion Racing.

In 2003, two Bentley Speed 8s, with engines designed by Audi, and driven by Joest drivers "loaned" to the fellow Volkswagen Group company, competed in the GTP class, and finished the race in the top two positions, while the Champion Racing R8 finished third overall, and first in the LMP900 class. Audi returned to the winner's podium at the 2004 race, with the top three finishers all driving R8s: Audi Sport Japan Team Goh finished first, Audi Sport UK Veloqx second, and Champion Racing third.

At the 2005 24 Hours of Le Mans, Champion Racing entered two R8s, along with an R8 from the Audi PlayStation Team Oreca. The R8s (which were built to old LMP900 regulations) received a narrower air inlet restrictor, reducing power, and an additional of weight compared to the newer LMP1 chassis. On average, the R8s were about 2–3 seconds off pace compared to the Pescarolo–Judd. But with a team of excellent drivers and experience, both Champion R8s were able to take first and third, while the Oreca team took fourth. The Champion team was also the first American team to win Le Mans since the Gulf Ford GTs in 1967. This also ends the long era of the R8; however, its replacement for 2006, called the Audi R10 TDI, was unveiled on 13 December 2005.

The R10 TDI employed many new and innovative features, the most notable being the twin-turbocharged direct injection diesel engine. It was first raced in the 2006 12 Hours of Sebring as a race-test in preparation for the 2006 24 Hours of Le Mans, which it later went on to win. Audi had a win in the first diesel sports car at 12 Hours of Sebring (the car was developed with a Diesel engine due to ACO regulations that favor diesel engines). As well as winning the 24 Hours of Le Mans in 2006, the R10 TDI beat the Peugeot 908 HDi FAP in , and in , (however Peugeot won the 24h in 2009) with a podium clean-sweep (all four 908 entries retired) while breaking a distance record (set by the Porsche 917K of Martini Racing in ), in with the R15 TDI Plus.

Audi's sports car racing success would continue with the Audi R18's victory at the 2011 24 Hours of Le Mans. Audi Sport Team Joest's Benoît Tréluyer earned Audi their first pole position in five years while the team's sister car locked out the front row. Early accidents eliminated two of Audi's three entries, but the sole remaining Audi R18 TDI of Tréluyer, Marcel Fässler, and André Lotterer held off the trio of Peugeot 908s to claim victory by a margin of 13.8 seconds.

Audi entered a factory racing team run by Joest Racing into the American Le Mans Series under the Audi Sport North America name in 2000. This was a successful operation with the team winning on its debut in the series at the 2000 12 Hours of Sebring. Factory backed Audi R8s were the dominant car in ALMS taking 25 victories between 2000 and the end of the 2002 season. In 2003 Audi sold customer cars to Champion Racing as well as continuing to race the factory Audi Sport North America team. Champion Racing won many races as a private team running Audi R8s and eventually replaced Team Joest as the Audi Sport North America between 2006 and 2008. Since 2009 Audi has not taken part in full American Le Mans Series Championships, but has competed in the series opening races at Sebring, using the 12-hour race as a test for Le Mans, and also as part of the 2012 FIA World Endurance Championship season calendar.

Audi participated in the 2003 1000km of Le Mans which was a one-off sports car race in preparation for the 2004 European Le Mans Series. The factory team Audi Sport UK won races and the championship in the 2004 season but Audi was unable to match their sweeping success of Audi Sport North America in the American Le Mans Series, partly due to the arrival of a factory competitor in LMP1, Peugeot. The French manufacturer's 908 HDi FAP became the car to beat in the series from 2008 onwards with 20 LMP wins. However, Audi were able to secure the championship in 2008 even though Peugeot scored more race victories in the season.

In 2012, the FIA sanctioned a World Endurance Championship which would be organised by the ACO as a continuation of the ILMC. Audi competed won the first WEC race at Sebring and followed this up with a further three successive wins, including the 2012 24 Hours of Le Mans. Audi scored a final 5th victory in the 2012 WEC in Bahrain and were able to win the inaugural WEC Manufacturers' Championship.

As defending champions, Audi once again entered the Audi R18 e-tron quattro chassis into the 2013 WEC and the team won the first five consecutive races, including the 2013 24 Hours of Le Mans. The victory at Round 5, Circuit of the Americas, was of particular significance as it marked the 100th win for Audi in Le Mans prototypes. Audi secured their second consecutive WEC Manufacturers' Championship at Round 6 after taking second place and half points in the red-flagged Fuji race.

For the 2014 season Audi entered a redesigned and upgraded R18 e-tron quattro which featured a 2 MJ energy recovery system. As defending champions, Audi would once again face a challenge in LMP1 from Toyota, and additionally from Porsche who returned to endurance racing after a 16-year absence. The season opening 6hrs of Silverstone was a disaster for Audi who saw both cars retire from the race, marking the first time that an Audi car has failed to score a podium in a World Endurance Championship race.

Audi provide factory support to Abt Sportsline in the FIA Formula E Championship, The team competed under the title of Audi Sport Abt Formula E Team in the inaugural 2014-15 Formula E season. On 13 February 2014 the team announced its driver line up as Daniel Abt and World Endurance Championship driver Lucas di Grassi.

Audi has been linked to Formula One in recent years but has always resisted due to the company's opinion that it is not relevant to road cars, but hybrid power unit technology has been adopted into the sport, swaying the company's view and encouraging research into the program by former Ferrari team principal Stefano Domenicali.

The Audi emblem is four overlapping rings that represent the four marques of Auto Union. The Audi emblem symbolises the amalgamation of Audi with DKW, Horch and Wanderer: the first ring from the left represents Audi, the second represents DKW, third is Horch, and the fourth and last ring Wanderer.
The design is popularly believed to have been the idea of Klaus von Oertzen, the director of sales at Wanderer - when Berlin was chosen as the host city for the 1936 Summer Olympics and that a form of the Olympic logo symbolized the newly established Auto Union's desire to succeed. Somewhat ironically, the International Olympic Committee later sued Audi in the International Trademark Court in 1995, where they lost.

The original "Audi" script, with the distinctive slanted tails on the "A" and "d" was created for the historic Audi company in 1920 by the famous graphic designer Lucian Bernhard, and was resurrected when Volkswagen revived the brand in 1965. Following the demise of NSU in 1977, less prominence was given to the four rings, in preference to the "Audi" script encased within a black (later red) ellipse, and was commonly displayed next to the Volkswagen roundel when the two brands shared a dealer network under the V.A.G banner. The ellipse (known as the Audi Oval) was phased out after 1994, when Audi formed its own independent dealer network, and prominence was given back to the four rings - at the same time Audi Sans (a derivative of Univers) was adopted as the font for all marketing materials, corporate communications and was also used in the vehicles themselves.

As part of Audi's centennial celebration in 2009, the company updated the logo, changing the font to left-aligned Audi Type, and altering the shading for the overlapping rings. The revised logo was designed by Rayan Abdullah.

Audi developed a Corporate Sound concept, with Audi Sound Studio designed for producing the Corporate Sound. The Corporate Sound project began with sound agency Klangerfinder GmbH & Co KG and s12 GmbH. Audio samples were created in Klangerfinder's sound studio in Stuttgart, becoming part of Audi Sound Studio collection. Other Audi Sound Studio components include The Brand Music Pool, The Brand Voice. Audi also developed Sound Branding Toolkit including certain instruments, sound themes, rhythm and car sounds which all are supposed to reflect the AUDI sound character.

Audi started using a beating heart sound trademark beginning in 1996. An updated heartbeat sound logo, developed by agencies KLANGERFINDER GmbH & Co KG of Stuttgart and S12 GmbH of Munich, was first used in 2010 in an Audi A8 commercial with the slogan "The Art of Progress."

Audi's corporate tagline is "Vorsprung durch Technik", meaning ""Progress through Technology"". The German-language tagline is used in many European countries, including the United Kingdom, and in other markets, such as Latin America, Oceania and parts of Asia including Japan. A few years ago, the North American tagline was ""Innovation through technology"", but in Canada the German tagline "Vorsprung durch Technik" was used in advertising. Since 2007, Audi has used the slogan "Truth in Engineering" in the U.S. However, since the Audi emissions testing scandal came to light in September 2015, this slogan was lambasted for being discordant with reality. In fact, just hours after disgraced Volkswagen CEO Martin Winterkorn admitted to cheating on emissions data, an advertisement during the 2015 Primetime Emmy Awards promoted Audi's latest advances in low emissions technology with Kermit the Frog stating, "It's not that easy being green."

It was first used in English-language advertising after Sir John Hegarty of the Bartle Bogle Hegarty advertising agency visited the Audi factory in 1982. In the original British television commercials, the phrase was voiced by Geoffrey Palmer. After its repeated use in advertising campaigns, the phrase found its way into popular culture, including the British comedy "Only Fools and Horses", the U2 song "Zooropa" and the Blur song "Parklife". Similar-sounding phrases have also been used, including as the punchline for a joke in the movie "Lock, Stock, and Two Smoking Barrels" and in the British TV series Peep Show.

Audi Sans (based on Univers Extended) was originally created in 1997 by Ole Schäfer for MetaDesign. MetaDesign was later commissioned for a new corporate typeface called Audi Type, designed by Paul van der Laan and Pieter van Rosmalen of Bold Monday. The font began to appear in Audi's 2009 products and marketing materials.

Audi is a strong partner of different kinds of sports. In football, long partnerships exist between Audi and domestic clubs including Bayern Munich, Hamburger SV, 1. FC Nürnberg, Hertha BSC, and Borussia Mönchengladbach and international clubs including Chelsea, Real Madrid, FC Barcelona, A.C. Milan, AFC Ajax and Perspolis. Audi also sponsors winter sports: The Audi FIS Alpine Ski World Cup is named after the company. Additionally, Audi supports the German Ski Association (DSV) as well as the alpine skiing national teams of Switzerland, Sweden, Finland, France, Liechtenstein, Italy, Austria and the U.S. For almost two decades, Audi fosters golf sport: for example with the Audi quattro Cup and the HypoVereinsbank Ladies German Open presented by Audi. In sailing, Audi is engaged in the Medcup regatta and supports the team Luna Rossa during the Louis Vuitton Pacific Series and also is the primary sponsor of the Melges 20 sailboat. Further, Audi sponsors the regional teams ERC Ingolstadt (hockey) and FC Ingolstadt 04 (soccer). In 2009, the year of Audi's 100th anniversary, the company organized the Audi Cup for the first time. Audi also sponsor the New York Yankees as well. In October 2010 they agreed to a three sponsorship year-deal with Everton. Audi also sponsors the England Polo Team and holds the Audi Polo Awards.

Since the start of the Marvel Cinematic Universe, Audi signed a deal to sponsor, promote and provide vehicles for eight films. These films include, Iron Man, Iron Man 2, Iron Man 3, , , , and . The R8 supercar became the personal vehicle for Tony Stark (played by Robert Downey Jr.) for six of these films. The e-Tron vehicles were promoted in Endgame and Far From Home. Several commercials were co-produced by Marvel and Audi to promote several new concepts and some of the latest vehicles such as the A8, SQ7 and the e-Tron fleet.

In 2001, Audi promoted the new multitronic continuously variable transmission with television commercials throughout Europe, featuring an impersonator of musician and actor Elvis Presley. A prototypical dashboard figure – later named "Wackel-Elvis" ("Wobble Elvis" or "Wobbly Elvis") – appeared in the commercials to demonstrate the smooth ride in an Audi equipped with the multitronic transmission. The dashboard figure was originally intended for use in the commercials only, but after they aired the demand for Wackel-Elvis fans grew among fans and the figure was mass-produced in China and marketed by Audi in their factory outlet store.

As part of Audi's attempt to promote its Diesel technology in 2009, the company began Audi Mileage Marathon. The driving tour featured a fleet of 23 Audi TDI vehicles from 4 models (Audi Q7 3.0 TDI, Audi Q5 3.0 TDI, Audi A4 3.0 TDI, Audi A3 Sportback 2.0 TDI with S tronic transmission) travelling across the American continent from New York to Los Angeles, passing major cities like Chicago, Dallas and Las Vegas during the 13 daily stages, as well as natural wonders including the Rocky Mountains, Death Valley and the Grand Canyon.

The next phase of technology Audi is developing is the e-tron electric drive powertrain system. They have shown several concept cars , each with different levels of size and performance. The original e-tron concept shown at the 2009 Frankfurt motor show is based on the platform of the R8 and has been scheduled for limited production. Power is provided by electric motors at all four wheels. The second concept was shown at the 2010 Detroit Motor Show. Power is provided by two electric motors at the rear axle. This concept is also considered to be the direction for a future mid-engined gas-powered 2-seat performance coupe. The Audi A1 e-tron concept, based on the Audi A1 production model, is a hybrid vehicle with a range extending Wankel rotary engine to provide power after the initial charge of the battery is depleted. It is the only concept of the three to have range extending capability. The car is powered through the front wheels, always using electric power.

It is all set to be displayed at the Auto Expo 2012 in New Delhi, India, from 5 January. Powered by a 1.4 litre engine, and can cover a distance up to 54 km s on a single charge. The e-tron was also shown in the 2013 blockbuster film Iron Man 3 and was driven by Tony Stark (Iron Man).

Audi has supported the European version of PlayStation Home, the PlayStation 3's online community-based service, by releasing a dedicated Home space. Audi is the first carmaker to develop a such a space for Home. On 17 December 2009, Audi released two spaces; the Audi Home Terminal and the Audi Vertical Run. The Audi Home Terminal features an Audi TV channel delivering video content, an Internet Browser feature, and a view of a city. The Audi Vertical Run is where users can access the mini-game Vertical Run, a futuristic mini-game featuring Audi's e-tron concept. Players collect energy and race for the highest possible speeds and the fastest players earn a place in the Audi apartments located in a large tower in the centre of the Audi Space. In both the Home Terminal and Vertical Run spaces, there are teleports where users can teleport back and forth between the two spaces. Audi had stated that additional content would be added in 2010. On 31 March 2015 Sony shutdown the PlayStation Home service rendering all content for it inaccessible.



</doc>
<doc id="849" url="https://en.wikipedia.org/wiki?curid=849" title="Aircraft">
Aircraft

An aircraft is a vehicle that is able to fly by gaining support from the air. It counters the force of gravity by using either static lift or by using the dynamic lift of an airfoil, or in a few cases the downward thrust from jet engines. Common examples of aircraft include airplanes, helicopters, airships (including blimps), gliders, paramotors and hot air balloons.

The human activity that surrounds aircraft is called "aviation". The science of aviation, including designing and building aircraft, is called "aeronautics." Crewed aircraft are flown by an onboard pilot, but unmanned aerial vehicles may be remotely controlled or self-controlled by onboard computers. Aircraft may be classified by different criteria, such as lift type, aircraft propulsion, usage and others.

Flying model craft and stories of manned flight go back many centuries; however, the first manned ascent — and safe descent — in modern times took place by larger hot-air balloons developed in the 18th century. Each of the two World Wars led to great technical advances. Consequently, the history of aircraft can be divided into five eras:

Aerostats use buoyancy to float in the air in much the same way that ships float on the water. They are characterized by one or more large cells or canopies, filled with a relatively low-density gas such as helium, hydrogen, or hot air, which is less dense than the surrounding air. When the weight of this is added to the weight of the aircraft structure, it adds up to the same weight as the air that the craft displaces.

Small hot-air balloons, called sky lanterns, were first invented in ancient China prior to the 3rd century BC and used primarily in cultural celebrations, and were only the second type of aircraft to fly, the first being kites, which were first invented in ancient China over two thousand years ago. (See Han Dynasty)
A balloon was originally any aerostat, while the term airship was used for large, powered aircraft designs — usually fixed-wing. In 1919 Frederick Handley Page was reported as referring to "ships of the air," with smaller passenger types as "Air yachts." In the 1930s, large intercontinental flying boats were also sometimes referred to as "ships of the air" or "flying-ships". — though none had yet been built. The advent of powered balloons, called dirigible balloons, and later of rigid hulls allowing a great increase in size, began to change the way these words were used. Huge powered aerostats, characterized by a rigid outer framework and separate aerodynamic skin surrounding the gas bags, were produced, the Zeppelins being the largest and most famous. There were still no fixed-wing aircraft or non-rigid balloons large enough to be called airships, so "airship" came to be synonymous with these aircraft. Then several accidents, such as the Hindenburg disaster in 1937, led to the demise of these airships. Nowadays a "balloon" is an unpowered aerostat and an "airship" is a powered one.

A powered, steerable aerostat is called a "dirigible". Sometimes this term is applied only to non-rigid balloons, and sometimes "dirigible balloon" is regarded as the definition of an airship (which may then be rigid or non-rigid). Non-rigid dirigibles are characterized by a moderately aerodynamic gasbag with stabilizing fins at the back. These soon became known as "blimps". During World War II, this shape was widely adopted for tethered balloons; in windy weather, this both reduces the strain on the tether and stabilizes the balloon. The nickname "blimp" was adopted along with the shape. In modern times, any small dirigible or airship is called a blimp, though a blimp may be unpowered as well as powered.

Heavier-than-air aircraft, such as airplanes, must find some way to push air or gas downwards, so that a reaction occurs (by Newton's laws of motion) to push the aircraft upwards. This dynamic movement through the air is the origin of the term "aerodyne". There are two ways to produce dynamic upthrust — aerodynamic lift, and powered lift in the form of engine thrust.

Aerodynamic lift involving wings is the most common, with fixed-wing aircraft being kept in the air by the forward movement of wings, and rotorcraft by spinning wing-shaped rotors sometimes called rotary wings. A wing is a flat, horizontal surface, usually shaped in cross-section as an aerofoil. To fly, air must flow over the wing and generate lift. A "flexible wing" is a wing made of fabric or thin sheet material, often stretched over a rigid frame. A "kite" is tethered to the ground and relies on the speed of the wind over its wings, which may be flexible or rigid, fixed, or rotary.

With powered lift, the aircraft directs its engine thrust vertically downward. V/STOL aircraft, such as the Harrier Jump Jet and Lockheed Martin F-35B take off and land vertically using powered lift and transfer to aerodynamic lift in steady flight.

A pure rocket is not usually regarded as an aerodyne, because it does not depend on the air for its lift (and can even fly into space); however, many aerodynamic lift vehicles have been powered or assisted by rocket motors. Rocket-powered missiles that obtain aerodynamic lift at very high speed due to airflow over their bodies are a marginal case.

The forerunner of the fixed-wing aircraft is the kite. Whereas a fixed-wing aircraft relies on its forward speed to create airflow over the wings, a kite is tethered to the ground and relies on the wind blowing over its wings to provide lift. Kites were the first kind of aircraft to fly, and were invented in China around 500 BC. Much aerodynamic research was done with kites before test aircraft, wind tunnels, and computer modelling programs became available.

The first heavier-than-air craft capable of controlled free-flight were gliders. A glider designed by George Cayley carried out the first true manned, controlled flight in 1853.

The practical, powered, fixed-wing aircraft (the airplane or aeroplane) was invented by Wilbur and Orville Wright. Besides the method of propulsion, fixed-wing aircraft are in general characterized by their wing configuration. The most important wing characteristics are:

A variable geometry aircraft can change its wing configuration during flight.

A "flying wing" has no fuselage, though it may have small blisters or pods. The opposite of this is a "lifting body", which has no wings, though it may have small stabilizing and control surfaces.

Wing-in-ground-effect vehicles are not considered aircraft. They "fly" efficiently close to the surface of the ground or water, like conventional aircraft during takeoff. An example is the Russian ekranoplan (nicknamed the "Caspian Sea Monster"). Man-powered aircraft also rely on ground effect to remain airborne with a minimal pilot power, but this is only because they are so underpowered—in fact, the airframe is capable of flying higher.
Rotorcraft, or rotary-wing aircraft, use a spinning rotor with aerofoil section blades (a "rotary wing") to provide lift. Types include helicopters, autogyros, and various hybrids such as gyrodynes and compound rotorcraft.

"Helicopters" have a rotor turned by an engine-driven shaft. The rotor pushes air downward to create lift. By tilting the rotor forward, the downward flow is tilted backward, producing thrust for forward flight. Some helicopters have more than one rotor and a few have rotors turned by gas jets at the tips.

"Autogyros" have unpowered rotors, with a separate power plant to provide thrust. The rotor is tilted backward. As the autogyro moves forward, air blows upward across the rotor, making it spin. This spinning increases the speed of airflow over the rotor, to provide lift. Rotor kites are unpowered autogyros, which are towed to give them forward speed or tethered to a static anchor in high-wind for kited flight.

"Cyclogyros" rotate their wings about a horizontal axis.

"Compound rotorcraft" have wings that provide some or all of the lift in forward flight. They are nowadays classified as "powered lift" types and not as rotorcraft. "Tiltrotor" aircraft (such as the Bell Boeing V-22 Osprey), tiltwing, tail-sitter, and coleopter aircraft have their rotors/propellers horizontal for vertical flight and vertical for forward flight.


The smallest aircraft are toys/recreational items, and even smaller, nano-aircraft.

The largest aircraft by dimensions and volume (as of 2016) is the 302-foot-long (about 95 meters) British Airlander 10, a hybrid blimp, with helicopter and fixed-wing features, and reportedly capable of speeds up to 90 mph (about 150 km/h), and an airborne endurance of two weeks with a payload of up to 22,050 pounds (11 tons).

The largest aircraft by weight and largest regular fixed-wing aircraft ever built, , is the Antonov An-225 "Mriya". That Ukrainian-built six-engine Russian transport of the 1980s is 84 meters (276 feet) long, with an 88-meter (289 foot) wingspan. It holds the world payload record, after transporting 428,834 pounds (200 tons) of goods, and has recently flown 100-ton loads commercially. Weighing in at somewhere between 1.1 and 1.4 million pounds (550–700 tons) maximum loaded weight, it is also the heaviest aircraft to be built, to date. It can cruise at 500 mph.

The largest military airplanes are the Ukrainian/Russian Antonov An-124 "Ruslan" (world's second-largest airplane, also used as a civilian transport), and American Lockheed C-5 Galaxy transport, weighing, loaded, over 765,000 pounds (over 380 tons). The 8-engine, piston/propeller Hughes H-4 "Hercules" "Spruce Goose" — an American World War II wooden flying boat transport with a greater wingspan (94 meters / 260 feet) than any current aircraft and a tail height equal to the tallest (Airbus A380-800 at 24.1 meters / 78 feet) — flew only one short hop in the late 1940s and never flew out of ground effect.

The largest civilian airplanes, apart from the above-noted An-225 and An-124, are the Airbus Beluga cargo transport derivative of the Airbus A300 jet airliner, the Boeing Dreamlifter cargo transport derivative of the Boeing 747 jet airliner/transport (the 747-200B was, at its creation in the 1960s, the heaviest aircraft ever built, with a maximum weight of 836,000 pounds (over 400 tons)), and the double-decker Airbus A380 "super-jumbo" jet airliner (the world's largest passenger airliner).
The fastest recorded powered aircraft flight and fastest recorded aircraft flight of an air-breathing powered aircraft was of the NASA X-43A "Pegasus", a scramjet-powered, hypersonic, lifting body experimental research aircraft, at Mach 9.6 (nearly 7,000 mph). The X-43A set that new mark, and broke its own world record of Mach 6.3, nearly 5,000 mph, set in March 2004, on its third and final flight on 16 November 2004.

Prior to the X-43A, the fastest recorded powered airplane flight (and still the record for the fastest manned, powered airplane / fastest manned, non-spacecraft aircraft) was of the North American X-15A-2, rocket-powered airplane at 4,520 mph (7,274 km/h), Mach 6.72, on 3 October 1967. On one flight it reached an altitude of 354,300 feet.

The fastest known, production aircraft (other than rockets and missiles) currently or formerly operational (as of 2016) are:

Gliders are heavier-than-air aircraft that do not employ propulsion once airborne. Take-off may be by launching forward and downward from a high location, or by pulling into the air on a tow-line, either by a ground-based winch or vehicle, or by a powered "tug" aircraft. For a glider to maintain its forward air speed and lift, it must descend in relation to the air (but not necessarily in relation to the ground). Many gliders can "soar", "i.e.", gain height from updrafts such as thermal currents. The first practical, controllable example was designed and built by the British scientist and pioneer George Cayley, whom many recognise as the first aeronautical engineer. Common examples of gliders are sailplanes, hang gliders and paragliders.

Balloons drift with the wind, though normally the pilot can control the altitude, either by heating the air or by releasing ballast, giving some directional control (since the wind direction changes with altitude). A wing-shaped hybrid balloon can glide directionally when rising or falling; but a spherically shaped balloon does not have such directional control.

Kites are aircraft that are tethered to the ground or other object (fixed or mobile) that maintains tension in the tether or kite line; they rely on virtual or real wind blowing over and under them to generate lift and drag. Kytoons are balloon-kite hybrids that are shaped and tethered to obtain kiting deflections, and can be lighter-than-air, neutrally buoyant, or heavier-than-air.

Powered aircraft have one or more onboard sources of mechanical power, typically aircraft engines although rubber and manpower have also been used. Most aircraft engines are either lightweight reciprocating engines or gas turbines. Engine fuel is stored in tanks, usually in the wings but larger aircraft also have additional fuel tanks in the fuselage.

Propeller aircraft use one or more propellers (airscrews) to create thrust in a forward direction. The propeller is usually mounted in front of the power source in "tractor configuration" but can be mounted behind in "pusher configuration". Variations of propeller layout include "contra-rotating propellers" and "ducted fans".

Many kinds of power plant have been used to drive propellers. Early airships used man power or steam engines. The more practical internal combustion piston engine was used for virtually all fixed-wing aircraft until World War II and is still used in many smaller aircraft. Some types use turbine engines to drive a propeller in the form of a turboprop or propfan. Human-powered flight has been achieved, but has not become a practical means of transport. Unmanned aircraft and models have also used power sources such as electric motors and rubber bands.

Jet aircraft use airbreathing jet engines, which take in air, burn fuel with it in a combustion chamber, and accelerate the exhaust rearwards to provide thrust.

Turbojet and turbofan engines use a spinning turbine to drive one or more fans, which provide additional thrust. An afterburner may be used to inject extra fuel into the hot exhaust, especially on military "fast jets". Use of a turbine is not absolutely necessary: other designs include the pulsejet and ramjet. These mechanically simple designs cannot work when stationary, so the aircraft must be launched to flying speed by some other method. Other variants have also been used, including the motorjet and hybrids such as the Pratt & Whitney J58, which can convert between turbojet and ramjet operation.

Compared to propellers, jet engines can provide much higher thrust, higher speeds and, above about , greater efficiency. They are also much more fuel-efficient than rockets. As a consequence nearly all large, high-speed or high-altitude aircraft use jet engines.

Some rotorcraft, such as helicopters, have a powered rotary wing or "rotor", where the rotor disc can be angled slightly forward so that a proportion of its lift is directed forwards. The rotor may, like a propeller, be powered by a variety of methods such as a piston engine or turbine. Experiments have also used jet nozzles at the rotor blade tips.


Aircraft are designed according to many factors such as customer and manufacturer demand, safety protocols and physical and economic constraints. For many types of aircraft the design process is regulated by national airworthiness authorities.

The key parts of an aircraft are generally divided into three categories:

The approach to structural design varies widely between different types of aircraft. Some, such as paragliders, comprise only flexible materials that act in tension and rely on aerodynamic pressure to hold their shape. A balloon similarly relies on internal gas pressure, but may have a rigid basket or gondola slung below it to carry its payload. Early aircraft, including airships, often employed flexible doped aircraft fabric covering to give a reasonably smooth aeroshell stretched over a rigid frame. Later aircraft employed semi-monocoque techniques, where the skin of the aircraft is stiff enough to share much of the flight loads. In a true monocoque design there is no internal structure left.

The key structural parts of an aircraft depend on what type it is.

Lighter-than-air types are characterised by one or more gasbags, typically with a supporting structure of flexible cables or a rigid framework called its hull. Other elements such as engines or a gondola may also be attached to the supporting structure.

Heavier-than-air types are characterised by one or more wings and a central fuselage. The fuselage typically also carries a tail or empennage for stability and control, and an undercarriage for takeoff and landing. Engines may be located on the fuselage or wings. On a fixed-wing aircraft the wings are rigidly attached to the fuselage, while on a rotorcraft the wings are attached to a rotating vertical shaft. Smaller designs sometimes use flexible materials for part or all of the structure, held in place either by a rigid frame or by air pressure. The fixed parts of the structure comprise the airframe.

The avionics comprise the aircraft flight control systems and related equipment, including the cockpit instrumentation, navigation, radar, monitoring, and communications systems.

The flight envelope of an aircraft refers to its approved design capabilities in terms of airspeed, load factor and altitude. The term can also refer to other assessments of aircraft performance such as maneuverability. When an aircraft is abused, for instance by diving it at too-high a speed, it is said to be flown "outside the envelope", something considered foolhardy since it has been taken beyond the design limits which have been established by the manufacturer. Going beyond the envelope may have a known outcome such as flutter or entry to a non-recoverable spin (possible reasons for the boundary).

The range is the distance an aircraft can fly between takeoff and landing, as limited by the time it can remain airborne.

For a powered aircraft the time limit is determined by the fuel load and rate of consumption.

For an unpowered aircraft, the maximum flight time is limited by factors such as weather conditions and pilot endurance. Many aircraft types are restricted to daylight hours, while balloons are limited by their supply of lifting gas. The range can be seen as the average ground speed multiplied by the maximum time in the air.

The Airbus A350 is now the longest range airliner.

Flight dynamics is the science of air vehicle orientation and control in three dimensions. The three critical flight dynamics parameters are the angles of rotation around three axes which pass through the vehicle's center of gravity, known as "pitch", "roll," and "yaw".

Flight dynamics is concerned with the stability and control of an aircraft's rotation about each of these axes.

An aircraft that is unstable tends to diverge from its intended flight path and so is difficult to fly. A very stable aircraft tends to stay on its flight path and is difficult to maneuver. Therefore, it is important for any design to achieve the desired degree of stability. Since the widespread use of digital computers, it is increasingly common for designs to be inherently unstable and rely on computerised control systems to provide artificial stability.

A fixed wing is typically unstable in pitch, roll, and yaw. Pitch and yaw stabilities of conventional fixed wing designs require horizontal and vertical stabilisers, which act similarly to the feathers on an arrow. These stabilizing surfaces allow equilibrium of aerodynamic forces and to stabilise the flight dynamics of pitch and yaw. They are usually mounted on the tail section (empennage), although in the canard layout, the main aft wing replaces the canard foreplane as pitch stabilizer. Tandem wing and tailless aircraft rely on the same general rule to achieve stability, the aft surface being the stabilising one.

A rotary wing is typically unstable in yaw, requiring a vertical stabiliser.

A balloon is typically very stable in pitch and roll due to the way the payload is slung underneath the center of lift.

Flight control surfaces enable the pilot to control an aircraft's flight attitude and are usually part of the wing or mounted on, or integral with, the associated stabilizing surface. Their development was a critical advance in the history of aircraft, which had until that point been uncontrollable in flight.

Aerospace engineers develop control systems for a vehicle's orientation (attitude) about its center of mass. The control systems include actuators, which exert forces in various directions, and generate rotational forces or moments about the aerodynamic center of the aircraft, and thus rotate the aircraft in pitch, roll, or yaw. For example, a pitching moment is a vertical force applied at a distance forward or aft from the aerodynamic center of the aircraft, causing the aircraft to pitch up or down. Control systems are also sometimes used to increase or decrease drag, for example to slow the aircraft to a safe speed for landing.

The two main aerodynamic forces acting on any aircraft are lift supporting it in the air and drag opposing its motion. Control surfaces or other techniques may also be used to affect these forces directly, without inducing any rotation.

Aircraft permit long distance, high speed travel and may be a more fuel efficient mode of transportation in some circumstances. Aircraft have environmental and climate impacts beyond fuel efficiency considerations, however. They are also relatively noisy compared to other forms of travel and high altitude aircraft generate contrails, which experimental evidence suggests may alter weather patterns.

Aircraft are produced in several different types optimized for various uses; military aircraft, which includes not just combat types but many types of supporting aircraft, and civil aircraft, which include all non-military types, experimental and model.

A military aircraft is any aircraft that is operated by a legal or insurrectionary armed service of any type. Military aircraft can be either combat or non-combat:

Most military aircraft are powered heavier-than-air types. Other types, such as gliders and balloons, have also been used as military aircraft; for example, balloons were used for observation during the American Civil War and World War I, and military gliders were used during World War II to land troops.

Civil aircraft divide into "commercial" and "general" types, however there are some overlaps.

Commercial aircraft include types designed for scheduled and charter airline flights, carrying passengers, mail and other cargo. The larger passenger-carrying types are the airliners, the largest of which are wide-body aircraft. Some of the smaller types are also used in general aviation, and some of the larger types are used as VIP aircraft.

General aviation is a catch-all covering other kinds of private (where the pilot is not paid for time or expenses) and commercial use, and involving a wide range of aircraft types such as business jets (bizjets), trainers, homebuilt, gliders, warbirds and hot air balloons to name a few. The vast majority of aircraft today are general aviation types.

An experimental aircraft is one that has not been fully proven in flight, or that carries a Special Airworthiness Certificate, called an Experimental Certificate in United States parlance. This often implies that the aircraft is testing new aerospace technologies, though the term also refers to amateur-built and kit-built aircraft, many of which are based on proven designs.

A model aircraft is a small unmanned type made to fly for fun, for static display, for aerodynamic research or for other purposes. A scale model is a replica of some larger design.





</doc>
<doc id="851" url="https://en.wikipedia.org/wiki?curid=851" title="Alfred Nobel">
Alfred Nobel

Alfred Bernhard Nobel ( , ; 21 October 1833 – 10 December 1896) was a Swedish businessman, chemist, engineer, inventor, and philanthropist. He held 355 different patents, dynamite being the most famous. The synthetic element nobelium was named after him. He owned Bofors, which he redirected from its previous role as primarily an iron and steel producer to a major manufacturer of cannon and other armaments. Having read a premature obituary which condemned him for profiting from the sales of arms, he bequeathed his fortune to institute the Nobel Prize. His name also survives in companies such as Dynamit Nobel and AkzoNobel, which are descendants of mergers with companies that Nobel established.

Born in Stockholm, Alfred Nobel was the third son of Immanuel Nobel (1801–1872), an inventor and engineer, and Karolina Andriette (Ahlsell) Nobel (1805–1889). The couple married in 1827 and had eight children. The family was impoverished, and only Alfred and his three brothers survived past childhood. Through his father, Alfred Nobel was a descendant of the Swedish scientist Olaus Rudbeck (1630–1702), and in his turn the boy was interested in engineering, particularly explosives, learning the basic principles from his father at a young age. Alfred Nobel's interest in technology was inherited from his father, an alumnus of Royal Institute of Technology in Stockholm.

Following various business failures, Nobel's father moved to Saint Petersburg in 1837 and grew successful there as a manufacturer of machine tools and explosives. He invented the veneer lathe (which allowed the production of modern plywood) and started work on the torpedo. In 1842, the family joined him in the city. Now prosperous, his parents were able to send Nobel to private tutors and the boy excelled in his studies, particularly in chemistry and languages, achieving fluency in English, French, German and Russian. For 18 months, from 1841 to 1842, Nobel went to the only school he ever attended as a child, in Stockholm.

As a young man, Nobel studied with chemist Nikolai Zinin; then, in 1850, went to Paris to further the work. There he met Ascanio Sobrero, who had invented nitroglycerin three years before. Sobrero strongly opposed the use of nitroglycerin, as it was unpredictable, exploding when subjected to heat or pressure. But Nobel became interested in finding a way to control and use nitroglycerin as a commercially usable explosive, as it had much more power than gunpowder. At age 18, he went to the United States for one year to study, working for a short period under Swedish-American inventor John Ericsson, who designed the American Civil War ironclad "USS Monitor". Nobel filed his first patent, an English patent for a gas meter, in 1857, while his first Swedish patent, which he received in 1863, was on 'ways to prepare gunpowder'.

The family factory produced armaments for the Crimean War (1853–1856), but had difficulty switching back to regular domestic production when the fighting ended and they filed for bankruptcy. In 1859, Nobel's father left his factory in the care of the second son, Ludvig Nobel (1831–1888), who greatly improved the business. Nobel and his parents returned to Sweden from Russia and Nobel devoted himself to the study of explosives, and especially to the safe manufacture and use of nitroglycerin. Nobel invented a detonator in 1863, and in 1865 designed the blasting cap.

On 3 September 1864, a shed used for preparation of nitroglycerin exploded at the factory in Heleneborg, Stockholm, Sweden, killing five people, including Nobel's younger brother Emil. Dogged and unfazed by more minor accidents, Nobel went on to build further factories, focusing on improving the stability of the explosives he was developing. Nobel invented dynamite in 1867, a substance easier and safer to handle than the more unstable nitroglycerin. Dynamite was patented in the US and the UK and was used extensively in mining and the building of transport networks internationally. In 1875 Nobel invented gelignite, more stable and powerful than dynamite, and in 1887 patented ballistite, a predecessor of cordite.

Nobel was elected a member of the Royal Swedish Academy of Sciences in 1884, the same institution that would later select laureates for two of the Nobel prizes, and he received an honorary doctorate from Uppsala University in 1893.
Nobel's brothers Ludvig and Robert exploited oilfields along the Caspian Sea and became hugely rich in their own right. Nobel invested in these and amassed great wealth through the development of these new oil regions. During his life Nobel was issued 355 patents internationally and by his death his business had established more than 90 armaments factories, despite his apparently pacifist character.

In 1888, the death of his brother Ludvig caused several newspapers to publish obituaries of Alfred in error. One French newspaper published an obituary titled "Le marchand de la mort est mort" "("The merchant of death is dead")". Nobel read the obituary and was appalled at the idea that he would be remembered in this way. His decision to posthumously donate the majority of his wealth to found the Nobel Prize has been credited at least in part to him wanting to leave behind a better legacy.

Nobel found that when nitroglycerin was incorporated in an absorbent inert substance like "kieselguhr" (diatomaceous earth) it became safer and more convenient to handle, and this mixture he patented in 1867 as "dynamite". Nobel demonstrated his explosive for the first time that year, at a quarry in Redhill, Surrey, England. In order to help reestablish his name and improve the image of his business from the earlier controversies associated with the dangerous explosives, Nobel had also considered naming the highly powerful substance "Nobel's Safety Powder", but settled with Dynamite instead, referring to the Greek word for "power" ().

Nobel later combined nitroglycerin with various nitrocellulose compounds, similar to collodion, but settled on a more efficient recipe combining another nitrate explosive, and obtained a transparent, jelly-like substance, which was a more powerful explosive than dynamite. 'Gelignite', or blasting gelatine, as it was named, was patented in 1876; and was followed by a host of similar combinations, modified by the addition of potassium nitrate and various other substances. Gelignite was more stable, transportable and conveniently formed to fit into bored holes, like those used in drilling and mining, than the previously used compounds and was adopted as the standard technology for mining in the "Age of Engineering" bringing Nobel a great amount of financial success, though at a significant cost to his health. An offshoot of this research resulted in Nobel's invention of ballistite, the precursor of many modern smokeless powder explosives and still used as a rocket propellant.

In 1888, Alfred's brother, Ludvig, died while visiting Cannes, and a French newspaper mistakenly published Alfred's obituary. It condemned him for his invention of military explosives (not, as is commonly quoted, dynamite, which was mainly used for civilian applications) and is said to have brought about his decision to leave a better legacy after his death. The obituary stated, "" ("The merchant of death is dead") and went on to say, "Dr. Alfred Nobel, who became rich by finding ways to kill more people faster than ever before, died yesterday." Alfred (who never had a wife or children) was disappointed with what he read and concerned with how he would be remembered.

On 27 November 1895, at the Swedish-Norwegian Club in Paris, Nobel signed his last will and testament and set aside the bulk of his estate to establish the Nobel Prizes, to be awarded annually without distinction of nationality. After taxes and bequests to individuals, Nobel's will allocated 94% of his total assets, 31,225,000 Swedish kronor, to establish the five Nobel Prizes. This converted to £1,687,837 (GBP) at the time. In 2012, the capital was worth around SEK 3.1 billion (US$472 million, EUR 337 million), which is almost twice the amount of the initial capital, taking inflation into account.

The first three of these prizes are awarded for eminence in physical science, in chemistry and in medical science or physiology; the fourth is for literary work "in an ideal direction" and the fifth prize is to be given to the person or society that renders the greatest service to the cause of international fraternity, in the suppression or reduction of standing armies, or in the establishment or furtherance of peace congresses.

The formulation for the literary prize being given for a work "in an ideal direction" (' in Swedish), is cryptic and has caused much confusion. For many years, the Swedish Academy interpreted "ideal" as "idealistic" (') and used it as a reason not to give the prize to important but less romantic authors, such as Henrik Ibsen and Leo Tolstoy. This interpretation has since been revised, and the prize has been awarded to, for example, Dario Fo and José Saramago, who do not belong to the camp of literary idealism.

There was room for interpretation by the bodies he had named for deciding on the physical sciences and chemistry prizes, given that he had not consulted them before making the will. In his one-page testament, he stipulated that the money go to discoveries or inventions in the physical sciences and to discoveries or improvements in chemistry. He had opened the door to technological awards, but had not left instructions on how to deal with the distinction between science and technology. Since the deciding bodies he had chosen were more concerned with the former, the prizes went to scientists more often than engineers, technicians or other inventors.

Sweden's central bank Sveriges Riksbank celebrated its 300th anniversary in 1968 by donating a large sum of money to the Nobel Foundation to be used to set up a sixth prize in the field of economics in honour of Alfred Nobel. In 2001, Alfred Nobel's great-great-nephew, Peter Nobel (b. 1931), asked the Bank of Sweden to differentiate its award to economists given "in Alfred Nobel's memory" from the five other awards. This request added to the controversy over whether the Bank of Sweden Prize in Economic Sciences in Memory of Alfred Nobel is actually a legitimate "Nobel Prize".

Nobel was accused of high treason against France for selling Ballistite to Italy, so he moved from Paris to Sanremo, Italy in 1891. On 10 December 1896, he suffered a stroke and died. He had left most of his wealth in trust, unbeknown to his family, in order to fund the Nobel Prize awards. He is buried in Norra begravningsplatsen in Stockholm.

Nobel was Lutheran and regularly attended the Church of Sweden Abroad during his Paris years, led by pastor Nathan Söderblom who received the Nobel Peace Prize in 1930. He became an agnostic in youth and was an atheist later in life.

Nobel travelled for much of his business life, maintaining companies in Europe and America while keeping a home in Paris from 1873 to 1891. He remained a solitary character, given to periods of depression. He remained unmarried, although his biographers note that he had at least three loves, the first in Russia with a girl named Alexandra who rejected his proposal. In 1876, Austro-Bohemian Countess Bertha Kinsky became his secretary, but she left him after a brief stay to marry her previous lover Baron Arthur Gundaccar von Suttner. Her contact with Nobel was brief, yet she corresponded with him until his death in 1896, and it is believed that she was a major influence in his decision to include a peace prize in his will. She was awarded the 1905 Nobel Peace prize "for her sincere peace activities". Nobel's longest-lasting relationship was with Sofie Hess from Vienna whom he met in 1876. The liaison lasted for 18 years. 

Nobel gained proficiency in Swedish, French, Russian, English, German, and Italian. He also developed sufficient literary skill to write poetry in English. His "Nemesis" is a prose tragedy in four acts about Beatrice Cenci. It was printed while he was dying, but the entire stock was destroyed immediately after his death except for three copies, being regarded as scandalous and blasphemous. It was published in Sweden in 2003 and has been translated into Slovenian and French.

The "Monument to Alfred Nobel" (, ) in Saint Petersburg is located along the Bolshaya Nevka River on Petrogradskaya Embankment. It was dedicated in 1991 to mark the 90th anniversary of the first Nobel Prize presentation. Diplomat Thomas Bertelman and Professor Arkady Melua initiators of creation of the monument (1989). Professor A. Melua has provided funds for the establishment of the monument (J.S.Co. "Humanistica", 1990–1991). The abstract metal sculpture was designed by local artists Sergey Alipov and Pavel Shevchenko, and appears to be an explosion or branches of a tree. Petrogradskaya Embankment is the street where the Nobel's family lived until 1859.

Criticism of Nobel focuses on his leading role in weapons manufacturing and sales, and some question his motives in creating his prizes, suggesting they are intended to improve his reputation.





</doc>
<doc id="852" url="https://en.wikipedia.org/wiki?curid=852" title="Alexander Graham Bell">
Alexander Graham Bell

Alexander Graham Bell (; March 3, 1847 – August 2, 1922) was a Scottish-born American inventor, scientist, and engineer who is credited with inventing and patenting the first practical telephone. He also co-founded the American Telephone and Telegraph Company (AT&T) in 1885.

Bell's father, grandfather, and brother had all been associated with work on elocution and speech and both his mother and wife were deaf, profoundly influencing Bell's life's work. His research on hearing and speech further led him to experiment with hearing devices which eventually culminated in Bell being awarded the first U.S. patent for the telephone, on March 7, 1876. Bell considered his invention an intrusion on his real work as a scientist and refused to have a telephone in his study.

Many other inventions marked Bell's later life, including groundbreaking work in optical telecommunications, hydrofoils, and aeronautics. Although Bell was not one of the 33 founders of the National Geographic Society, he had a strong influence on the magazine while serving as the second president from January 7, 1898, until 1903.

Beyond his scientific work, Bell was an advocate of compulsory sterilization, and served as chairman or president of several eugenics organizations. 

Alexander Bell was born in Edinburgh, Scotland, on March 3, 1847. The family home was at South Charlotte Street, and has a stone inscription marking it as Alexander Graham Bell's birthplace. He had two brothers: Melville James Bell (1845–1870) and Edward Charles Bell (1848–1867), both of whom would die of tuberculosis. His father was Professor Alexander Melville Bell, a phonetician, and his mother was Eliza Grace (née Symonds). Born as just "Alexander Bell", at age 10, he made a plea to his father to have a middle name like his two brothers. For his 11th birthday, his father acquiesced and allowed him to adopt the name "Graham", chosen out of respect for Alexander Graham, a Canadian being treated by his father who had become a family friend. To close relatives and friends he remained "Aleck".

As a child, young Bell displayed a curiosity about his world; he in gathered botanical specimens and ran experiments at an early age. His best friend was Ben Herdman, a neighbour whose family operated a flour mill. At the age of 12, Bell built a homemade device that combined rotating paddles with sets of nail brushes, creating a simple dehusking machine that was put into operation at the mill and used steadily for a number of years. In return, Ben's father John Herdman gave both boys the run of a small workshop in which to "invent".

From his early years, Bell showed a sensitive nature and a talent for art, poetry, and music that was encouraged by his mother. With no formal training, he mastered the piano and became the family's pianist. Despite being normally quiet and introspective, he revelled in mimicry and "voice tricks" akin to ventriloquism that continually entertained family guests during their occasional visits. Bell was also deeply affected by his mother's gradual deafness (she began to lose her hearing when he was 12), and learned a manual finger language so he could sit at her side and tap out silently the conversations swirling around the family parlour. He also developed a technique of speaking in clear, modulated tones directly into his mother's forehead wherein she would hear him with reasonable clarity. Bell's preoccupation with his mother's deafness led him to study acoustics.

His family was long associated with the teaching of elocution: his grandfather, Alexander Bell, in London, his uncle in Dublin, and his father, in Edinburgh, were all elocutionists. His father published a variety of works on the subject, several of which are still well known, especially his "The Standard Elocutionist" (1860), which appeared in Edinburgh in 1868. "The Standard Elocutionist" appeared in 168 British editions and sold over a quarter of a million copies in the United States alone. In this treatise, his father explains his methods of how to instruct deaf-mutes (as they were then known) to articulate words and read other people's lip movements to decipher meaning. Bell's father taught him and his brothers not only to write Visible Speech but to identify any symbol and its accompanying sound. Bell became so proficient that he became a part of his father's public demonstrations and astounded audiences with his abilities. He could decipher Visible Speech representing virtually every language, including Latin, Scottish Gaelic, and even Sanskrit, accurately reciting written tracts without any prior knowledge of their pronunciation.

As a young child, Bell, like his brothers, received his early schooling at home from his father. At an early age, he was enrolled at the Royal High School, Edinburgh, Scotland, which he left at the age of 15, having completed only the first four forms. His school record was undistinguished, marked by absenteeism and lacklustre grades. His main interest remained in the sciences, especially biology, while he treated other school subjects with indifference, to the dismay of his father. Upon leaving school, Bell travelled to London to live with his grandfather, Alexander Bell. During the year he spent with his grandfather, a love of learning was born, with long hours spent in serious discussion and study. The elder Bell took great efforts to have his young pupil learn to speak clearly and with conviction, the attributes that his pupil would need to become a teacher himself. At the age of 16, Bell secured a position as a "pupil-teacher" of elocution and music, in Weston House Academy at Elgin, Moray, Scotland. Although he was enrolled as a student in Latin and Greek, he instructed classes himself in return for board and £10 per session. The following year, he attended the University of Edinburgh; joining his older brother Melville who had enrolled there the previous year. In 1868, not long before he departed for Canada with his family, Bell completed his matriculation exams and was accepted for admission to University College London.

His father encouraged Bell's interest in speech and, in 1863, took his sons to see a unique automaton developed by Sir Charles Wheatstone based on the earlier work of Baron Wolfgang von Kempelen. The rudimentary "mechanical man" simulated a human voice. Bell was fascinated by the machine and after he obtained a copy of von Kempelen's book, published in German, and had laboriously translated it, he and his older brother Melville built their own automaton head. Their father, highly interested in their project, offered to pay for any supplies and spurred the boys on with the enticement of a "big prize" if they were successful. While his brother constructed the throat and larynx, Bell tackled the more difficult task of recreating a realistic skull. His efforts resulted in a remarkably lifelike head that could "speak", albeit only a few words. The boys would carefully adjust the "lips" and when a bellows forced air through the windpipe, a very recognizable "Mama" ensued, to the delight of neighbours who came to see the Bell invention.

Intrigued by the results of the automaton, Bell continued to experiment with a live subject, the family's Skye Terrier, "Trouve". After he taught it to growl continuously, Bell would reach into its mouth and manipulate the dog's lips and vocal cords to produce a crude-sounding "Ow ah oo ga ma ma". With little convincing, visitors believed his dog could articulate "How are you, grandma?" Indicative of his playful nature, his experiments convinced onlookers that they saw a "talking dog". These initial forays into experimentation with sound led Bell to undertake his first serious work on the transmission of sound, using tuning forks to explore resonance.

At age 19, Bell wrote a report on his work and sent it to philologist Alexander Ellis, a colleague of his father (who would later be portrayed as Professor Henry Higgins in "Pygmalion"). Ellis immediately wrote back indicating that the experiments were similar to existing work in Germany, and also lent Bell a copy of Hermann von Helmholtz's work, "The Sensations of Tone as a Physiological Basis for the Theory of Music".

Dismayed to find that groundbreaking work had already been undertaken by Helmholtz who had conveyed vowel sounds by means of a similar tuning fork "contraption", Bell pored over the German scientist's book. Working from his own erroneous mistranslation of a French edition, Bell fortuitously then made a deduction that would be the underpinning of all his future work on transmitting sound, reporting: "Without knowing much about the subject, it seemed to me that if vowel sounds could be produced by electrical means, so could consonants, so could articulate speech." He also later remarked: "I thought that Helmholtz had done it ... and that my failure was due only to my ignorance of electricity. It was a valuable blunder ... If I had been able to read German in those days, I might never have commenced my experiments!"

In 1865, when the Bell family moved to London, Bell returned to Weston House as an assistant master and, in his spare hours, continued experiments on sound using a minimum of laboratory equipment. Bell concentrated on experimenting with electricity to convey sound and later installed a telegraph wire from his room in Somerset College to that of a friend. Throughout late 1867, his health faltered mainly through exhaustion. His younger brother, Edward "Ted," was similarly bed-ridden, suffering from tuberculosis. While Bell recovered (by then referring to himself in correspondence as "A. G. Bell") and served the next year as an instructor at Somerset College, Bath, England, his brother's condition deteriorated. Edward would never recover. Upon his brother's death, Bell returned home in 1867. His older brother Melville had married and moved out. With aspirations to obtain a degree at University College London, Bell considered his next years as preparation for the degree examinations, devoting his spare time at his family's residence to studying.

Helping his father in Visible Speech demonstrations and lectures brought Bell to Susanna E. Hull's private school for the deaf in South Kensington, London. His first two pupils were deaf-mute girls who made remarkable progress under his tutelage. While his older brother seemed to achieve success on many fronts including opening his own elocution school, applying for a patent on an invention, and starting a family, Bell continued as a teacher. However, in May 1870, Melville died from complications due to tuberculosis, causing a family crisis. His father had also suffered a debilitating illness earlier in life and had been restored to health by a convalescence in Newfoundland. Bell's parents embarked upon a long-planned move when they realized that their remaining son was also sickly. Acting decisively, Alexander Melville Bell asked Bell to arrange for the sale of all the family property, conclude all of his brother's affairs (Bell took over his last student, curing a pronounced lisp), and join his father and mother in setting out for the "New World". Reluctantly, Bell also had to conclude a relationship with Marie Eccleston, who, as he had surmised, was not prepared to leave England with him.

In 1870, 23-year-old Bell travelled with his parents and his brother's widow, Caroline Margaret Ottaway, to Paris, Ontario, to stay with the Reverend Thomas Henderson, a family friend. The Bell family soon purchased a farm of at Tutelo Heights (now called Tutela Heights), near Brantford, Ontario. The property consisted of an orchard, large farmhouse, stable, pigsty, hen-house, and a carriage house, which bordered the Grand River.

At the homestead, Bell set up his own workshop in the converted carriage house near to what he called his "dreaming place", a large hollow nestled in trees at the back of the property above the river. Despite his frail condition upon arriving in Canada, Bell found the climate and environs to his liking, and rapidly improved. He continued his interest in the study of the human voice and when he discovered the Six Nations Reserve across the river at Onondaga, he learned the Mohawk language and translated its unwritten vocabulary into Visible Speech symbols. For his work, Bell was awarded the title of Honorary Chief and participated in a ceremony where he donned a Mohawk headdress and danced traditional dances.

After setting up his workshop, Bell continued experiments based on Helmholtz's work with electricity and sound. He also modified a melodeon (a type of pump organ) so that it could transmit its music electrically over a distance. Once the family was settled in, both Bell and his father made plans to establish a teaching practice and in 1871, he accompanied his father to Montreal, where Melville was offered a position to teach his System of Visible Speech.

Bell's father was invited by Sarah Fuller, principal of the Boston School for Deaf Mutes (which continues today as the public Horace Mann School for the Deaf), in Boston, Massachusetts, United States, to introduce the Visible Speech System by providing training for Fuller's instructors, but he declined the post in favour of his son. Travelling to Boston in April 1871, Bell proved successful in training the school's instructors. He was subsequently asked to repeat the programme at the American Asylum for Deaf-mutes in Hartford, Connecticut, and the Clarke School for the Deaf in Northampton, Massachusetts.

Returning home to Brantford after six months abroad, Bell continued his experiments with his "harmonic telegraph". The basic concept behind his device was that messages could be sent through a single wire if each message was transmitted at a different pitch, but work on both the transmitter and receiver was needed.

Unsure of his future, he first contemplated returning to London to complete his studies, but decided to return to Boston as a teacher. His father helped him set up his private practice by contacting Gardiner Greene Hubbard, the president of the Clarke School for the Deaf for a recommendation. Teaching his father's system, in October 1872, Alexander Bell opened his "School of Vocal Physiology and Mechanics of Speech" in Boston, which attracted a large number of deaf pupils, with his first class numbering 30 students. While he was working as a private tutor, one of his pupils was Helen Keller, who came to him as a young child unable to see, hear, or speak. She was later to say that Bell dedicated his life to the penetration of that "inhuman silence which separates and estranges". In 1893, Keller performed the sod-breaking ceremony for the construction of Bell's new Volta Bureau, dedicated to "the increase and diffusion of knowledge relating to the deaf".

Several influential people of the time, including Bell, viewed deafness as something that should be eradicated, and also believed that with resources and effort, they could teach the deaf to speak and avoid the use of sign language, thus enabling their integration within the wider society from which many were often being excluded. Owing to his efforts to suppress the teaching of sign language, Bell is often viewed negatively by those embracing Deaf culture.

In 1872, Bell became professor of Vocal Physiology and Elocution at the Boston University School of Oratory. During this period, he alternated between Boston and Brantford, spending summers in his Canadian home. At Boston University, Bell was "swept up" by the excitement engendered by the many scientists and inventors residing in the city. He continued his research in sound and endeavored to find a way to transmit musical notes and articulate speech, but although absorbed by his experiments, he found it difficult to devote enough time to experimentation. While days and evenings were occupied by his teaching and private classes, Bell began to stay awake late into the night, running experiment after experiment in rented facilities at his boarding house. Keeping "night owl" hours, he worried that his work would be discovered and took great pains to lock up his notebooks and laboratory equipment. Bell had a specially made table where he could place his notes and equipment inside a locking cover. Worse still, his health deteriorated as he suffered severe headaches. Returning to Boston in fall 1873, Bell made a fateful decision to concentrate on his experiments in sound.

Deciding to give up his lucrative private Boston practice, Bell retained only two students, six-year-old "Georgie" Sanders, deaf from birth, and 15-year-old Mabel Hubbard. Each pupil would play an important role in the next developments. George's father, Thomas Sanders, a wealthy businessman, offered Bell a place to stay in nearby Salem with Georgie's grandmother, complete with a room to "experiment". Although the offer was made by George's mother and followed the year-long arrangement in 1872 where her son and his nurse had moved to quarters next to Bell's boarding house, it was clear that Mr. Sanders was backing the proposal. The arrangement was for teacher and student to continue their work together, with free room and board thrown in. Mabel was a bright, attractive girl who was ten years Bell's junior but became the object of his affection. Having lost her hearing after a near-fatal bout of scarlet fever close to her fifth birthday, she had learned to read lips but her father, Gardiner Greene Hubbard, Bell's benefactor and personal friend, wanted her to work directly with her teacher.

By 1874, Bell's initial work on the harmonic telegraph had entered a formative stage, with progress made both at his new Boston "laboratory" (a rented facility) and at his family home in Canada a big success. While working that summer in Brantford, Bell experimented with a "phonautograph", a pen-like machine that could draw shapes of sound waves on smoked glass by tracing their vibrations. Bell thought it might be possible to generate undulating electrical currents that corresponded to sound waves. Bell also thought that multiple metal reeds tuned to different frequencies like a harp would be able to convert the undulating currents back into sound. But he had no working model to demonstrate the feasibility of these ideas.

In 1874, telegraph message traffic was rapidly expanding and in the words of Western Union President William Orton, had become "the nervous system of commerce". Antonio Meucci sent a telephone model and technical details to the Western Union telegraph company but failed to win a meeting with executives. When he asked for his materials to be returned, in 1874, he was told they had been lost. Two years later Bell, who shared a laboratory with Meucci, filed a patent for a telephone, became a celebrity and made a lucrative deal with Western Union. Meucci sued and was nearing victory—the supreme court agreed to hear the case and fraud charges were initiated against Bell—when the Florentine died in 1889. The legal action died with him. Orton had contracted with inventors Thomas Edison and Elisha Gray to find a way to send multiple telegraph messages on each telegraph line to avoid the great cost of constructing new lines. When Bell mentioned to Gardiner Hubbard and Thomas Sanders that he was working on a method of sending multiple tones on a telegraph wire using a multi-reed device, the two wealthy patrons began to financially support Bell's experiments. Patent matters would be handled by Hubbard's patent attorney, Anthony Pollok.

In March 1875, Bell and Pollok visited the scientist Joseph Henry, who was then director of the Smithsonian Institution, and asked Henry's advice on the electrical multi-reed apparatus that Bell hoped would transmit the human voice by telegraph. Henry replied that Bell had "the germ of a great invention". When Bell said that he did not have the necessary knowledge, Henry replied, "Get it!" That declaration greatly encouraged Bell to keep trying, even though he did not have the equipment needed to continue his experiments, nor the ability to create a working model of his ideas. However, a chance meeting in 1874 between Bell and Thomas A. Watson, an experienced electrical designer and mechanic at the electrical machine shop of Charles Williams, changed all that.

With financial support from Sanders and Hubbard, Bell hired Thomas Watson as his assistant, and the two of them experimented with acoustic telegraphy. On June 2, 1875, Watson accidentally plucked one of the reeds and Bell, at the receiving end of the wire, heard the overtones of the reed; overtones that would be necessary for transmitting speech. That demonstrated to Bell that only one reed or armature was necessary, not multiple reeds. This led to the "gallows" sound-powered telephone, which could transmit indistinct, voice-like sounds, but not clear speech.

In 1875, Bell developed an acoustic telegraph and drew up a patent application for it. Since he had agreed to share U.S. profits with his investors Gardiner Hubbard and Thomas Sanders, Bell requested that an associate in Ontario, George Brown, attempt to patent it in Britain, instructing his lawyers to apply for a patent in the U.S. only after they received word from Britain (Britain would issue patents only for discoveries not previously patented elsewhere).

Meanwhile, Elisha Gray was also experimenting with acoustic telegraphy and thought of a way to transmit speech using a water transmitter. On February 14, 1876, Gray filed a caveat with the U.S. Patent Office for a telephone design that used a water transmitter. That same morning, Bell's lawyer filed Bell's application with the patent office. There is considerable debate about who arrived first and Gray later challenged the primacy of Bell's patent. Bell was in Boston on February 14 and did not arrive in Washington until February 26.

Bell's patent 174,465, was issued to Bell on March 7, 1876, by the U.S. Patent Office. Bell's patent covered "the method of, and apparatus for, transmitting vocal or other sounds telegraphically ... by causing electrical undulations, similar in form to the vibrations of the air accompanying the said vocal or other sound" Bell returned to Boston the same day and the next day resumed work, drawing in his notebook a diagram similar to that in Gray's patent caveat.

On March 10, 1876, three days after his patent was issued, Bell succeeded in getting his telephone to work, using a liquid transmitter similar to Gray's design. Vibration of the diaphragm caused a needle to vibrate in the water, varying the electrical resistance in the circuit. When Bell spoke the sentence "Mr. Watson—Come here—I want to see you" into the liquid transmitter, Watson, listening at the receiving end in an adjoining room, heard the words clearly.

Although Bell was, and still is, accused of stealing the telephone from Gray, Bell used Gray's water transmitter design only after Bell's patent had been granted, and only as a proof of concept scientific experiment, to prove to his own satisfaction that intelligible "articulate speech" (Bell's words) could be electrically transmitted. After March 1876, Bell focused on improving the electromagnetic telephone and never used Gray's liquid transmitter in public demonstrations or commercial use.

The question of priority for the variable resistance feature of the telephone was raised by the examiner before he approved Bell's patent application. He told Bell that his claim for the variable resistance feature was also described in Gray's caveat. Bell pointed to a variable resistance device in his previous application in which he described a cup of mercury, not water. He had filed the mercury application at the patent office a year earlier on February 25, 1875, long before Elisha Gray described the water device. In addition, Gray abandoned his caveat, and because he did not contest Bell's priority, the examiner approved Bell's patent on March 3, 1876. Gray had reinvented the variable resistance telephone, but Bell was the first to write down the idea and the first to test it in a telephone.

The patent examiner, Zenas Fisk Wilber, later stated in an affidavit that he was an alcoholic who was much in debt to Bell's lawyer, Marcellus Bailey, with whom he had served in the Civil War. He claimed he showed Gray's patent caveat to Bailey. Wilber also claimed (after Bell arrived in Washington D.C. from Boston) that he showed Gray's caveat to Bell and that Bell paid him $100 (). Bell claimed they discussed the patent only in general terms, although in a letter to Gray, Bell admitted that he learned some of the technical details. Bell denied in an affidavit that he ever gave Wilber any money.

On March 10, 1876 Bell used "the instrument" in Boston to call Thomas Watson who was in another room but out of earshot. He said, "Mr. Watson, come here – I want to see you" and Watson soon appeared at his side.

Continuing his experiments in Brantford, Bell brought home a working model of his telephone. On August 3, 1876, from the telegraph office in Brantford, Ontario, Bell sent a tentative telegram to the village of Mount Pleasant distant, indicating that he was ready. He made a telephone call via telegraph wires and faint voices were heard replying. The following night, he amazed guests as well as his family with a call between the Bell Homestead and the office of the Dominion Telegraph Company in Brantford along an improvised wire strung up along telegraph lines and fences, and laid through a tunnel. This time, guests at the household distinctly heard people in Brantford reading and singing. The third test on August 10, 1876, was made via the telegraph line between Brantford and Paris, Ontario, distant. This test was said by many sources to be the "world's first long distance call". The final test certainly proved that the telephone could work over long distances, at least as a one-way call.

The first two-way (reciprocal) conversation over a line occurred between Cambridge and Boston (roughly 2.5 miles) on October 9, 1876. During that conversation, Bell was on Kilby Street in Boston and Watson was at the offices of the Walworth Manufacturing Company. 

Bell and his partners, Hubbard and Sanders, offered to sell the patent outright to Western Union for $100,000. The president of Western Union balked, countering that the telephone was nothing but a toy. Two years later, he told colleagues that if he could get the patent for $25 million he would consider it a bargain. By then, the Bell company no longer wanted to sell the patent. Bell's investors would become millionaires while he fared well from residuals and at one point had assets of nearly one million dollars.

Bell began a series of public demonstrations and lectures to introduce the new invention to the scientific community as well as the general public. A short time later, his demonstration of an early telephone prototype at the 1876 Centennial Exposition in Philadelphia brought the telephone to international attention. Influential visitors to the exhibition included Emperor Pedro II of Brazil. One of the judges at the Exhbition, Sir William Thomson (later, Lord Kelvin), a renowned Scottish scientist, described the telephone as "the greatest by far of all the marvels of the electric telegraph". 

On January 14, 1878, at Osborne House, on the Isle of Wight, Bell demonstrated the device to Queen Victoria, placing calls to Cowes, Southampton and London. These were the first publicly-witnessed long distance telephone calls in the UK. The queen considered the process to be "quite extraordinary" although the sound was "rather faint". She later asked to buy the equipment that was used, but Bell offered to make "a set of telephones" specifically for her.

The Bell Telephone Company was created in 1877, and by 1886, more than 150,000 people in the U.S. owned telephones. Bell Company engineers made numerous other improvements to the telephone, which emerged as one of the most successful products ever. In 1879, the Bell company acquired Edison's patents for the carbon microphone from Western Union. This made the telephone practical for longer distances, and it was no longer necessary to shout to be heard at the receiving telephone.

Emperor Pedro II of Brazil was the first person to buy stock in Bell's company, the Bell Telephone Company. One of the first telephones in a private residence was installed in his palace in Petrópolis, his summer retreat from Rio de Janeiro.

In January 1915, Bell made the first ceremonial transcontinental telephone call. Calling from the AT&T head office at 15 Dey Street in New York City, Bell was heard by Thomas Watson at 333 Grant Avenue in San Francisco. The "New York Times" reported:

As is sometimes common in scientific discoveries, simultaneous developments can occur, as evidenced by a number of inventors who were at work on the telephone. Over a period of 18 years, the Bell Telephone Company faced 587 court challenges to its patents, including five that went to the U.S. Supreme Court, but none was successful in establishing priority over the original Bell patent and the Bell Telephone Company never lost a case that had proceeded to a final trial stage. Bell's laboratory notes and family letters were the key to establishing a long lineage to his experiments. The Bell company lawyers successfully fought off myriad lawsuits generated initially around the challenges by Elisha Gray and Amos Dolbear. In personal correspondence to Bell, both Gray and Dolbear had acknowledged his prior work, which considerably weakened their later claims.

On January 13, 1887, the U.S. Government moved to annul the patent issued to Bell on the grounds of fraud and misrepresentation. After a series of decisions and reversals, the Bell company won a decision in the Supreme Court, though a couple of the original claims from the lower court cases were left undecided. By the time that the trial wound its way through nine years of legal battles, the U.S. prosecuting attorney had died and the two Bell patents (No. 174,465 dated March 7, 1876, and No. 186,787 dated January 30, 1877) were no longer in effect, although the presiding judges agreed to continue the proceedings due to the case's importance as a precedent. With a change in administration and charges of conflict of interest (on both sides) arising from the original trial, the US Attorney General dropped the lawsuit on November 30, 1897, leaving several issues undecided on the merits.

During a deposition filed for the 1887 trial, Italian inventor Antonio Meucci also claimed to have created the first working model of a telephone in Italy in 1834. In 1886, in the first of three cases in which he was involved, Meucci took the stand as a witness in the hope of establishing his invention's priority. Meucci's testimony in this case was disputed due to a lack of material evidence for his inventions, as his working models were purportedly lost at the laboratory of American District Telegraph (ADT) of New York, which was later incorporated as a subsidiary of Western Union in 1901. Meucci's work, like many other inventors of the period, was based on earlier acoustic principles and despite evidence of earlier experiments, the final case involving Meucci was eventually dropped upon Meucci's death. However, due to the efforts of Congressman Vito Fossella, the U.S. House of Representatives on June 11, 2002, stated that Meucci's "work in the invention of the telephone should be acknowledged". This did not put an end to the still-contentious issue. Some modern scholars do not agree with the claims that Bell's work on the telephone was influenced by Meucci's inventions.

The value of the Bell patent was acknowledged throughout the world, and patent applications were made in most major countries, but when Bell delayed the German patent application, the electrical firm of Siemens & Halske (S&H) set up a rival manufacturer of Bell telephones under their own patent. The Siemens company produced near-identical copies of the Bell telephone without having to pay royalties. The establishment of the International Bell Telephone Company in Brussels, Belgium in 1880, as well as a series of agreements in other countries eventually consolidated a global telephone operation. The strain put on Bell by his constant appearances in court, necessitated by the legal battles, eventually resulted in his resignation from the company.

On July 11, 1877, a few days after the Bell Telephone Company was established, Bell married Mabel Hubbard (1857–1923) at the Hubbard estate in Cambridge, Massachusetts. His wedding present to his bride was to turn over 1,487 of his 1,497 shares in the newly formed Bell Telephone Company. Shortly thereafter, the newlyweds embarked on a year-long honeymoon in Europe. During that excursion, Bell took a handmade model of his telephone with him, making it a "working holiday". The courtship had begun years earlier; however, Bell waited until he was more financially secure before marrying. Although the telephone appeared to be an "instant" success, it was not initially a profitable venture and Bell's main sources of income were from lectures until after 1897. One unusual request exacted by his fiancée was that he use "Alec" rather than the family's earlier familiar name of "Aleck". From 1876, he would sign his name "Alec Bell". They had four children:
The Bell family home was in Cambridge, Massachusetts, until 1880 when Bell's father-in-law bought a house in Washington, D.C.; in 1882 he bought a home in the same city for Bell's family, so they could be with him while he attended to the numerous court cases involving patent disputes.

Bell was a British subject throughout his early life in Scotland and later in Canada until 1882 when he became a naturalized citizen of the United States. In 1915, he characterized his status as: "I am not one of those hyphenated Americans who claim allegiance to two countries." Despite this declaration, Bell has been proudly claimed as a "native son" by all three countries he resided in: the United States, Canada, and the United Kingdom.

By 1885, a new summer retreat was contemplated. That summer, the Bells had a vacation on Cape Breton Island in Nova Scotia, spending time at the small village of Baddeck. Returning in 1886, Bell started building an estate on a point across from Baddeck, overlooking Bras d'Or Lake. By 1889, a large house, christened "The Lodge" was completed and two years later, a larger complex of buildings, including a new laboratory, were begun that the Bells would name Beinn Bhreagh (Gaelic: "beautiful mountain") after Bell's ancestral Scottish highlands. Bell also built the Bell Boatyard on the estate, employing up to 40 people building experimental craft as well as wartime lifeboats and workboats for the Royal Canadian Navy and pleasure craft for the Bell family. He was an enthusiastic boater, and Bell and his family sailed or rowed a long series of vessels on Bras d'Or Lake, ordering additional vessels from the H.W. Embree and Sons boatyard in Port Hawkesbury, Nova Scotia. In his final, and some of his most productive years, Bell split his residency between Washington, D.C., where he and his family initially resided for most of the year, and Beinn Bhreagh, where they spent increasing amounts of time.

Until the end of his life, Bell and his family would alternate between the two homes, but "Beinn Bhreagh" would, over the next 30 years, become more than a summer home as Bell became so absorbed in his experiments that his annual stays lengthened. Both Mabel and Bell became immersed in the Baddeck community and were accepted by the villagers as "their own". The Bells were still in residence at "Beinn Bhreagh" when the Halifax Explosion occurred on December 6, 1917. Mabel and Bell mobilized the community to help victims in Halifax.

Although Alexander Graham Bell is most often associated with the invention of the telephone, his interests were extremely varied. According to one of his biographers, Charlotte Gray, Bell's work ranged "unfettered across the scientific landscape" and he often went to bed voraciously reading the "Encyclopædia Britannica", scouring it for new areas of interest. The range of Bell's inventive genius is represented only in part by the 18 patents granted in his name alone and the 12 he shared with his collaborators. These included 14 for the telephone and telegraph, four for the photophone, one for the phonograph, five for aerial vehicles, four for "hydroairplanes", and two for selenium cells. Bell's inventions spanned a wide range of interests and included a metal jacket to assist in breathing, the audiometer to detect minor hearing problems, a device to locate icebergs, investigations on how to separate salt from seawater, and work on finding alternative fuels.

Bell worked extensively in medical research and invented techniques for teaching speech to the deaf. During his Volta Laboratory period, Bell and his associates considered impressing a magnetic field on a record as a means of reproducing sound. Although the trio briefly experimented with the concept, they could not develop a workable prototype. They abandoned the idea, never realizing they had glimpsed a basic principle which would one day find its application in the tape recorder, the hard disc and floppy disc drive, and other magnetic media.

Bell's own home used a primitive form of air conditioning, in which fans blew currents of air across great blocks of ice. He also anticipated modern concerns with fuel shortages and industrial pollution. Methane gas, he reasoned, could be produced from the waste of farms and factories. At his Canadian estate in Nova Scotia, he experimented with composting toilets and devices to capture water from the atmosphere. In a magazine interview published shortly before his death, he reflected on the possibility of using solar panels to heat houses.

Bell and his assistant Charles Sumner Tainter jointly invented a wireless telephone, named a photophone, which allowed for the transmission of both sounds and normal human conversations on a beam of light. Both men later became full associates in the Volta Laboratory Association.

On June 21, 1880, Bell's assistant transmitted a wireless voice telephone message a considerable distance, from the roof of the Franklin School in Washington, D.C., to Bell at the window of his laboratory, some away, 19 years before the first voice radio transmissions.

Bell believed the photophone's principles were his life's "greatest achievement", telling a reporter shortly before his death that the photophone was "the greatest invention [I have] ever made, greater than the telephone". The photophone was a precursor to the fiber-optic communication systems which achieved popular worldwide usage in the 1980s. Its master patent was issued in December 1880, many decades before the photophone's principles came into popular use.

Bell is also credited with developing one of the early versions of a metal detector through the use of an induction balance, after the shooting of U.S. President James A. Garfield in 1881. According to some accounts, the metal detector worked flawlessly in tests but did not find Guiteau's bullet, partly because the metal bed frame on which the President was lying disturbed the instrument, resulting in static. Garfield's surgeons, led by self-appointed chief physician Doctor Willard Bliss, were skeptical of the device, and ignored Bell's requests to move the President to a bed not fitted with metal springs. Alternatively, although Bell had detected a slight sound on his first test, the bullet may have been lodged too deeply to be detected by the crude apparatus.

Bell's own detailed account, presented to the American Association for the Advancement of Science in 1882, differs in several particulars from most of the many and varied versions now in circulation, by concluding that extraneous metal was not to blame for failure to locate the bullet. Perplexed by the peculiar results he had obtained during an examination of Garfield, Bell "proceeded to the Executive Mansion the next morning ... to ascertain from the surgeons whether they were perfectly sure that all metal had been removed from the neighborhood of the bed. It was then recollected that underneath the horse-hair mattress on which the President lay was another mattress composed of steel wires. Upon obtaining a duplicate, the mattress was found to consist of a sort of net of woven steel wires, with large meshes. The extent of the [area that produced a response from the detector] having been so small, as compared with the area of the bed, it seemed reasonable to conclude that the steel mattress had produced no detrimental effect." In a footnote, Bell adds, "The death of President Garfield and the subsequent "post-mortem" examination, however, proved that the bullet was at too great a distance from the surface to have affected our apparatus."

The March 1906 "Scientific American" article by American pioneer William E. Meacham explained the basic principle of hydrofoils and hydroplanes. Bell considered the invention of the hydroplane as a very significant achievement. Based on information gained from that article, he began to sketch concepts of what is now called a hydrofoil boat. Bell and assistant Frederick W. "Casey" Baldwin began hydrofoil experimentation in the summer of 1908 as a possible aid to airplane takeoff from water. Baldwin studied the work of the Italian inventor Enrico Forlanini and began testing models. This led him and Bell to the development of practical hydrofoil watercraft.

During his world tour of 1910–11, Bell and Baldwin met with Forlanini in France. They had rides in the Forlanini hydrofoil boat over Lake Maggiore. Baldwin described it as being as smooth as flying. On returning to Baddeck, a number of initial concepts were built as experimental models, including the "Dhonnas Beag" (Scottish Gaelic for "little devil"), the first self-propelled Bell-Baldwin hydrofoil. The experimental boats were essentially proof-of-concept prototypes that culminated in the more substantial HD-4, powered by Renault engines. A top speed of was achieved, with the hydrofoil exhibiting rapid acceleration, good stability, and steering, along with the ability to take waves without difficulty. In 1913, Dr. Bell hired Walter Pinaud, a Sydney yacht designer and builder as well as the proprietor of Pinaud's Yacht Yard in Westmount, Nova Scotia, to work on the pontoons of the HD-4. Pinaud soon took over the boatyard at Bell Laboratories on Beinn Bhreagh, Bell's estate near Baddeck, Nova Scotia. Pinaud's experience in boat-building enabled him to make useful design changes to the HD-4. After the First World War, work began again on the HD-4. Bell's report to the U.S. Navy permitted him to obtain two engines in July 1919. On September 9, 1919, the HD-4 set a world marine speed record of , a record which stood for ten years.

In 1891, Bell had begun experiments to develop motor-powered heavier-than-air aircraft. The AEA was first formed as Bell shared the vision to fly with his wife, who advised him to seek "young" help as Bell was at the age of 60.

In 1898, Bell experimented with tetrahedral box kites and wings constructed of multiple compound tetrahedral kites covered in maroon silk. The tetrahedral wings were named "Cygnet" I, II, and III, and were flown both unmanned and manned ("Cygnet I" crashed during a flight carrying Selfridge) in the period from 1907–1912. Some of Bell's kites are on display at the Alexander Graham Bell National Historic Site.

Bell was a supporter of aerospace engineering research through the Aerial Experiment Association (AEA), officially formed at Baddeck, Nova Scotia, in October 1907 at the suggestion of his wife Mabel and with her financial support after the sale of some of her real estate. The AEA was headed by Bell and the founding members were four young men: American Glenn H. Curtiss, a motorcycle manufacturer at the time and who held the title "world's fastest man", having ridden his self-constructed motor bicycle around in the shortest time, and who was later awarded the Scientific American Trophy for the first official one-kilometre flight in the Western hemisphere, and who later became a world-renowned airplane manufacturer; Lieutenant Thomas Selfridge, an official observer from the U.S. Federal government and one of the few people in the army who believed that aviation was the future; Frederick W. Baldwin, the first Canadian and first British subject to pilot a public flight in Hammondsport, New York; and J. A .D. McCurdy–Baldwin and McCurdy being new engineering graduates from the University of Toronto.

The AEA's work progressed to heavier-than-air machines, applying their knowledge of kites to gliders. Moving to Hammondsport, the group then designed and built the "Red Wing", framed in bamboo and covered in red silk and powered by a small air-cooled engine. On March 12, 1908, over Keuka Lake, the biplane lifted off on the first public flight in North America. The innovations that were incorporated into this design included a cockpit enclosure and tail rudder (later variations on the original design would add ailerons as a means of control). One of the AEA's inventions, a practical wingtip form of the aileron, was to become a standard component on all aircraft. The "White Wing" and "June Bug" were to follow and by the end of 1908, over 150 flights without mishap had been accomplished. However, the AEA had depleted its initial reserves and only a $15,000 grant from Mrs. Bell allowed it to continue with experiments. Lt. Selfridge had also become the first person killed in a powered heavier-than-air flight in a crash of the Wright Flyer at Fort Myer, Virginia, on September 17, 1908.

Their final aircraft design, the "Silver Dart", embodied all of the advancements found in the earlier machines. On February 23, 1909, Bell was present as the "Silver Dart" flown by J. A. D. McCurdy from the frozen ice of Bras d'Or made the first aircraft flight in Canada. Bell had worried that the flight was too dangerous and had arranged for a doctor to be on hand. With the successful flight, the AEA disbanded and the "Silver Dart" would revert to Baldwin and McCurdy, who began the Canadian Aerodrome Company and would later demonstrate the aircraft to the Canadian Army.

Bell was connected with the eugenics movement in the United States. In his lecture "Memoir upon the formation of a deaf variety of the human race" presented to the National Academy of Sciences on November 13, 1883 (the year of his election as a Member of the National Academy of Sciences), he noted that congenitally deaf parents were more likely to produce deaf children and tentatively suggested that couples where both parties were deaf should not marry. However, it was his hobby of livestock breeding which led to his appointment to biologist David Starr Jordan's Committee on Eugenics, under the auspices of the American Breeders' Association. The committee unequivocally extended the principle to humans. From 1912 until 1918, he was the chairman of the board of scientific advisers to the Eugenics Record Office associated with Cold Spring Harbor Laboratory in New York, and regularly attended meetings. In 1921, he was the honorary president of the Second International Congress of Eugenics held under the auspices of the American Museum of Natural History in New York. Organizations such as these advocated passing laws (with success in some states) that established the compulsory sterilization of people deemed to be, as Bell called them, a "defective variety of the human race". By the late 1930s, about half the states in the U.S. had eugenics laws, and California's compulsory sterilization law was used as a model for that of Nazi Germany.

Honors and tributes flowed to Bell in increasing numbers as his invention became ubiquitous and his personal fame grew. Bell received numerous honorary degrees from colleges and universities to the point that the requests almost became burdensome. During his life, he also received dozens of major awards, medals, and other tributes. These included statuary monuments to both him and the new form of communication his telephone created, including the Bell Telephone Memorial erected in his honor in "Alexander Graham Bell Gardens" in Brantford, Ontario, in 1917.

A large number of Bell's writings, personal correspondence, notebooks, papers, and other documents reside in both the United States Library of Congress Manuscript Division (as the "Alexander Graham Bell Family Papers"), and at the Alexander Graham Bell Institute, Cape Breton University, Nova Scotia; major portions of which are available for online viewing.

A number of historic sites and other marks commemorate Bell in North America and Europe, including the first telephone companies in the United States and Canada. Among the major sites are:

In 1880, Bell received the Volta Prize with a purse of 50,000 French francs (approximately US$ in today's dollars) for the invention of the telephone from the French government. Among the luminaries who judged were Victor Hugo and Alexandre Dumas, "fils". The Volta Prize was conceived by Napoleon III in 1852, and named in honor of Alessandro Volta, with Bell becoming the second recipient of the grand prize in its history. Since Bell was becoming increasingly affluent, he used his prize money to create endowment funds (the 'Volta Fund') and institutions in and around the United States capital of Washington, D.C.. These included the prestigious" 'Volta Laboratory Association' "(1880), also known as the" Volta Laboratory "and as the" 'Alexander Graham Bell Laboratory', "and which eventually led to the Volta Bureau (1887) as a center for studies on deafness which is still in operation in Georgetown, Washington, D.C. The Volta Laboratory became an experimental facility devoted to scientific discovery, and the very next year it improved Edison's phonograph by substituting wax for tinfoil as the recording medium and incising the recording rather than indenting it, key upgrades that Edison himself later adopted. The laboratory was also the site where he and his associate invented his "proudest achievement", "the photophone", the "optical telephone" which presaged fibre optical telecommunications while the Volta Bureau would later evolve into the Alexander Graham Bell Association for the Deaf and Hard of Hearing (the AG Bell), a leading center for the research and pedagogy of deafness.

In partnership with Gardiner Greene Hubbard, Bell helped establish the publication "Science" during the early 1880s. In 1898, Bell was elected as the second president of the National Geographic Society, serving until 1903, and was primarily responsible for the extensive use of illustrations, including photography, in the magazine. He also served for many years as a Regent of the Smithsonian Institution (1898–1922). The French government conferred on him the decoration of the Légion d'honneur (Legion of Honor); the Royal Society of Arts in London awarded him the Albert Medal in 1902; the University of Würzburg, Bavaria, granted him a PhD, and he was awarded the Franklin Institute's Elliott Cresson Medal in 1912. He was one of the founders of the American Institute of Electrical Engineers in 1884 and served as its president from 1891–92. Bell was later awarded the AIEE's Edison Medal in 1914 "For meritorious achievement in the invention of the telephone".

The "bel" (B) and the smaller "decibel" (dB) are units of measurement of sound pressure level (SPL) invented by Bell Labs and named after him. Since 1976, the IEEE's Alexander Graham Bell Medal has been awarded to honor outstanding contributions in the field of telecommunications.
In 1936, the US Patent Office declared Bell first on its list of the country's greatest inventors, leading to the US Post Office issuing a commemorative stamp honoring Bell in 1940 as part of its 'Famous Americans Series'. The First Day of Issue ceremony was held on October 28 in Boston, Massachusetts, the city where Bell spent considerable time on research and working with the deaf. The Bell stamp became very popular and sold out in little time. The stamp became, and remains to this day, the most valuable one of the series.

The 150th anniversary of Bell's birth in 1997 was marked by a special issue of commemorative £1 banknotes from the Royal Bank of Scotland. The illustrations on the reverse of the note include Bell's face in profile, his signature, and objects from Bell's life and career: users of the telephone over the ages; an audio wave signal; a diagram of a telephone receiver; geometric shapes from engineering structures; representations of sign language and the phonetic alphabet; the geese which helped him to understand flight; and the sheep which he studied to understand genetics. Additionally, the Government of Canada honored Bell in 1997 with a C$100 gold coin, in tribute also to the 150th anniversary of his birth, and with a silver dollar coin in 2009 in honor of the 100th anniversary of flight in Canada. That first flight was made by an airplane designed under Dr. Bell's tutelage, named the Silver Dart. Bell's image, and also those of his many inventions have graced paper money, coinage, and postal stamps in numerous countries worldwide for many dozens of years.

Alexander Graham Bell was ranked 57th among the 100 Greatest Britons (2002) in an official BBC nationwide poll, and among the Top Ten Greatest Canadians (2004), and the 100 Greatest Americans (2005). In 2006, Bell was also named as one of the 10 greatest Scottish scientists in history after having been listed in the National Library of Scotland's 'Scottish Science Hall of Fame'. Bell's name is still widely known and used as part of the names of dozens of educational institutes, corporate namesakes, street and place names around the world.

Alexander Graham Bell, who could not complete the university program of his youth, received at least a dozen honorary degrees from academic institutions, including eight honorary LL.D.s (Doctorate of Laws), two Ph.D.s, a D.Sc., and an M.D.:


Bell died of complications arising from diabetes on August 2, 1922, at his private estate in Cape Breton, Nova Scotia, at age 75. Bell had also been afflicted with pernicious anemia. His last view of the land he had inhabited was by moonlight on his mountain estate at 2:00 a.m. While tending to him after his long illness, Mabel, his wife, whispered, "Don't leave me." By way of reply, Bell signed "no...", lost consciousness, and died shortly after.

On learning of Bell's death, the Canadian Prime Minister, Mackenzie King, cabled Mrs. Bell, saying:
Bell's coffin was constructed of Beinn Bhreagh pine by his laboratory staff, lined with the same red silk fabric used in his tetrahedral kite experiments. To help celebrate his life, his wife asked guests not to wear black (the traditional funeral color) while attending his service, during which soloist Jean MacDonald sang a verse of Robert Louis Stevenson's "Requiem":

Upon the conclusion of Bell's funeral, "every phone on the continent of North America was silenced in honor of the man who had given to mankind the means for direct communication at a distance".

Dr. Alexander Graham Bell was buried atop Beinn Bhreagh mountain, on his estate where he had resided increasingly for the last 35 years of his life, overlooking Bras d'Or Lake. He was survived by his wife Mabel, his two daughters, Elsie May and Marian, and nine of his grandchildren.





</doc>
<doc id="854" url="https://en.wikipedia.org/wiki?curid=854" title="Anatolia">
Anatolia

Anatolia (from Greek: , ', "east" or "[sun]rise"; ), also known as Asia Minor (Medieval and Modern Greek: , ', "small Asia"; ), Asian Turkey, the Anatolian peninsula or the Anatolian plateau, is a large peninsula in West Asia and the westernmost protrusion of the Asian continent. It makes up the majority of modern-day Turkey. The region is bounded by the Black Sea to the north, the Mediterranean Sea to the south, the Armenian Highlands to the east and the Aegean Sea to the west. The Sea of Marmara forms a connection between the Black and Aegean seas through the Bosphorus and Dardanelles straits and separates Anatolia from Thrace on the Balkan peninsula of Europe.

The eastern border of Anatolia is traditionally held to be a line between the Gulf of Alexandretta and the Black Sea, bounded by the Armenian Highland to the east and Mesopotamia to the southeast. Thus, traditionally Anatolia is the territory that comprises approximately the western two-thirds of the Asian part of Turkey. Today, Anatolia is also often considered to be synonymous with Asian Turkey, which comprises almost the entire country; its eastern and southeastern borders are widely taken to be Turkey's eastern border. By some definitions, the Armenian Highlands lies beyond the boundary of the Anatolian plateau. The official name of this inland region is the Eastern Anatolia Region.

The ancient inhabitants of Anatolia spoke the now-extinct Anatolian languages, which were largely replaced by the Greek language starting from classical antiquity and during the Hellenistic, Roman and Byzantine periods. Major Anatolian languages included Hittite, Luwian, and Lydian, among other more poorly attested relatives. The Turkification of Anatolia began under the Seljuk Empire in the late 11th century and continued under the Ottoman Empire between the late 13th and early 20th centuries. However, various non-Turkic languages continue to be spoken by minorities in Anatolia today, including Kurdish, Neo-Aramaic, Armenian, Arabic, Laz, Georgian and Greek. Other ancient peoples in the region included Galatians, Hurrians, Assyrians, Hattians, Cimmerians, as well as Ionian, Dorian and Aeolian Greeks.

Traditionally, Anatolia is considered to extend in the east to an indefinite line running from the Gulf of Alexandretta to the Black Sea, coterminous with the Anatolian Plateau. This traditional geographical definition is used, for example, in the latest edition of "Merriam-Webster's Geographical Dictionary". Under this definition, Anatolia is bounded to the east by the Armenian Highlands, and the Euphrates before that river bends to the southeast to enter Mesopotamia. To the southeast, it is bounded by the ranges that separate it from the Orontes valley in Syria (region) and the Mesopotamian plain.

Following the Armenian genocide, Ottoman Armenia was renamed "Eastern Anatolia" by the newly established Turkish government. Vazken Davidian terms the expanded use of "Anatolia" to apply to territory formerly referred to as Armenia an "ahistorical imposition", and notes that a growing body of literature is uncomfortable with referring to the Ottoman East as "Eastern Anatolia". Most archeological sources consider the boundary of Anatolia to be Turkey's eastern border.

The highest mountains in "Eastern Anatolia" (in Armenian Plateau) are Mount Süphan (4058 m) and Mount Ararat (5123 m). The Euphrates, Araxes, Karasu and Murat rivers connect the Armenian plateau to the South Caucasus and the Upper Euphrates Valley. Along with the Çoruh, these rivers are the longest in "Eastern Anatolia".

The English-language name "Anatolia" derives from the Greek ("") meaning "the East" or more literally "sunrise" (comparable to the Latin-derived terms "levant" and "orient"). The precise reference of this term has varied over time, perhaps originally referring to the Aeolian, Ionian and Dorian colonies on the west coast of Asia Minor. In the Byzantine Empire, the Anatolic Theme (Ἀνατολικόν θέμα "the Eastern theme") was a "theme" covering the western and central parts of Turkey's present-day Central Anatolia Region, centered around Iconium, but ruled from the city of Amorium. 

The term "Anatolia", with its "-ia" ending, is probably a Medieval Latin innovation. The modern Turkish form "Anadolu" derives directly from the Greek name Aνατολή ("Anatolḗ"). The Russian male name Anatoly, the French Anatole and plain Anatol, all stemming from saints Anatolius of Laodicea (d. 283) and Anatolius of Constantinople (d. 458; the first Patriarch of Constantinople), share the same linguistic origin.

The oldest known reference to Anatolia – as "Land of the Hatti" – appears on Mesopotamian cuneiform tablets from the period of the Akkadian Empire (2350–2150 BC). The first recorded name the Greeks used for the Anatolian peninsula, though not particularly popular at the time, was Ἀσία ("Asía"), perhaps from an Akkadian expression for the "sunrise", or possibly echoing the name of the Assuwa league in western Anatolia. The Romans used it as the name of their province, comprising the ancient landscapes to the west of the peninsula plus the adhering islands of the Aegean. As the name "Asia" broadened its scope to apply to the vaster region east of the Mediterranean, some Greeks in Late Antiquity came to use the name Μικρὰ Ἀσία ("Mikrà Asía") or Asia Minor, meaning "Lesser Asia" to refer to present-day Anatolia, whereas the administration of the Empire preferred the description as Ἀνατολή ("Anatolḗ" "the East").

The endonym Ῥωμανία ("Rhōmanía" "the land of the Romans, i. e. the Eastern Roman Empire") was understood as another name for the province by the invading Seljuq Turks, who founded a Sultanate of Rûm in 1077. Thus "(land of the) Rûm" became another name for Anatolia. By the 12th century Europeans had started referring to Anatolia as "Turchia".

During the era of the Ottoman Empire mapmakers outside the Empire referred to the mountainous plateau in eastern Anatolia as Armenia. Other contemporary sources called the same area Kurdistan. Geographers have variously used the terms east Anatolian plateau and Armenian plateau to refer to the region, although the territory encompassed by each term largely overlaps with the other. According to archaeologist Lori Khatchadourian this difference in terminology "primarily result[s] from the shifting political fortunes and cultural trajectories of the region since the nineteenth century."

Turkey's First Geography Congress in 1941 created two regions to the east of the Gulf of Iskenderun-Black Sea line named the Eastern Anatolia Region and the Southeastern Anatolia Region, the former largely corresponding to the western part of the Armenian Highland, the latter to the northern part of the Mesopotamian plain. According to Richard Hovannisian this changing of toponyms was "necessary to obscure all evidence" of Armenian presence as part of a campaign of genocide denial embarked upon by the newly established Turkish government and what Hovannisian calls its "foreign collaborators".

Human habitation in Anatolia dates back to the Paleolithic. Neolithic Anatolia has been proposed as the homeland of the Indo-European language family, although linguists tend to favour a later origin in the steppes north of the Black Sea. However, it is clear that the Anatolian languages, the earliest attested branch of Indo-European, have been spoken in Anatolia since at least the 19th century BC.

The earliest historical records of Anatolia stem from the southeast of the region and are from the Mesopotamian-based Akkadian Empire during the reign of Sargon of Akkad in the 24th century BC. Scholars generally believe the earliest indigenous populations of Anatolia were the Hattians and Hurrians. The Hattians spoke a language of unclear affiliation, and the Hurrian language belongs to a small family called Hurro-Urartian, all these languages now being extinct; relationships with indigenous languages of the Caucasus have been proposed but are not generally accepted. The region was famous for exporting raw materials, and areas of Hattian- and Hurrian-populated southeast Anatolia were colonised by the Akkadians.

After the fall of the Akkadian Empire in the mid-21st century BC, the Assyrians, who were the northern branch of the Akkadian people, colonised parts of the region between the 21st and mid-18th centuries BC and claimed its resources, notably silver. One of the numerous cuneiform records dated circa 20th century BC, found in Anatolia at the Assyrian colony of Kanesh, uses an advanced system of trading computations and credit lines.

Unlike the Akkadians and their descendants, the Assyrians, whose Anatolian possessions were peripheral to their core lands in Mesopotamia, the Hittites were centred at Hattusa (modern Boğazkale) in north-central Anatolia by the 17th century BC. They were speakers of an Indo-European language, the Hittite language, or "nesili" (the language of Nesa) in Hittite. The Hittites originated of local ancient cultures that grew in Anatolia, in addition to the arrival of Indo-European languages. Attested for the first time in the Assyrian tablets of Nesa around 2000BC, they conquered Hattusa in the 18th century BC, imposing themselves over Hattian- and Hurrian-speaking populations. According to the widely accepted Kurgan theory on the Proto-Indo-European homeland, however, the Hittites (along with the other Indo-European ancient Anatolians) were themselves relatively recent immigrants to Anatolia from the north. However, they did not necessarily displace the population genetically, they would rather assimilate into the former peoples' culture, preserving the Hittite language.
The Hittites adopted the cuneiform script, invented in Mesopotamia. During the Late Bronze Age circa 1650 BC, they created a kingdom, the Hittite New Kingdom, which became an empire in the 14th century BC after the conquest of Kizzuwatna in the south-east and the defeat of the Assuwa league in western Anatolia. The empire reached its height in the 13th century BC, controlling much of Asia Minor, northwestern Syria and northwest upper Mesopotamia. They failed to reach the Anatolian coasts of the Black Sea, however, as a non-Indo-European people, the semi-nomadic pastoralist and tribal Kaskians, had established themselves there, displacing earlier Palaic-speaking Indo-Europeans. Much of the history of the Hittite Empire concerned war with the rival empires of Egypt, Assyria and the Mitanni.

The Egyptians eventually withdrew from the region after failing to gain the upper hand over the Hittites and becoming wary of the power of Assyria, which had destroyed the Mitanni Empire. The Assyrians and Hittites were then left to battle over control of eastern and southern Anatolia and colonial territories in Syria. The Assyrians had better success than the Egyptians, annexing much Hittite (and Hurrian) territory in these regions.

After 1180 BC, during the Late Bronze Age collapse, the Hittite empire disintegrated into several independent Syro-Hittite states, subsequent to losing much territory to the Middle Assyrian Empire and being finally overrun by the Phrygians, another Indo-European people who are believed to have migrated from the Balkans. The Phrygian expansion into southeast Anatolia was eventually halted by the Assyrians, who controlled that region.

Arameans encroached over the borders of south central Anatolia in the century or so after the fall of the Hittite empire, and some of the Syro-Hittite states in this region became an amalgam of Hittites and Arameans. These became known as Syro-Hittite states.

Another Indo-European people, the Luwians, rose to prominence in central and western Anatolia circa 2000 BC. Their language belonged to the same linguistic branch as Hittite. The general consensus amongst scholars is that Luwian was spoken across a large area of western Anatolia, including (possibly) Wilusa (Troy), the Seha River Land (to be identified with the Hermos and/or Kaikos valley), and the kingdom of Mira-Kuwaliya with its core territory of the Maeander valley. From the 9th century BC, Luwian regions coalesced into a number of states such as Lydia, Caria and Lycia, all of which had Hellenic influence.

From the 10th to late 7th centuries BC, much of Anatolia (particularly the southeastern regions) fell to the Neo-Assyrian Empire, including all of the Syro-Hittite states, Tabal, Kingdom of Commagene, the Cimmerians and Scythians and swathes of Cappadocia.

The Neo-Assyrian empire collapsed due to a bitter series of civil wars followed by a combined attack by Medes, Persians, Scythians and their own Babylonian relations. The last Assyrian city to fall was Harran in southeast Anatolia. This city was the birthplace of the last king of Babylon, the Assyrian Nabonidus and his son and regent Belshazzar. Much of the region then fell to the short-lived Iran-based Median Empire, with the Babylonians and Scythians briefly appropriating some territory.

From the late 8th century BC, a new wave of Indo-European-speaking raiders entered northern and northeast Anatolia: the Cimmerians and Scythians. The Cimmerians overran Phrygia and the Scythians threatened to do the same to Urartu and Lydia, before both were finally checked by the Assyrians.

The north-western coast of Anatolia was inhabited by Greeks of the Achaean/Mycenaean culture from the 20th century BC, related to the Greeks of south eastern Europe and the Aegean. Beginning with the Bronze Age collapse at the end of the 2nd millennium BC, the west coast of Anatolia was settled by Ionian Greeks, usurping the area of the related but earlier Mycenaean Greeks. Over several centuries, numerous Ancient Greek city-states were established on the coasts of Anatolia. Greeks started Western philosophy on the western coast of Anatolia (Pre-Socratic philosophy).

In classical antiquity, Anatolia was described by Herodotus and later historians as divided into regions that were diverse in culture, language and religious practices. The northern regions included Bithynia, Paphlagonia and Pontus; to the west were Mysia, Lydia and Caria; and Lycia, Pamphylia and Cilicia belonged to the southern shore. There were also several inland regions: Phrygia, Cappadocia, Pisidia and Galatia. Languages spoken included the late surviving Anatolic languages Isaurian and Pisidian, Greek in Western and coastal regions, Phrygian spoken until the 7th century CE, local variants of Thracian in the Northwest, the Galatian variant of Gaulish in Galatia until the 6th century CE, Cappadocian and Armenian in the East, and Kartvelian languages in the Northeast.

Anatolia is known as the birthplace of minted coinage (as opposed to unminted coinage, which first appears in Mesopotamia at a much earlier date) as a medium of exchange, some time in the 7th century BC in Lydia. The use of minted coins continued to flourish during the Greek and Roman eras.

During the 6th century BC, all of Anatolia was conquered by the Persian Achaemenid Empire, the Persians having usurped the Medes as the dominant dynasty in Iran. In 499 BC, the Ionian city-states on the west coast of Anatolia rebelled against Persian rule. The Ionian Revolt, as it became known, though quelled, initiated the Greco-Persian Wars, which ended in a Greek victory in 449 BC, and the Ionian cities regained their independence. By the Peace of Antalcidas (387 BC), which ended the Corinthian War, Persia regained control over Ionia.

In 334 BC, the Macedonian Greek king Alexander the Great conquered the peninsula from the Achaemenid Persian Empire. Alexander's conquest opened up the interior of Asia Minor to Greek settlement and influence.
Following the death of Alexander and the breakup of his empire, Anatolia was ruled by a series of Hellenistic kingdoms, such as the Attalids of Pergamum and the Seleucids, the latter controlling most of Anatolia. A period of peaceful Hellenization followed, such that the local Anatolian languages had been supplanted by Greek by the 1st century BC. In 133 BC the last Attalid king bequeathed his kingdom to the Roman Republic, and western and central Anatolia came under Roman control, but Hellenistic culture remained predominant. Further annexations by Rome, in particular of the Kingdom of Pontus by Pompey, brought all of Anatolia under Roman control, except for the eastern frontier with the Parthian Empire, which remained unstable for centuries, causing a series of wars, culminating in the Roman-Parthian Wars.

After the division of the Roman Empire, Anatolia became part of the East Roman, or Byzantine Empire. Anatolia was one of the first places where Christianity spread, so that by the 4th century AD, western and central Anatolia were overwhelmingly Christian and Greek-speaking. For the next 600 years, while Imperial possessions in Europe were subjected to barbarian invasions, Anatolia would be the center of the Hellenic world.

It was one of the wealthiest and most densely populated places in the Late Roman Empire. Anatolia's wealth grew during the 4th and 5th centuries thanks, in part, to the Pilgrim's Road that ran through the peninsula. Literary evidence about the rural landscape has come down to us from the hagiographies of 6th century Nicholas of Sion and 7th century Theodore of Sykeon. Large urban centers included Ephesus, Pergamum, Sardis and Aphrodisias. Scholars continue to debate the cause of urban decline in the 6th and 7th centuries variously attributing it to the Plague of Justinian (541), and the 7th century Persian incursion and Arab conquest of the Levant.

In the ninth and tenth century a resurgent Byzantine Empire regained its lost territories, including even long lost territory such as Armenia and Syria (ancient Aram).

In the 10 years following the Battle of Manzikert in 1071, the Seljuk Turks from Central Asia migrated over large areas of Anatolia, with particular concentrations around the northwestern rim. The Turkish language and the Islamic religion were gradually introduced as a result of the Seljuk conquest, and this period marks the start of Anatolia's slow transition from predominantly Christian and Greek-speaking, to predominantly Muslim and Turkish-speaking (although ethnic groups such as Armenians, Greeks, and Assyrians remained numerous and retained Christianity and their native languages). In the following century, the Byzantines managed to reassert their control in western and northern Anatolia. Control of Anatolia was then split between the Byzantine Empire and the Seljuk Sultanate of Rûm, with the Byzantine holdings gradually being reduced.

In 1255, the Mongols swept through eastern and central Anatolia, and would remain until 1335. The Ilkhanate garrison was stationed near Ankara. After the decline of the Ilkhanate from 1335–1353, the Mongol Empire's legacy in the region was the Uyghur Eretna Dynasty that was overthrown by Kadi Burhan al-Din in 1381.

By the end of the 14th century, most of Anatolia was controlled by various Anatolian beyliks. Smyrna fell in 1330, and the last Byzantine stronghold in Anatolia, Philadelphia, fell in 1390. The Turkmen Beyliks were under the control of the Mongols, at least nominally, through declining Seljuk sultans. The Beyliks did not mint coins in the names of their own leaders while they remained under the suzerainty of the Mongol Ilkhanids. The Osmanli ruler Osman I was the first Turkish ruler who minted coins in his own name in 1320s, for it bears the legend "Minted by Osman son of Ertugrul". Since the minting of coins was a prerogative accorded in Islamic practice only to a sovereign, it can be considered that the Osmanli, or Ottoman Turks, became formally independent from the Mongol Khans.

Among the Turkish leaders, the Ottomans emerged as great power under Osman I and his son Orhan I. The Anatolian beyliks were successively absorbed into the rising Ottoman Empire during the 15th century. It is not well understood how the Osmanlı, or Ottoman Turks, came to dominate their neighbours, as the history of medieval Anatolia is still little known. The Ottomans completed the conquest of the peninsula in 1517 with the taking of Halicarnassus (modern Bodrum) from the Knights of Saint John.

With the acceleration of the decline of the Ottoman Empire in the early 19th century, and as a result of the expansionist policies of the Russian Empire in the Caucasus, many Muslim nations and groups in that region, mainly Circassians, Tatars, Azeris, Lezgis, Chechens and several Turkic groups left their homelands and settled in Anatolia. As the Ottoman Empire further shrank in the Balkan regions and then fragmented during the Balkan Wars, much of the non-Christian populations of its former possessions, mainly Balkan Muslims (Bosnian Muslims, Albanians, Turks, Muslim Bulgarians and Greek Muslims such as the Vallahades from Greek Macedonia), were resettled in various parts of Anatolia, mostly in formerly Christian villages throughout Anatolia.

A continuous reverse migration occurred since the early 19th century, when Greeks from Anatolia, Constantinople and Pontus area migrated toward the newly independent Kingdom of Greece, and also towards the United States, southern part of the Russian Empire, Latin America and rest of Europe.

Following the Russo-Persian Treaty of Turkmenchay (1828) and the incorporation of the Eastern Armenia into the Russian Empire, another migration involved the large Armenian population of Anatolia, which recorded significant migration rates from Western Armenia (Eastern Anatolia) toward the Russian Empire, especially toward its newly established Armenian provinces.

Anatolia remained multi-ethnic until the early 20th century (see the rise of nationalism under the Ottoman Empire). During World War I, the Armenian Genocide, the Greek genocide (especially in Pontus), and the Assyrian genocide almost entirely removed the ancient indigenous communities of Armenian, Greek, and Assyrian populations in Anatolia and surrounding regions. Following the Greco-Turkish War of 1919–1922, most remaining ethnic Anatolian Greeks were forced out during the 1923 population exchange between Greece and Turkey. Many more have left Turkey since, leaving fewer than 5,000 Greeks in Anatolia today. Since the foundation of the Republic of Turkey in 1923, Anatolia has been within Turkey, its inhabitants being mainly Turks and Kurds (see demographics of Turkey and history of Turkey).

Anatolia's terrain is structurally complex. A central massif composed of uplifted blocks and downfolded troughs, covered by recent deposits and giving the appearance of a plateau with rough terrain, is wedged between two folded mountain ranges that converge in the east. True lowland is confined to a few narrow coastal strips along the Aegean, Mediterranean, and Black Sea coasts. Flat or gently sloping land is rare and largely confined to the deltas of the Kızıl River, the coastal plains of Çukurova and the valley floors of the Gediz River and the Büyük Menderes River as well as some interior high plains in Anatolia, mainly around Lake Tuz (Salt Lake) and the Konya Basin ("Konya Ovasi").

There are two mountain ranges in southern Anatolia: the Taurus and the Zagros mountains.

Anatolia has a varied range of climates. The central plateau is characterized by a continental climate, with hot summers and cold snowy winters. The south and west coasts enjoy a typical Mediterranean climate, with mild rainy winters, and warm dry summers. The Black Sea and Marmara coasts have a temperate oceanic climate, with cool foggy summers and much rainfall throughout the year.

There is a diverse number of plant and animal communities.

The mountains and coastal plain of northern Anatolia experiences humid and mild climate. There are temperate broadleaf, mixed and coniferous forests. The central and eastern plateau, with its drier continental climate, has deciduous forests and forest steppes. Western and southern Anatolia, which have a Mediterranean climate, contain Mediterranean forests, woodlands, and scrub ecoregions.

Almost 80% of the people currently residing in Anatolia are Turks. Kurds constitute a major community in southeastern Anatolia, and are the largest ethnic minority. Abkhazians, Albanians, Arabs, Arameans, Armenians, Assyrians, Azerbaijanis, Bosnian Muslims, Circassians, Gagauz, Georgians, Serbs, Greeks, Hemshin, Jews, Laz, Levantines, Pomaks, Zazas and a number of other ethnic groups also live in Anatolia in smaller numbers.

Bamia is a traditional Anatolian-era stew dish prepared using lamb, okra and tomatoes as primary ingredients.



</doc>
<doc id="856" url="https://en.wikipedia.org/wiki?curid=856" title="Apple Inc.">
Apple Inc.

Apple Inc. is an American multinational technology company headquartered in Cupertino, California, that designs, develops, and sells consumer electronics, computer software, and online services. It is considered one of the Big Four technology companies, along with Amazon, Google, and Facebook.

The company's hardware products include the iPhone smartphone, the iPad tablet computer, the Mac personal computer, the iPod portable media player, the Apple Watch smartwatch, the Apple TV digital media player, the AirPods wireless earbuds and the HomePod smart speaker. Apple's software includes the macOS, iOS, iPadOS, watchOS, and tvOS operating systems, the iTunes media player, the Safari web browser, the Shazam acoustic fingerprint utility, and the iLife and iWork creativity and productivity suites, as well as professional applications like Final Cut Pro, Logic Pro, and Xcode. Its online services include the iTunes Store, the iOS App Store, Mac App Store, Apple Music, Apple TV+, iMessage, and iCloud. Other services include Apple Store, Genius Bar, AppleCare, Apple Pay, Apple Pay Cash, and Apple Card.

Apple was founded by Steve Jobs, Steve Wozniak, and Ronald Wayne in April 1976 to develop and sell Wozniak's Apple I personal computer, though Wayne sold his share back within 12 days. It was incorporated as Apple Computer, Inc., in January 1977, and sales of its computers, including the Apple II, grew quickly. Within a few years, Jobs and Wozniak had hired a staff of computer designers and had a production line. Apple went public in 1980 to instant financial success. Over the next few years, Apple shipped new computers featuring innovative graphical user interfaces, such as the original Macintosh in 1984, and Apple's marketing advertisements for its products received widespread critical acclaim. However, the high price of its products and limited application library caused problems, as did power struggles between executives. In 1985, Wozniak departed Apple amicably and remained an honorary employee, while Jobs and others resigned to found NeXT.

As the market for personal computers expanded and evolved through the 1990s, Apple lost market share to the lower-priced duopoly of Microsoft Windows on Intel PC clones. The board recruited CEO Gil Amelio to what would be a 500-day charge for him to rehabilitate the financially troubled company—reshaping it with layoffs, executive restructuring, and product focus. In 1997, he led Apple to buy NeXT, solving the desperately failed operating system strategy and bringing Jobs back. Jobs pensively regained leadership status, becoming CEO in 2000. Apple swiftly returned to profitability under the revitalizing Think different campaign, as he rebuilt Apple's status by launching the iMac in 1998, opening the retail chain of Apple Stores in 2001, and acquiring numerous companies to broaden the software portfolio. In January 2007, Jobs renamed the company Apple Inc., reflecting its shifted focus toward consumer electronics, and launched the iPhone to great critical acclaim and financial success. In August 2011, Jobs resigned as CEO due to health complications, and Tim Cook became the new CEO. Two months later, Jobs died, marking the end of an era for the company. In June 2019, Jony Ive, Apple's CDO, left the company to start his own firm, but stated he would work with Apple as its primary client.

Apple is well known for its size and revenues. Its worldwide annual revenue totaled $265 billion for the 2018 fiscal year. Apple is the world's largest technology company by revenue and one of the world's most valuable companies. It is also the world's third-largest mobile phone manufacturer after Samsung and Huawei. In August 2018, Apple became the first public U.S. company to be valued at over $1 trillion. The company employs 123,000 full-time employees and maintains 504 retail stores in 24 countries . It operates the iTunes Store, which is the world's largest music retailer. , more than 1.3 billion Apple products are actively in use worldwide. The company also has a high level of brand loyalty and is ranked as the world's most valuable brand. However, Apple receives significant criticism regarding the labor practices of its contractors, its environmental practices and unethical business practices, including anti-competitive behavior, as well as the origins of source materials.

Apple Computer Company was founded on April 1, 1976, by Steve Jobs, Steve Wozniak, and Ronald Wayne as a business partnership. The company's first product is the Apple I, a computer designed and hand-built entirely by Wozniak, and first shown to the public at the Homebrew Computer Club. Apple I was sold as a motherboard (with CPU, RAM, and basic textual-video chips)—a base kit concept which would now not be marketed as a complete personal computer. The Apple I went on sale in July 1976 and was market-priced at $666.66 ($ in dollars, adjusted for inflation). Wozniak later said he had no idea about the relation between the number and the mark of the beast, and that he came up with the price because he liked "repeating digits".

Apple Computer, Inc. was incorporated on January 3, 1977, without Wayne, who had left and sold his share of the company back to Jobs and Wozniak for $800 only twelve days after having co-founded Apple. Multimillionaire Mike Markkula provided essential business expertise and funding of $250,000 during the incorporation of Apple. During the first five years of operations, revenues grew exponentially, doubling about every four months. Between September 1977 and September 1980, yearly sales grew from $775,000 to $118 million, an average annual growth rate of 533%.

The Apple II, also invented by Wozniak, was introduced on April 16, 1977, at the first West Coast Computer Faire. It differs from its major rivals, the TRS-80 and Commodore PET, because of its character cell-based color graphics and open architecture. While early Apple II models use ordinary cassette tapes as storage devices, they were superseded by the introduction of a -inch floppy disk drive and interface called the Disk II in 1978. The Apple II was chosen to be the desktop platform for the first "killer application" of the business world: VisiCalc, a spreadsheet program released in 1979. VisiCalc created a business market for the Apple II and gave home users an additional reason to buy an Apple II: compatibility with the office. Before VisiCalc, Apple had been a distant third place competitor to Commodore and Tandy.

By the end of the 1970s, Apple had a staff of computer designers and a production line. The company introduced the Apple III in May 1980 in an attempt to compete with IBM in the business and corporate computing market. Jobs and several Apple employees, including human–computer interface expert Jef Raskin, visited Xerox PARC in December 1979 to see a demonstration of the Xerox Alto. Xerox granted Apple engineers three days of access to the PARC facilities in return for the option to buy 100,000 shares (5.6 million split-adjusted shares ) of Apple at the pre-IPO price of $10 a share.

Jobs was immediately convinced that all future computers would use a graphical user interface (GUI), and development of a GUI began for the Apple Lisa. In 1982, however, he was pushed from the Lisa team due to infighting. Jobs then took over Wozniak's and Raskin's low-cost-computer project, the Macintosh, and redefined it as a graphical system cheaper and faster than Lisa. In 1983, Lisa became the first personal computer sold to the public with a GUI, but was a commercial failure due to its high price and limited software titles, so in 1985 it would be repurposed as the high end Macintosh and discontinued in its second year.

On December 12, 1980, Apple (ticker symbol "AAPL") went public selling 4.6 million shares at $22 per share ($.39 per share when adjusting for stock splits ), generating over $100 million, which was more capital than any IPO since Ford Motor Company in 1956. By the end of the day, the stock rose to $29 per share and 300 millionaires were created. Apple's market cap was $1.778 billion at the end of its first day of trading.

In 1984, Apple launched the Macintosh, the first personal computer to be sold without a programming language. Its debut was signified by "1984", a $1.5 million television advertisement directed by Ridley Scott that aired during the third quarter of Super Bowl XVIII on January 22, 1984. This is now hailed as a watershed event for Apple's success and was called a "masterpiece" by CNN and one of the greatest TV advertisements of all time by "TV Guide".

Macintosh sales were initially good, but began to taper off dramatically after the first three months due to its high price, slow speed, and limited range of available software. In early 1985, this sales slump triggered a power struggle between Steve Jobs and CEO John Sculley, who had been hired two years earlier by Jobs using the famous line, "Do you want to sell sugar water for the rest of your life or come with me and change the world?" Sculley decided to remove Jobs as the general manager of the Macintosh division, and gained unanimous support from the Apple board of directors.

The board of directors instructed Sculley to contain Jobs and his ability to launch expensive forays into untested products. Rather than submit to Sculley's direction, Jobs attempted to oust him from his leadership role at Apple. Informed by Jean-Louis Gassée, Sculley found out that Jobs had been attempting to organize a coup and called an emergency executive meeting at which Apple's executive staff sided with Sculley and stripped Jobs of all operational duties. Jobs resigned from Apple in September 1985 and took a number of Apple employees with him to found NeXT Inc. Wozniak had also quit his active employment at Apple earlier in 1985 to pursue other ventures, expressing his frustration with Apple's treatment of the Apple II division and stating that the company had "been going in the wrong direction for the last five years". Despite Wozniak's grievances, he left the company amicably and both Jobs and Wozniak remained Apple shareholders. Wozniak continues to represent the company at events or in interviews, receiving a stipend estimated to be $120,000 per year for this role.

The outlook on Macintosh improved with the introduction of the LaserWriter, the first reasonably priced PostScript laser printer, and PageMaker, an early desktop publishing application released in July 1985. It has been suggested that the combination of Macintosh, LaserWriter, and PageMaker was responsible for the creation of the desktop publishing market.
After the departures of Jobs and Wozniak, the Macintosh product line underwent a steady change of focus to higher price points, the so-called "high-right policy" named for the position on a chart of price vs. profits. Jobs had argued the company should produce products aimed at the consumer market and aimed for a $1,000 price for the Macintosh, which they were unable to meet. Newer models selling at higher price points offered higher profit margin, and appeared to have no effect on total sales as power users snapped up every increase in power. Although some worried about pricing themselves out of the market, the high-right policy was in full force by the mid-1980s, notably due to Jean-Louis Gassée's mantra of "fifty-five or die", referring to the 55% profit margins of the Macintosh II. Selling Macintosh at such high profit margins was only possible because of its dominant position in the desktop publishing market.

This policy began to backfire in the last years of the decade as new desktop publishing programs appeared on PC clones that offered some or much of the same functionality of the Macintosh but at far lower price points. The company lost its monopoly in this market and had already estranged many of its original consumer customer base who could no longer afford their high-priced products. The Christmas season of 1989 is the first in the company's history to have declining sales, which led to a 20% drop in Apple's stock price. During this period, the relationship between Sculley and Gassée deteriorated, leading Sculley to effectively demote Gassée in January 1990 by appointing Michael Spindler as the chief operating officer. Gassée left the company later that year. In October 1990, Apple introduced three lower-cost models, the Macintosh Classic, Macintosh LC, and Macintosh IIsi, all of which saw significant sales due to pent-up demand.

In 1991, Apple introduced the PowerBook, replacing the "luggable" Macintosh Portable with a design that set the current shape for almost all modern laptops. The same year, Apple introduced System 7, a major upgrade to the operating system which added color to the interface and introduced new networking capabilities. It remained the architectural basis for the Classic Mac OS. The success of the PowerBook and other products brought increasing revenue. For some time, Apple was doing incredibly well, introducing fresh new products and generating increasing profits in the process. The magazine "MacAddict" named the period between 1989 and 1991 as the "first golden age" of the Macintosh.

Apple believed the Apple II series was too expensive to produce and took away sales from the low-end Macintosh. In October 1990, Apple released the Macintosh LC, and began efforts to promote that computer by advising developer technical support staff to recommend developing applications for Macintosh rather than Apple II, and authorizing salespersons to direct consumers towards Macintosh and away from Apple II. The Apple IIe was discontinued in 1993.

The success of Apple's lower-cost consumer models, especially the LC, also led to the cannibalization of their higher-priced machines. To address this, management introduced several new brands, selling largely identical machines at different price points aimed at different markets. These were the high-end Quadra, the mid-range Centris line, and the consumer-marketed Performa series. This led to significant market confusion, as customers did not understand the difference between models.

Apple also experimented with a number of other unsuccessful consumer targeted products during the 1990s, including digital cameras, portable CD audio players, speakers, video consoles, the eWorld online service, and TV appliances. Enormous resources were also invested in the problem-plagued Newton division based on John Sculley's unrealistic market forecasts. Ultimately, none of these products helped and Apple's market share and stock prices continued to slide.

Throughout this period, Microsoft continued to gain market share with Windows by focusing on delivering software to cheap commodity personal computers, while Apple was delivering a richly engineered but expensive experience. Apple relied on high profit margins and never developed a clear response; instead, they sued Microsoft for using a GUI similar to the Apple Lisa in "Apple Computer, Inc. v. Microsoft Corp." The lawsuit dragged on for years before it was finally dismissed. At this time, a series of major product flops and missed deadlines sullied Apple's reputation, and Sculley was replaced as CEO by Michael Spindler.

By the late 1980s, Apple was developing alternative platforms to System 6, such as A/UX and Pink. The System 6 platform itself was outdated because it was not originally built for multitasking. By the 1990s, Apple was facing competition from OS/2 and UNIX vendors such as Sun Microsystems. System 6 and 7 would need to be replaced by a new platform or reworked to run on modern hardware.

In 1994, Apple, IBM, and Motorola formed the AIM alliance with the goal of creating a new computing platform (the PowerPC Reference Platform), which would use IBM and Motorola hardware coupled with Apple software. The AIM alliance hoped that PReP's performance and Apple's software would leave the PC far behind and thus counter Microsoft's monopoly. The same year, Apple introduced the Power Macintosh, the first of many Apple computers to use Motorola's PowerPC processor.

In 1996, Spindler was replaced by Gil Amelio as CEO. Hired for his reputation as a corporate rehabilitator, Amelio made deep changes, including extensive layoffs and cost-cutting. After numerous failed attempts to modernize Mac OS, first with the Pink project from 1988 and later with Copland from 1994, Apple in 1997 purchased NeXT for its NeXTSTEP operating system and to bring Steve Jobs back. Apple was only weeks away from bankruptcy when Jobs returned.

The NeXT acquisition was finalized on February 9, 1997, bringing Jobs back to Apple as an advisor. On July 9, 1997, Amelio was ousted by the board of directors after overseeing a three-year record-low stock price and crippling financial losses. Jobs acted as the interim CEO and began restructuring the company's product line; it was during this period that he identified the design talent of Jonathan Ive, and the pair worked collaboratively to rebuild Apple's status.

At the August 1997 Macworld Expo in Boston, Jobs announced that Apple would join Microsoft to release new versions of Microsoft Office for the Macintosh, and that Microsoft had made a $150 million investment in non-voting Apple stock. On November 10, 1997, Apple introduced the Apple Store website, which was tied to a new build-to-order manufacturing strategy.

On August 15, 1998, Apple introduced a new all-in-one computer reminiscent of the Macintosh 128K: the iMac. The iMac design team was led by Ive, who would later design the iPod and the iPhone. The iMac featured modern technology and a unique design, and sold almost 800,000 units in its first five months.

During this period, Apple completed numerous acquisitions to create a portfolio of digital production software for both professionals and consumers. In 1998, Apple purchased Macromedia's Key Grip software project, signaling an expansion into the digital video editing market. The sale was an outcome of Macromedia's decision to solely focus on web development software. The product, still unfinished at the time of the sale, was renamed "Final Cut Pro" when it was launched on the retail market in April 1999. The development of Key Grip also led to Apple's release of the consumer video-editing product iMovie in October 1999. Next, Apple successfully acquired the German company Astarte, which had developed DVD authoring technology, as well as Astarte's corresponding products and engineering team in April 2000. Astarte's digital tool DVDirector was subsequently transformed into the professional-oriented DVD Studio Pro software product. Apple then employed the same technology to create iDVD for the consumer market. In July 2001, Apple acquired Spruce Technologies, a PC DVD authoring platform, to incorporate their technology into Apple's expanding portfolio of digital video projects.

SoundJam MP, released by Casady & Greene in 1998, was renamed "iTunes" when Apple purchased it in 2000. The primary developers of the MP3 player and music library software moved to Apple as part of the acquisition, and simplified SoundJam's user interface, added the ability to burn CDs, and removed its recording feature and skin support. SoundJam was Apple's second choice for the core of Apple's music software project, originally codenamed iMusic, behind Panic's Audion. Apple was not able to set up a meeting with Panic in time to be fully considered as the latter was in the middle of similar negotiations with AOL.

In 2002, Apple purchased Nothing Real for their advanced digital compositing application Shake, as well as Emagic for the music productivity application Logic. The purchase of Emagic made Apple the first computer manufacturer to own a music software company. The acquisition was followed by the development of Apple's consumer-level GarageBand application. The release of iPhoto in the same year completed the iLife suite.

Mac OS X, based on NeXT's NeXTSTEP, OPENSTEPand BSD Unix, was released on March 24, 2001, after several years of development. Aimed at consumers and professionals alike, Mac OS X aimed to combine the stability, reliability, and security of Unix with the ease of use afforded by an overhauled user interface. To aid users in migrating from Mac OS 9, the new operating system allowed the use of OS 9 applications within Mac OS X via the Classic Environment.

On May 19, 2001, Apple opened its first official eponymous retail stores in Virginia and California. On October 23 of the same year, Apple debuted the iPod portable digital audio player. The product, which was first sold on November 10, 2001, was phenomenally successful with over 100 million units sold within six years. In 2003, Apple's iTunes Store was introduced. The service offered online music downloads for $0.99 a song and integration with the iPod. The iTunes Store quickly became the market leader in online music services, with over five billion downloads by June 19, 2008. Two years later, the iTunes Store was the world's largest music retailer.

At the Worldwide Developers Conference keynote address on June 6, 2005, Jobs announced that Apple would begin producing Intel-based Mac computers in 2006. On January 10, 2006, the new MacBook Pro and iMac became the first Apple computers to use Intel's Core Duo CPU. By August 7, 2006, Apple made the transition to Intel chips for the entire Mac product line—over one year sooner than announced. The Power Mac, iBook, and PowerBook brands were retired during the transition; the Mac Pro, MacBook, and MacBook Pro became their respective successors. On April 29, 2009, "The Wall Street Journal" reported that Apple was building its own team of engineers to design microchips. Apple also introduced Boot Camp in 2006 to help users install Windows XP or Windows Vista on their Intel Macs alongside Mac OS X.

Apple's success during this period was evident in its stock price. Between early 2003 and 2006, the price of Apple's stock increased more than tenfold, from around $6 per share (split-adjusted) to over $80. When Apple surpassed Dell's market cap in January 2006, Jobs sent an email to Apple employees saying Dell's CEO Michael Dell should eat his words. Nine years prior, Dell had said that if he ran Apple he would "shut it down and give the money back to the shareholders".

Although Apple's market share in computers had grown, it remained far behind its competitor Microsoft Windows, accounting for about 8% of desktops and laptops in the US.

Since 2001, Apple's design team has progressively abandoned the use of translucent colored plastics first used in the iMac G3. This design change began with the titanium-made PowerBook and was followed by the iBook's white polycarbonate structure and the flat-panel iMac.

During his keynote speech at the Macworld Expo on January 9, 2007, Jobs announced that Apple Computer, Inc. would thereafter be known as "Apple Inc.", because the company had shifted its emphasis from computers to consumer electronics. This event also saw the announcement of the iPhone and the Apple TV. The company sold 270,000 iPhone units during the first 30 hours of sales, and the device was called "a game changer for the industry". Apple would achieve widespread success with its iPhone, iPod Touch, and iPad products, which introduced innovations in mobile phones, portable music players, and personal computers respectively. Furthermore, by early 2007, 800,000 Final Cut Pro users were registered.

In an article posted on Apple's website on February 6, 2007, Jobs wrote that Apple would be willing to sell music on the iTunes Store without digital rights management (DRM), thereby allowing tracks to be played on third-party players, if record labels would agree to drop the technology. On April 2, 2007, Apple and EMI jointly announced the removal of DRM technology from EMI's catalog in the iTunes Store, effective in May 2007. Other record labels eventually followed suit and Apple published a press release in January 2009 to announce that all songs on the iTunes Store are available without their FairPlay DRM.

In July 2008, Apple launched the App Store to sell third-party applications for the iPhone and iPod Touch. Within a month, the store sold 60 million applications and registered an average daily revenue of $1 million, with Jobs speculating in August 2008 that the App Store could become a billion-dollar business for Apple. By October 2008, Apple was the third-largest mobile handset supplier in the world due to the popularity of the iPhone.

On December 16, 2008, Apple announced that 2009 would be the last year the corporation would attend the Macworld Expo, after more than 20 years of attendance, and that senior vice president of Worldwide Product Marketing Phil Schiller would deliver the 2009 keynote address in lieu of the expected Jobs. The official press release explained that Apple was "scaling back" on trade shows in general, including Macworld Tokyo and the Apple Expo in Paris, France, primarily because the enormous successes of the Apple Retail Stores and website had rendered trade shows a minor promotional channel.

On January 14, 2009, Jobs announced in an internal memo that he would be taking a six-month medical leave of absence from Apple until the end of June 2009 and would spend the time focusing on his health. In the email, Jobs stated that "the curiosity over my personal health continues to be a distraction not only for me and my family, but everyone else at Apple as well", and explained that the break would allow the company "to focus on delivering extraordinary products". Though Jobs was absent, Apple recorded its best non-holiday quarter (Q1 FY 2009) during the recession with revenue of $8.16 billion and profit of $1.21 billion.

After years of speculation and multiple rumored "leaks", Apple unveiled a large screen, tablet-like media device known as the iPad on January 27, 2010. The iPad ran the same touch-based operating system as the iPhone, and all iPhone apps were compatible with the iPad. This gave the iPad a large app catalog on launch, though having very little development time before the release. Later that year on April 3, 2010, the iPad was launched in the US. It sold more than 300,000 units on its first day, and 500,000 by the end of the first week. In May of the same year, Apple's market cap exceeded that of competitor Microsoft for the first time since 1989.

In June 2010, Apple released the iPhone 4, which introduced video calling, multitasking, and a new uninsulated stainless steel design that acted as the phone's antenna. Later that year, Apple again refreshed its iPod line of MP3 players by introducing a multi-touch iPod Nano, an iPod Touch with FaceTime, and an iPod Shuffle that brought back the clickwheel buttons of earlier generations. It also introduced the smaller, cheaper second generation Apple TV which allowed renting of movies and shows.

In October 2010, Apple shares hit an all-time high, eclipsing $300 (~$43 split adjusted). Later that month, Apple updated the MacBook Air laptop, iLife suite of applications, and unveiled Mac OS X Lion, the last version with the name Mac OS X.

On January 6, 2011, the company opened its Mac App Store, a digital software distribution platform similar to the iOS App Store.

On January 17, 2011, Jobs announced in an internal Apple memo that he would take another medical leave of absence for an indefinite period to allow him to focus on his health. Chief Operating Officer Tim Cook assumed Jobs's day-to-day operations at Apple, although Jobs would still remain "involved in major strategic decisions". Apple became the most valuable consumer-facing brand in the world. In June 2011, Jobs surprisingly took the stage and unveiled iCloud, an online storage and syncing service for music, photos, files, and software which replaced MobileMe, Apple's previous attempt at content syncing. This would be the last product launch Jobs would attend before his death.

Alongside peer entities such as Atari and Cisco Systems, Apple was featured in the documentary "Something Ventured", which premiered in 2011 and explored the three-decade era that led to the establishment and dominance of Silicon Valley. It has been argued that Apple has achieved such efficiency in its supply chain that the company operates as a monopsony (one buyer with many sellers) and can dictate terms to its suppliers. In July 2011, due to the American debt-ceiling crisis, Apple's financial reserves were briefly larger than those of the U.S. Government.

On August 24, 2011, Jobs resigned his position as CEO of Apple. He was replaced by Cook and Jobs became Apple's chairman. Apple did not have a chairman at the time and instead had two co-lead directors, Andrea Jung and Arthur D. Levinson, who continued with those titles until Levinson replaced Jobs as chairman of the board in November after Jobs' death.

On October 5, 2011, Steve Jobs died, marking the end of an era for Apple. The first major product announcement by Apple following Jobs's passing occurred on January 19, 2012, when Apple's Phil Schiller introduced iBooks Textbooks for iOS and iBook Author for Mac OS X in New York City. Jobs had stated in his biography that he wanted to reinvent the textbook industry and education.

From 2011 to 2012, Apple released the iPhone 4S and iPhone 5, which featured improved cameras, an intelligent software assistant named Siri, and cloud-synced data with iCloud; the third and fourth generation iPads, which featured Retina displays; and the iPad Mini, which featured a 7.9-inch screen in contrast to the iPad's 9.7-inch screen. These launches were successful, with the iPhone 5 (released September 21, 2012) becoming Apple's biggest iPhone launch with over two million pre-orders and sales of three million iPads in three days following the launch of the iPad Mini and fourth generation iPad (released November 3, 2012). Apple also released a third-generation 13-inch MacBook Pro with a Retina display and new iMac and Mac Mini computers.

On August 20, 2012, Apple's rising stock price increased the company's market capitalization to a world-record $624 billion. This beat the non-inflation-adjusted record for market capitalization set by Microsoft in 1999. On August 24, 2012, a US jury ruled that Samsung should pay Apple $1.05 billion (£665m) in damages in an intellectual property lawsuit. Samsung appealed the damages award, which the Court reduced by $450 million. The Court further granted Samsung's request for a new trial. On November 10, 2012, Apple confirmed a global settlement that would dismiss all lawsuits between Apple and HTC up to that date, in favor of a ten-year license agreement for current and future patents between the two companies. It is predicted that Apple will make $280 million a year from this deal with HTC.

A previously confidential email written by Jobs a year before his death was presented during the proceedings of the "Apple Inc. v. Samsung Electronics Co." lawsuits and became publicly available in early April 2014. With a subject line that reads "Top 100 – A," the email was sent only to the company's 100 most senior employees and outlines Jobs's vision of Apple Inc.'s future under 10 subheadings. Notably, Jobs declares a "Holy War with Google" for 2011 and schedules a "new campus" for 2015.

In March 2013, Apple filed a patent for an augmented reality (AR) system that can identify objects in a live video stream and present information corresponding to these objects through a computer-generated information layer overlaid on top of the real-world image. The company also made several high-profile hiring decisions in 2013. On July 2, 2013, Apple recruited Paul Deneve, Belgian President and CEO of Yves Saint Laurent as a vice president reporting directly to Tim Cook. A mid-October 2013 announcement revealed that Burberry CEO Angela Ahrendts will commence as a senior vice president at Apple in mid-2014. Ahrendts oversaw Burberry's digital strategy for almost eight years and, during her tenure, sales increased to about $3.2 billion and shares gained more than threefold. She resigned from Apple in 2019.

Alongside Google vice-president Vint Cerf and AT&T CEO Randall Stephenson, Cook attended a closed-door summit held by President Obama on August 8, 2013, in regard to government surveillance and the Internet in the wake of the Edward Snowden NSA incident. On February 4, 2014, Cook met with Abdullah Gül, the President of Turkey, in Ankara to discuss the company's involvement in the Fatih project.

In the first quarter of 2014, Apple reported sales of 51 million iPhones and 26 million iPads, becoming all-time quarterly sales records. It also experienced a significant year-over-year increase in Mac sales. This was contrasted with a significant drop in iPod sales. In May 2014, the company confirmed its intent to acquire Dr. Dre and Jimmy Iovine's audio company Beats Electronics—producer of the "Beats by Dr. Dre" line of headphones and speaker products, and operator of the music streaming service Beats Music—for $3 billion, and to sell their products through Apple's retail outlets and resellers. Iovine believed that Beats had always "belonged" with Apple, as the company modeled itself after Apple's "unmatched ability to marry culture and technology." The acquisition was the largest purchase in Apple's history.

Apple was at the top of Interbrand's annual Best Global Brands report for six consecutive years; 2013, 2014, 2015, 2016, 2017, and 2018 with a valuation of $214.48 billion.

In January 2016, it was announced that one billion Apple devices were in active use worldwide.

On May 12, 2016, Apple Inc., invested $1 billion in DiDi, a Chinese transportation network company. "The Information" reported in October 2016 that Apple had taken a board seat in Didi Chuxing, a move that James Vincent of "The Verge" speculated could be a strategic company decision by Apple to get closer to the automobile industry, particularly Didi Chuxing's reported interest in self-driving cars.

On June 6, 2016, Fortune released Fortune 500, their list of companies ranked on revenue generation. In the trailing fiscal year (2015), Apple appeared on the list as the top tech company. It ranked third, overall, with $233 billion in revenue. This represents a movement upward of two spots from the previous year's list.

On April 6, 2017, Apple launched Clips, an app that allows iPad and iPhone users to make and edit short videos with text, graphics, and effects. The app provides a way to produce short videos to share with other users on the Messages app, Instagram, Facebook, and other social networks. Apple also introduced Live Titles for Clips that allows users to add live animated captions and titles using their voice.

In May 2017, Apple refreshed two of its website designs. Their public relations "Apple Press Info" website was changed to an "Apple Newsroom" site, featuring a greater emphasis on imagery and therefore lower information density, and combines press releases, news items, and photos. Its "Apple Leadership" overview of company executives was also refreshed, adding a simpler layout with a prominent header image and two-column text fields. "9to5Mac" noted the design similarities to several of Apple's redesigned apps in iOS 10, particularly its Apple Music and News software.

In June 2017, Apple announced the HomePod, its smart speaker aimed to compete against Sonos, Google Home, and Amazon Echo. Towards the end of the year, "TechCrunch" reported that Apple was acquiring Shazam, a company specializing in music, TV, film and advertising recognition. The acquisition was confirmed a few days later, reportedly costing Apple $400 million, with media reports noting that the purchase looked like a move by Apple to get data and tools to bolster its Apple Music streaming service. The purchase was approved by EU later in September 2018.

Also in June 2017, Apple appointed Jamie Erlicht and Zack Van Amburg to head the newly formed worldwide video unit. In November 2017, Apple announced it was branching out into original scripted programming: a drama series starring Jennifer Aniston and Reese Witherspoon, and a reboot of the anthology series Amazing Stories with Steven Spielberg. In June 2018, Apple signed the Writer's Guild of America's minimum basic agreement and Oprah Winfrey to a multi-year content partnership. Additional partnerships for original series include Sesame Workshop and DHX Media and its subsidiary Peanuts Worldwide, as well as a partnership with A24 to create original films. , Apple has ordered twenty-one television series and one film. There are five series in development at Apple.

On June 5, 2018, Apple deprecated OpenGL and OpenGL ES across all operating systems and urged developers to use Metal instead.

In August 2018, Apple purchased Akonia Holographics for its augmented reality goggle lens. On February 14, 2019, Apple acquired DataTiger for its digital marketing technology.

On January 29, 2019, Apple reported its first decline in revenues and profits in a decade.

In February 2019 they bought Conversational computing company PullString (formerly ToyTalk)

On July 25, 2019, Apple and Intel announced an agreement for Apple to acquire the smartphone modem business of Intel Mobile Communications for US$1 billion.

Macintoshes currently in production:
Apple sells a variety of computer accessories for Macs, including Thunderbolt Display, Magic Mouse, Magic Trackpad, Magic Keyboard, the AirPort wireless networking products, and Time Capsule.

On October 23, 2001, Apple introduced the iPod digital music player. Several updated models have since been introduced, and the iPod brand is now the market leader in portable music players by a significant margin. More than 390 million units have shipped . Apple has partnered with Nike to offer the Nike+iPod Sports Kit, enabling runners to synchronize and monitor their runs with iTunes and the Nike+ website.

In late July 2017, Apple discontinued its iPod Nano and iPod Shuffle models, leaving only the iPod Touch available for purchase.

At the Macworld Conference & Expo in January 2007, Steve Jobs introduced the long-anticipated iPhone, a convergence of an Internet-enabled smartphone and iPod. The first-generation iPhone was released on June 29, 2007, for $499 (4 GB) and $599 (8 GB) with an AT&T contract. On February 5, 2008, it was updated to have 16 GB of memory, in addition to the 8 GB and 4 GB models. It combined a 2.5G quad band GSM and EDGE cellular phone with features found in handheld devices, running a scaled-down version of OS X (dubbed iPhone OS after the launch and later renamed to iOS), with various Mac OS X applications such as Safari and Mail. It also includes web-based and Dashboard apps such as Google Maps and Weather. The iPhone features a touchscreen display, Bluetooth, and Wi-Fi (both "b" and "g").

A second version, the iPhone 3G, was released on July 11, 2008, with a reduced price of $199 for the 8 GB model and $299 for the 16 GB model. This version added support for 3G networking and assisted GPS navigation. The flat silver back and large antenna square of the original model were eliminated in favor of a glossy, curved black or white back. Software capabilities were improved with the release of the App Store, which provided iPhone-compatible applications to download. On April 24, 2009, the App Store surpassed one billion downloads. On June 8, 2009, Apple announced the iPhone 3GS. It provided an incremental update to the device, including faster internal components, support for faster 3G speeds, video recording capability, and voice control.

At the Worldwide Developers Conference (WWDC) on June 7, 2010, Apple announced the redesigned iPhone 4. It featured a 960 × 640 display, the Apple A4 processor, a gyroscope for enhanced gaming, a 5MP camera with LED flash, front-facing VGA camera and FaceTime video calling. Shortly after its release, reception issues were discovered by consumers, due to the stainless steel band around the edge of the device, which also serves as the phone's cellular signal and Wi-Fi antenna. The issue was corrected by a "Bumper Case" distributed by Apple for free to all owners for a few months. In June 2011, Apple overtook Nokia to become the world's biggest smartphone maker by volume. On October 4, 2011, Apple unveiled the iPhone 4S, which was first released on October 14, 2011. It features the Apple A5 processor and Siri voice assistant technology, the latter of which Apple had acquired in 2010 from SRI International Artificial Intelligence Center. It also features an updated 8MP camera with new optics. Apple began a new accessibility feature, Made for iPhone Hearing Aids with the iPhone 4S. Made for iPhone Hearing Aids feature Live Listen, it can help the user hear a conversation in a noisy room or hear someone speaking across the room. Apple sold 4 million iPhone 4S phones in the first three days of availability.

On September 12, 2012, Apple introduced the iPhone 5. It has a 4-inch display, 4G LTE connectivity, and the upgraded Apple A6 chip, among several other improvements. Two million iPhones were sold in the first twenty-four hours of pre-ordering and over five million handsets were sold in the first three days of its launch. Upon the launch of the iPhone 5S and iPhone 5C, Apple set a new record for first-weekend smartphone sales by selling over nine million devices in the first three days of its launch. The release of the iPhone 5S and 5C is the first time that Apple simultaneously launched two models.

A patent filed in July 2013 revealed the development of a new iPhone battery system that uses location data in combination with data on the user's habits to moderate the handsets' power settings accordingly. Apple is working towards a power management system that will provide features such as the ability of the iPhone to estimate the length of time a user will be away from a power source to modify energy usage and a detection function that adjusts the charging rate to best suit the type of power source that is being used.

In a March 2014 interview, Apple designer Jonathan Ive used the iPhone as an example of Apple's ethos of creating high-quality, life-changing products. He explained that the phones are comparatively expensive due to the intensive effort that is used to make them:

On September 9, 2014, Apple introduced the iPhone 6, alongside the iPhone 6 Plus that both have screen sizes over 4-inches. One year later, Apple introduced the iPhone 6S, and iPhone 6S Plus, which introduced a new technology called 3D Touch, including an increase of the rear camera to 12 MP, and the FaceTime camera to 5 MP. On March 21, 2016, Apple introduced the iPhone SE that has a 4-inch screen size last used with the 5S and has nearly the same internal hardware as the 6S.

In July 2016, Apple announced that one billion iPhones had been sold.

On September 7, 2016, Apple introduced the iPhone 7 and the iPhone 7 Plus, which feature improved system and graphics performance, add water resistance, a new rear dual-camera system on the 7 Plus model, and, controversially, remove the 3.5 mm headphone jack.
On September 12, 2017, Apple introduced the iPhone 8 and iPhone 8 Plus, standing as evolutionary updates to its previous phones with a faster processor, improved display technology, upgraded camera systems and wireless charging. The company also announced iPhone X, which radically changes the hardware of the iPhone lineup, removing the home button in favor of facial recognition technology and featuring a near bezel-less design along with wireless charging.

On September 12, 2018, Apple introduced the iPhone XS, iPhone XS Max and iPhone XR. The iPhone XS and iPhone XS Max features Super Retina displays, a faster and improved dual camera system that offers breakthrough photo and video features, the first 7-nanometer chip in a smartphone — the A12 Bionic chip with next-generation Neural Engine — faster Face ID, wider stereo sound and introduces Dual SIM to iPhone. The iPhone XR comes in an all-screen glass and aluminium design with the most advanced LCD in a smartphone featuring a 6.1-inch Liquid Retina display, A12 Bionic chip with next-generation Neural Engine, the TrueDepth camera system, Face ID and an advanced camera system that creates dramatic portraits using a single camera lens.

On September 10, 2019, Apple introduced the iPhone 11, iPhone 11 Pro, and the iPhone 11 Pro Max. The iPhone 11 features the same Liquid Retina LCD display used in 2018's iPhone XR. Overall, the iPhone 11 retains the same glass and aluminum design as the iPhone XR while adding in new features such as the addition of an Ultra-Wide 12mp camera, a battery that lasts 1 hour longer than the iPhone XR and an IP 68 rating. The iPhone 11 Pro and iPhone 11 Pro Max features an all-new textured matte glass and stainless steel design and a triple camera setup that included an Ultra Wide, Wide and Telephoto camera. The iPhone 11 Pro series' battery life was capable of lasting up to 5 hours more than the iPhone XS and XS Max. The iPhone 11 Pro and Pro Max also features a new Super Retina XDR OLED display that was capable of a screen brightness of 800 nits. All new iPhones announced at Apple's September 2019 feature an A13 Bionic chip with a third-generation Neural Engine, an Apple U1 chip, spatial audio playback, a low light photo mode and an improved Face ID system.

On January 27, 2010, Apple introduced their much-anticipated media tablet, the iPad. It offers multi-touch interaction with multimedia formats including newspapers, e-books, photos, videos, music, word processing documents, video games, and most existing iPhone apps using a 9.7-inch screen. It also includes a mobile version of Safari for web browsing, as well as access to the App Store, iTunes Library, iBookstore, Contacts, and Notes. Content is downloadable via Wi-Fi and optional 3G service or synced through the user's computer. AT&T was initially the sole U.S. provider of 3G wireless access for the iPad.

On March 2, 2011, Apple introduced the iPad 2, which had a faster processor and a camera on the front and back. It also added support for optional 3G service provided by Verizon in addition to AT&T. The availability of the iPad 2 was initially limited as a result of a devastating earthquake and tsunami in Japan in March 2011.

The third-generation iPad was released on March 7, 2012, and marketed as "the new iPad". It added LTE service from AT&T or Verizon, an upgraded A5X processor, and Retina display. The dimensions and form factor remained relatively unchanged, with the new iPad being a fraction thicker and heavier than the previous version and featuring minor positioning changes.

On October 23, 2012, Apple's fourth-generation iPad came out, marketed as the "iPad with Retina display". It added the upgraded A6X processor and replaced the traditional 30-pin dock connector with the all-digital Lightning connector. The iPad Mini was also introduced. It featured a reduced 7.9-inch display and much of the same internal specifications as the iPad 2.

On October 22, 2013, Apple introduced the iPad Air and the iPad Mini with Retina Display, both featuring a new 64-bit Apple A7 processor.

The iPad Air 2 was unveiled on October 16, 2014. It added better graphics and central processing and a camera burst mode as well as minor updates. The iPad Mini 3 was unveiled at the same time.

Since its launch, iPad users have downloaded over three billion apps. The total number of App Store downloads, , is over 100 billion.

On September 9, 2015, Apple announced the iPad Pro, an iPad with a 12.9-inch display that supports two new accessories, the Smart Keyboard and Apple Pencil. An updated IPad Mini 4 was announced at the same time. A 9.7-inch iPad Pro was announced on March 21, 2016. On June 5, 2017, Apple announced a new iPad Pro with a 10.5-inch display to replace the 9.7 inch model and an updated 12.9-inch model.

The original Apple Watch smartwatch was announced by Tim Cook on September 9, 2014, being introduced as a product with health and fitness-tracking. It was released on April 24, 2015.

The second generation of Apple Watch, Apple Watch Series 2, was released in September 2016, featuring greater water resistance, a faster processor, and brighter display. It was also released alongside a cheaper Series 1.

On September 12, 2017, Apple introduced the Apple Watch Series 3 featuring LTE cellular connectivity, giving the wearable independence from an iPhone except for the setup process.

On September 12, 2018, Apple introduced the Apple Watch Series 4, featuring new display, electrocardiogram, and fall detection.

On September 10, 2019, Apple introduced the Apple Watch Series 5, featuring a new magnetometer, a faster processor, and a new always-on display. The Series 4 was discontinued.

At the 2007 Macworld conference, Jobs demonstrated the Apple TV (Jobs accidentally referred to the device as "iTV", its codename, while on stage), a set-top video device intended to bridge the sale of content from iTunes with high-definition televisions. The device, running a variant of Mac OS X, links up to a user's TV and syncs over the wireless or wired network with one computer's iTunes library and can stream content from an additional four. The Apple TV originally incorporated a 40 GB hard drive for storage, included outputs for HDMI and component video, and played video at a maximum resolution of 720p. On May 30, 2007, a 160 GB hard disk drive was released alongside the existing 40 GB model. A software update released on January 15, 2008, allowed media to be purchased directly from the Apple TV.

In September 2009, Apple discontinued the original 40 GB Apple TV but continued to produce and sell the 160 GB Apple TV. On September 1, 2010, Apple released a completely redesigned Apple TV running on an iOS variant and discontinued the older model, which ran on a Mac OS X variant. The new device is 1/4 the size, runs quieter, and replaces the need for a hard drive with media streaming from any iTunes library on the network along with 8 GB of flash memory to cache downloaded media. Like the iPad and the iPhone, Apple TV runs on an A4 processor. The memory included in the device is half of that in the iPhone 4 at 256 MB; the same as the iPad, iPhone 3GS, third and fourth-generation iPod Touch.

It has HDMI out as the only video output source. Features include access to the iTunes Store to rent movies and TV shows (purchasing has been discontinued), streaming from internet video sources, including YouTube and Netflix, and media streaming from an iTunes library. Apple also reduced the price of the device to $99. A third generation of the device was introduced at an Apple event on March 7, 2012, with new features such as higher resolution (1080p) and a new user interface.

At the September 9, 2015, event, Apple unveiled an overhauled Apple TV, which now runs a subsequent variant of iOS called tvOS, and contains 32 GB or 64 GB of NAND Flash to store games, programs, and to cache the current media playing. The release also coincided with the opening of a separate Apple TV App Store and a new Siri Remote with a glass touchpad, gyroscope, and microphone.

On December 12, 2016, Apple released a new iOS and tvOS media player app called TV to replace the existing "Videos" iOS application.

At the September 12, 2017 event, Apple released a new 4K Apple TV with the same form factor as the 4th Generation model. The 4K model is powered by the A10X SoC designed in-house that also powers their second-generation iPad Pro. The 4K model also has support for high dynamic range.

On March 25, 2019, Apple announced Apple TV+, their upcoming over-the-top subscription video on-demand web television service, will arrive Fall 2019. TV+ features exclusive original shows, movies, and documentaries. They also announced an update to the TV app with a new "Channels" feature and that the TV app will expand to macOS, numerous smart television models, Roku devices, and Amazon Fire TV devices later in 2019.

Apple's first smart speaker, the HomePod was released on February 9, 2018 after being delayed from its initial December 2017 release. It also features 7 tweeters in the base, a four-inch woofer in the top, and six microphones for voice control and acoustic optimization On September 12, 2018, Apple announced that HomePod is adding new features—search by lyrics, set multiple timers, make and receive phone calls, Find My iPhone, Siri Shortcuts—and Siri languages. In 2019, Apple, Google, Amazon, and Zigbee Alliance announced a partnership to make smart home products work together.

Apple develops its own operating systems to run on its devices, including macOS for Mac personal computers, iOS for its iPhone, iPad and iPod Touch smartphones and tablets, watchOS for its Apple Watch smartwatches, and tvOS for its Apple TV digital media player.

For iOS and macOS, Apple also develops its own software titles, including Pages for writing, Numbers for spreadsheets, and Keynote for presentations, as part of its iWork productivity suite. For macOS, it also offers iMovie and Final Cut Pro X for video editing, and GarageBand and Logic Pro X for music creation.

Apple's range of server software includes the operating system macOS Server; Apple Remote Desktop, a remote systems management application; and Xsan, a storage area network file system.

Apple also offers online services with iCloud, which provides cloud storage and synchronization for a wide range of user data, including documents, photos, music, device backups, and application data, and Apple Music, its music and video streaming service.

According to the "Sydney Morning Herald", Apple wants to start producing an electric car with autonomous driving as soon as 2020. Apple has made efforts to recruit battery development engineers and other electric automobile engineers from A123 Systems, LG Chem, Samsung Electronics, Panasonic, Toshiba, Johnson Controls and Tesla Motors.

According to Steve Jobs, the company's name was inspired by his visit to an apple farm while on a fruitarian diet. Jobs thought the name "Apple" was "fun, spirited and not intimidating".

Apple's first logo, designed by Ron Wayne, depicts Sir Isaac Newton sitting under an apple tree. It was almost immediately replaced by Rob Janoff's "rainbow Apple", the now-familiar rainbow-colored silhouette of an apple with a bite taken out of it. Janoff presented Jobs with several different monochromatic themes for the "bitten" logo, and Jobs immediately took a liking to it. However, Jobs insisted that the logo be colorized to humanize the company. The logo was designed with a bite so that it would not be confused with a cherry. The colored stripes were conceived to make the logo more accessible, and to represent the fact the Apple II could generate graphics in color. This logo is often erroneously referred to as a tribute to Alan Turing, with the bite mark a reference to his method of suicide. Both Janoff and Apple deny any homage to Turing in the design of the logo.

On August 27, 1999 (the year following the introduction of the iMac G3), Apple officially dropped the rainbow scheme and began to use monochromatic logos nearly identical in shape to the previous rainbow incarnation. An Aqua-themed version of the monochrome logo was used from 1998 to 2003, and a glass-themed version was used from 2007 to 2013.

Steve Jobs and Steve Wozniak were Beatles fans, but Apple Inc. had name and logo trademark issues with Apple Corps Ltd., a multimedia company started by the Beatles in 1968. This resulted in a series of lawsuits and tension between the two companies. These issues ended with the settling of their lawsuit in 2007.

Apple's first slogan, "Byte into an Apple", was coined in the late 1970s. From 1997 to 2002, the slogan "Think Different" was used in advertising campaigns, and is still closely associated with Apple. Apple also has slogans for specific product lines — for example, "iThink, therefore iMac" was used in 1998 to promote the iMac, and "Say hello to iPhone" has been used in iPhone advertisements. "Hello" was also used to introduce the original Macintosh, Newton, iMac ("hello (again)"), and iPod.

From the introduction of the Macintosh in 1984, with the 1984 Super Bowl advertisement to the more modern Get a Mac adverts, Apple has been recognized for its efforts towards effective advertising and marketing for its products. However, claims made by later campaigns were criticized, particularly the 2005 Power Mac ads. Apple's product advertisements gained a lot of attention as a result of their eye-popping graphics and catchy tunes. Musicians who benefited from an improved profile as a result of their songs being included on Apple advertisements include Canadian singer Feist with the song "1234" and Yael Naïm with the song "New Soul".

Apple owns a YouTube channel where they release advertisements, tips, and introductions for their devices.

Apple customers gained a reputation for devotion and loyalty early in the company's history. "BYTE" in 1984 stated that

Apple evangelists were actively engaged by the company at one time, but this was after the phenomenon had already been firmly established. Apple evangelist Guy Kawasaki has called the brand fanaticism "something that was stumbled upon," while Ive explained in 2014 that "People have an incredibly personal relationship" with Apple's products. Apple Store openings and new product releases can draw crowds of hundreds, with some waiting in line as much as a day before the opening. The opening of New York City's Fifth Avenue "Cube" store in 2006 became the setting of a marriage proposal, and had visitors from Europe who flew in for the event. In June 2017, a newlywed couple took their wedding photos inside the then-recently opened Orchard Road Apple Store in Singapore. The high level of brand loyalty has been criticized and ridiculed, applying the epithet "Apple fanboy" and mocking the lengthy lines before a product launch. An internal memo leaked in 2015 suggested the company planned to discourage long lines and direct customers to purchase its products on its website.

"Fortune" magazine named Apple the most admired company in the United States in 2008, and in the world from 2008 to 2012. On September 30, 2013, Apple surpassed Coca-Cola to become the world's most valuable brand in the Omnicom Group's "Best Global Brands" report. Boston Consulting Group has ranked Apple as the world's most innovative brand every year since 2005.

"The New York Times" in 1985 stated that "Apple above all else is a marketing company". John Sculley agreed, telling "The Guardian" newspaper in 1997 that "People talk about technology, but Apple was a marketing company. It was the marketing company of the decade." Research in 2002 by NetRatings indicate that the average Apple consumer was usually more affluent and better educated than other PC company consumers. The research indicated that this correlation could stem from the fact that on average Apple Inc. products were more expensive than other PC products.

In response to a query about the devotion of loyal Apple consumers, Jonathan Ive responded:

The Apple website home page has been used to commemorate, or pay tribute to, milestones and events outside of Apple's product offerings:

Apple Inc.'s world corporate headquarters are located in the middle of Silicon Valley, at 1–6 Infinite Loop, Cupertino, California. This Apple campus has six buildings that total and was built in 1993 by Sobrato Development Cos.

Apple has a satellite campus in neighboring Sunnyvale, California, where it houses a testing and research laboratory. AppleInsider claimed in March 2014 that Apple has a top-secret facility for development of the SG5 electric vehicle project codenamed "Titan" under the shell company name SixtyEight Research.
In 2006, Apple announced its intention to build a second campus in Cupertino about east of the current campus and next to Interstate 280. The new campus building has been designed by Norman Foster. The Cupertino City Council approved the proposed "spaceship" design campus on October 15, 2013, after a 2011 presentation by Jobs detailing the architectural design of the new building and its environs. The new campus is planned to house up to 13,000 employees in one central, four-storied, circular building surrounded by extensive landscape. It will feature a café with room for 3,000 sitting people and parking underground as well as in a parking structure. The 2.8 million square foot facility will also include Jobs's original designs for a fitness center and a corporate auditorium.
Apple has expanded its campuses in Austin, Texas concurrently with building Apple Park in Cupertino. The expansion consists of two locations, with one having of workspace, and the other . Apple will invest $1 billion to build the North Austin campus. At the biggest location, 6,000 employees work on technical support, manage Apple's network of suppliers to fulfill product shipments, aid in maintaining iTunes Store and App Store, handle economy, and continuously update Apple Maps with new data. At its smaller campus, 500 engineers work on next-generation processor chips to run in future Apple products.

Apple's headquarters for Europe, the Middle East and Africa (EMEA) are located in Cork in the south of Ireland. The facility, which opened in 1980, is Apple's first location outside of the United States. Apple Sales International, which deals with all of Apple's international sales outside of the US, is located at Apple's campus in Cork along with Apple Distribution International, which similarly deals with Apple's international distribution network. On April 20, 2012, Apple added 500 new jobs at its European headquarters, increasing the total workforce from around 2,800 to 3,300 employees. The company will build a new office block on its Hollyhill Campus to accommodate the additional staff. Its United Kingdom headquarters is at Stockley Park on the outskirts of London.

In February 2015, Apple opened their new 180,000-square-foot headquarters in Herzliya, Israel, designed to accommodate approximately 800 employees. This is Apple's third office located within Israel; the first, also in Herzliya, was obtained as part of the Anobit acquisition, and the other is a research center in Haifa.

In December 2015, Apple bought the 70,000-square-foot manufacturing facility in North San Jose previously used by Maxim Integrated, in an $18.2 million deal.

The first Apple Stores were originally opened as two locations in May 2001 by then-CEO Steve Jobs, after years of attempting but failing store-within-a-store concepts. Seeing a need for improved retail presentation of the company's products, he began an effort in 1997 to revamp the retail program to get an improved relationship to consumers, and hired Ron Johnson in 2000. Jobs relaunched Apple's online store in 1997, and opened the first two physical stores in 2001. The media initially speculated that Apple would fail, but its stores were highly successful, bypassing the sales numbers of competing nearby stores and within three years reached US$1 billion in annual sales, becoming the fastest retailer in history to do so. Over the years, Apple has expanded the number of retail locations and its geographical coverage, with 499 stores across 22 countries worldwide . Strong product sales have placed Apple among the top-tier retail stores, with sales over $16 billion globally in 2011.

In May 2016, Angela Ahrendts, Apple's then Senior Vice President of Retail, unveiled a significantly redesigned Apple Store in Union Square, San Francisco, featuring large glass doors for the entry, open spaces, and rebranded rooms. In addition to purchasing products, consumers can get advice and help from "Creative Pros" – individuals with specialized knowledge of creative arts; get product support in a tree-lined Genius Grove; and attend sessions, conferences and community events, with Ahrendts commenting that the goal is to make Apple Stores into "town squares", a place where people naturally meet up and spend time. The new design will be applied to all Apple Stores worldwide, a process that has seen stores temporarily relocate or close.

Many Apple Stores are located inside shopping malls, but Apple has built several stand-alone "flagship" stores in high-profile locations. It has been granted design patents and received architectural awards for its stores' designs and construction, specifically for its use of glass staircases and cubes. The success of Apple Stores have had significant influence over other consumer electronics retailers, who have lost traffic, control and profits due to a perceived higher quality of service and products at Apple Stores. Apple's notable brand loyalty among consumers causes long lines of hundreds of people at new Apple Store openings or product releases. Due to the popularity of the brand, Apple receives a large number of job applications, many of which come from young workers. Although Apple Store employees receive above-average pay, are offered money toward education and health care, and receive product discounts, there are limited or no paths of career advancement. A May 2016 report with an anonymous retail employee highlighted a hostile work environment with harassment from customers, intense internal criticism, and a lack of significant bonuses for securing major business contracts.

Apple is one of several highly successful companies founded in the 1970s that bucked the traditional notions of corporate culture. Jobs often walked around the office barefoot even after Apple became a Fortune 500 company. By the time of the "1984" television advertisement, Apple's informal culture had become a key trait that differentiated it from its competitors. According to a 2011 report in "Fortune," this has resulted in a corporate culture more akin to a startup rather than a multinational corporation.

As the company has grown and been led by a series of differently opinionated chief executives, it has arguably lost some of its original character. Nonetheless, it has maintained a reputation for fostering individuality and excellence that reliably attracts talented workers, particularly after Jobs returned to the company. Numerous Apple employees have stated that projects without Jobs's involvement often took longer than projects with it.

To recognize the best of its employees, Apple created the Apple Fellows program which awards individuals who make extraordinary technical or leadership contributions to personal computing while at the company. The Apple Fellowship has so far been awarded to individuals including Bill Atkinson, Steve Capps, Rod Holt, Alan Kay, Guy Kawasaki, Al Alcorn, Don Norman, Rich Page, and Steve Wozniak.
At Apple, employees are intended to be specialists who are not exposed to functions outside their area of expertise. Jobs saw this as a means of having "best-in-class" employees in every role. For instance, Ron Johnson—Senior Vice President of Retail Operations until November 1, 2011—was responsible for site selection, in-store service, and store layout, yet had no control of the inventory in his stores. This was done by Tim Cook, who had a background in supply-chain management. Apple is known for strictly enforcing accountability. Each project has a "directly responsible individual" or "DRI" in Apple jargon. As an example, when iOS senior vice president Scott Forstall refused to sign Apple's official apology for numerous errors in the redesigned Maps app, he was forced to resign. Unlike other major U.S. companies, Apple provides a relatively simple compensation policy for executives that does not include perks enjoyed by other CEOs like country club fees or private use of company aircraft. The company typically grants stock options to executives every other year.

In 2015, Apple had 110,000 full-time employees. This increased to 116,000 full-time employees the next year, a notable hiring decrease, largely due to its first revenue decline. Apple does not specify how many of its employees work in retail, though its 2014 SEC filing put the number at approximately half of its employee base. In September 2017, Apple announced that it had over 123,000 full-time employees.

Apple has a strong culture of corporate secrecy, and has an anti-leak Global Security team that recruits from the National Security Agency, the Federal Bureau of Investigation, and the United States Secret Service.

In December 2017, Glassdoor named Facebook the best place to work, according to reviews from anonymous employees, with Apple dropping to 48th place, having originally entered at rank 19 in 2009, peaking at rank 10 in 2012, and falling down the ranks in subsequent years.

An editorial article in "The Verge" in September 2016 by technology journalist Thomas Ricker explored some of the public's perceived lack of innovation at Apple in recent years, specifically stating that Samsung has "matched and even surpassed Apple in terms of smartphone industrial design" and citing the belief that Apple is incapable of producing another breakthrough moment in technology with its products. He goes on to write that the criticism focuses on individual pieces of hardware rather than the ecosystem as a whole, stating "Yes, iteration is boring. But it's also how Apple does business. [...] It enters a new market and then refines and refines and continues refining until it yields a success". He acknowledges that people are wishing for the "excitement of revolution", but argues that people want "the comfort that comes with harmony". Furthermore, he writes that "a device is only the starting point of an experience that will ultimately be ruled by the ecosystem in which it was spawned", referring to how decent hardware products can still fail without a proper ecosystem (specifically mentioning that Walkman didn't have an ecosystem to keep users from leaving once something better came along), but how Apple devices in different hardware segments are able to communicate and cooperate through the iCloud cloud service with features including Universal Clipboard (in which text copied on one device can be pasted on a different device) as well as inter-connected device functionality including Auto Unlock (in which an Apple Watch can unlock a Mac in close proximity). He argues that Apple's ecosystem is its greatest innovation.

"The Wall Street Journal" reported in June 2017 that Apple's increased reliance on Siri, its virtual personal assistant, has raised questions about how much Apple can actually accomplish in terms of functionality. Whereas Google and Amazon make use of big data and analyze customer information to personalize results, Apple has a strong pro-privacy stance, intentionally not retaining user data. "Siri is a textbook of leading on something in tech and then losing an edge despite having all the money and the talent and sitting in Silicon Valley", Holger Mueller, a technology analyst, told the "Journal". The report further claims that development on Siri has suffered due to team members and executives leaving the company for competitors, a lack of ambitious goals, and shifting strategies. Though switching Siri's functions to machine learning and algorithms, which dramatically cut its error rate, the company reportedly still failed to anticipate the popularity of Amazon's Echo, which features the Alexa personal assistant. Improvements to Siri stalled, executives clashed, and there were disagreements over the restrictions imposed on third-party app interactions. While Apple acquired an England-based startup specializing in conversational assistants, Google's Assistant had already become capable of helping users select Wi-Fi networks by voice, and Siri was lagging in functionality.

In December 2017, two articles from "The Verge" and "ZDNet" debated what had been a particularly devastating week for Apple's macOS and iOS software platforms. The former had experienced a severe security vulnerability, in which Macs running the then-latest macOS High Sierra software were vulnerable to a bug that let anyone gain administrator privileges by entering "root" as the username in system prompts, leaving the password field empty and twice clicking "unlock", gaining full access. The bug was publicly disclosed on Twitter, rather than through proper bug bounty programs. Apple released a security fix within a day and issued an apology, stating that "regrettably we stumbled" in regards to the security of the latest updates. After installing the security patch, however, file sharing was broken for users, with Apple releasing a support document with instructions to separately fix that issue. Though Apple publicly stated the promise of "auditing our development processes to help prevent this from happening again", users who installed the security update while running the older 10.13.0 version of the High Sierra operating system rather than the then-newest 10.13.1 release experienced that the "root" security vulnerability was re-introduced, and persisted even after fully updating their systems. On iOS, a date bug caused iOS devices that received local app notifications at 12:15am on December 2, 2017 to repeatedly restart. Users were recommended to turn off notifications for their apps. Apple quickly released an update, done during the nighttime in Cupertino, California time and outside of their usual software release window, with one of the headlining features of the update needing to be delayed for a few days. The combined problems of the week on both macOS and iOS caused "The Verge"s Tom Warren to call it a "nightmare" for Apple's software engineers and described it as a significant lapse in Apple's ability to protect its more than 1 billion devices. "ZDNet"s Adrian Kingsley-Hughes wrote that "it's hard to not come away from the last week with the feeling that Apple is slipping". Kingsley-Hughes also concluded his piece by referencing an earlier article, in which he wrote that "As much as I don't want to bring up the tired old 'Apple wouldn't have done this under Steve Jobs's watch' trope, a lot of what's happening at Apple lately is different from what they came to expect under Jobs. Not to say that things didn't go wrong under his watch, but product announcements and launches felt a lot tighter for sure, as did the overall quality of what Apple was releasing." He did, however, also acknowledge that such failures "may indeed have happened" with Jobs in charge, though returning to the previous praise for his demands of quality, stating "it's almost guaranteed that given his personality that heads would have rolled, which limits future failures".

The company's manufacturing, procurement, and logistics enable it to execute massive product launches without having to maintain large, profit-sapping inventories. In 2011, Apple's profit margins were 40 percent, compared with between 10 and 20 percent for most other hardware companies. Cook's catchphrase to describe his focus on the company's operational arm is: "Nobody wants to buy sour milk".

During the Mac's early history Apple generally refused to adopt prevailing industry standards for hardware, instead creating their own. This trend was largely reversed in the late 1990s, beginning with Apple's adoption of the PCI bus in the 7500/8500/9500 Power Macs. Apple has since joined the industry standards groups to influence the future direction of technology standards such as USB, AGP, HyperTransport, Wi-Fi, NVMe, PCIe and others in its products. FireWire is an Apple-originated standard that was widely adopted across the industry after it was standardized as IEEE 1394 and is a legally mandated port in all Cable TV boxes in the United States.

Apple has gradually expanded its efforts in getting its products into the Indian market. In July 2012, during a conference call with investors, CEO Tim Cook said that he "[loves] India", but that Apple saw larger opportunities outside the region. India's requirement that 30% of products sold be manufactured in the country was described as "really adds cost to getting product to market". In October 2013, Indian Apple executives unveiled a plan for selling devices through instalment plans and store-within-a-store concepts, in an effort to expand further into the market. The news followed Cook's acknowledgment of the country in July when sales results showed that iPhone sales in India grew 400% during the second quarter of 2013. In March 2016, "The Times of India" reported that Apple had sought permission from the Indian government to sell refurbished iPhones in the country. However, two months later, the application was rejected, citing official country policy. In May 2016, Apple opened an iOS app development center in Bangalore and a maps development office for 4,000 staff in Hyderabad. In February 2017, Apple once again requested permission to sell used iPhones in the country. The same month, "Bloomberg" reported that Apple was close to receiving permission to open its first retail store in the country. In March, "The Wall Street Journal" reported that Apple would begin manufacturing iPhone models in India "over the next two months", and in May, the "Journal" wrote that an Apple manufacturer had begun production of iPhone SE in the country, while Apple told "CNBC" that the manufacturing was for a "small number" of units. Reuters reported in December 2017, that Apple and the Indian government were clashing over planned increases to import taxes for components used in mobile phone production, with Apple having engaged in talks with government officials to try to delay the plans, but the Indian government sticking to its policies of no exemptions to its "Made in India" initiative. The import tax increases went into effect a few days later, with Apple being hurt the most out of all phone manufacturers, having nine of out ten phones imported into the country, whereas main smartphone competitor Samsung produces almost all of its devices locally. In April 2019, Apple initiated manufacturing of iPhone 7 at its Bengaluru facility, keeping in mind demand from local customers even as they seek more incentives from the government of India.

In May 2017, the company announced a $1 billion funding project for "advanced manufacturing" in the United States, and subsequently invested $200 million in Corning Inc., a manufacturer of toughened Gorilla Glass technology used in its iPhone devices. The following December, Apple's chief operating officer, Jeff Williams, told "CNBC" that the "$1 billion" amount was "absolutely not" the final limit on its spending, elaborating that "We're not thinking in terms of a fund limit. ... We're thinking about, where are the opportunities across the U.S. to help nurture companies that are making the advanced technology — and the advanced manufacturing that goes with that — that quite frankly is essential to our innovation".

The company advertised its products as being made in America until the late 1990s; however, as a result of outsourcing initiatives in the 2000s, almost all of its manufacturing is now handled abroad. According to a report by "The New York Times", Apple insiders "believe the vast scale of overseas factories, as well as the flexibility, diligence and industrial skills of foreign workers, have so outpaced their American counterparts that "Made in the U.S.A." is no longer a viable option for most Apple products".

In 2006, the "Mail on Sunday" reported on the working conditions of the Chinese factories where contract manufacturers Foxconn and Inventec produced the iPod. The article stated that one complex of factories that assembled the iPod and other items had over 200,000 workers living and working within it. Employees regularly worked more than 60 hours per week and made around $100 per month. A little over half of the workers' earnings was required to pay for rent and food from the company.

Apple immediately launched an investigation after the 2006 media report, and worked with their manufacturers to ensure acceptable working conditions. In 2007, Apple started yearly audits of all its suppliers regarding worker's rights, slowly raising standards and pruning suppliers that did not comply. Yearly progress reports have been published since 2008. In 2011, Apple admitted that its suppliers' child labor practices in China had worsened.

The Foxconn suicides occurred between January and November 2010, when 18 Foxconn (Chinese: 富士康) employees attempted suicide, resulting in 14 deaths—the company was the world's largest contract electronics manufacturer, for clients including Apple, at the time. The suicides drew media attention, and employment practices at Foxconn were investigated by Apple. Apple issued a public statement about the suicides, and company spokesperson Steven Dowling said:

The statement was released after the results from the company's probe into its suppliers' labor practices were published in early 2010. Foxconn was not specifically named in the report, but Apple identified a series of serious labor violations of labor laws, including Apple's own rules, and some child labor existed in a number of factories. Apple committed to the implementation of changes following the suicides.

Also in 2010, workers in China planned to sue iPhone contractors over poisoning by a cleaner used to clean LCD screens. One worker claimed that he and his coworkers had not been informed of possible occupational illnesses. After a high suicide rate in a Foxconn facility in China making iPads and iPhones, albeit a lower rate than that of China as a whole, workers were forced to sign a legally binding document guaranteeing that they would not kill themselves. Workers in factories producing Apple products have also been exposed to n-hexane, a neurotoxin that is a cheaper alternative than alcohol for cleaning the products.

A 2014 BBC investigation found excessive hours and other problems persisted, despite Apple's promise to reform factory practice after the 2010 Foxconn suicides. The Pegatron factory was once again the subject of review, as reporters gained access to the working conditions inside through recruitment as employees. While the BBC maintained that the experiences of its reporters showed that labor violations were continuing since 2010, Apple publicly disagreed with the BBC and stated: "We are aware of no other company doing as much as Apple to ensure fair and safe working conditions".

In December 2014, the Institute for Global Labour and Human Rights published a report which documented inhumane conditions for the 15,000 workers at a Zhen Ding Technology factory in Shenzhen, China, which serves as a major supplier of circuit boards for Apple's iPhone and iPad. According to the report, workers are pressured into 65-hour work weeks which leaves them so exhausted that they often sleep during lunch breaks. They are also made to reside in "primitive, dark and filthy dorms" where they sleep "on plywood, with six to ten workers in each crowded room." Omnipresent security personnel also routinely harass and beat the workers.

In 2019, there were reports stating that some of Foxconn's managers had used rejected parts to build iPhones, and that Apple was investigating the issue.

Apple Energy, LLC is a wholly owned subsidiary of Apple Inc. that sells solar energy. , Apple's solar farms in California and Nevada have been declared to provide 217.9 megawatts of solar generation capacity. In addition to the company's solar energy production, Apple has received regulatory approval to construct a landfill gas energy plant in North Carolina. Apple will use the methane emissions to generate electricity. Apple's North Carolina data center is already powered entirely with energy from renewable sources.

Following a Greenpeace protest, Apple released a statement on April 17, 2012, committing to ending its use of coal and shifting to 100% renewable clean energy. By 2013, Apple was using 100% renewable energy to power their data centers. Overall, 75% of the company's power came from clean renewable sources.

In 2010, Climate Counts, a nonprofit organization dedicated to directing consumers toward the greenest companies, gave Apple a score of 52 points out of a possible 100, which puts Apple in their top category "Striding". This was an increase from May 2008, when Climate Counts only gave Apple 11 points out of 100, which placed the company last among electronics companies, at which time Climate Counts also labeled Apple with a "stuck icon", adding that Apple at the time was "a choice to avoid for the climate-conscious consumer".

In May 2015, Greenpeace evaluated the state of the Green Internet and commended Apple on their environmental practices saying, "Apple's commitment to renewable energy has helped set a new bar for the industry, illustrating in very concrete terms that a 100% renewable Internet is within its reach, and providing several models of intervention for other companies that want to build a sustainable Internet."

, Apple states that 100% of its U.S. operations run on renewable energy, 100% of Apple's data centers run on renewable energy and 93% of Apple's global operations run on renewable energy. However, the facilities are connected to the local grid which usually contains a mix of fossil and renewable sources, so Apple carbon offsets its electricity use. The Electronic Product Environmental Assessment Tool (EPEAT) allows consumers to see the effect a product has on the environment. Each product receives a Gold, Silver, or Bronze rank depending on its efficiency and sustainability. Every Apple tablet, notebook, desktop computer, and display that EPEAT ranks achieves a Gold rating, the highest possible. Although Apple's data centers recycle water 35 times, the increased activity in retail, corporate and data centers also increase the amount of water use to in 2015.

During an event on March 21, 2016, Apple provided a status update on its environmental initiative to be 100% renewable in all of its worldwide operations. Lisa P. Jackson, Apple's vice president of Environment, Policy and Social Initiatives who reports directly to CEO, Tim Cook, announced that , 93% of Apple's worldwide operations are powered with renewable energy. Also featured was the company's efforts to use sustainable paper in their product packaging; 99% of all paper used by Apple in the product packaging comes from post-consumer recycled paper or sustainably managed forests, as the company continues its move to all paper packaging for all of its products. Apple working in partnership with Conservation Fund, have preserved 36,000 acres of working forests in Maine and North Carolina. Another partnership announced is with the World Wildlife Fund to preserve up to of forests in China. Featured was the company's installation of a 40 MW solar power plant in the Sichuan province of China that was tailor-made to coexist with the indigenous yaks that eat hay produced on the land, by raising the panels to be several feet off of the ground so the yaks and their feed would be unharmed grazing beneath the array. This installation alone compensates for more than all of the energy used in Apple's Stores and Offices in the whole of China, negating the company's energy carbon footprint in the country. In Singapore, Apple has worked with the Singaporean government to cover the rooftops of 800 buildings in the city-state with solar panels allowing Apple's Singapore operations to be run on 100% renewable energy. Liam was introduced to the world, an advanced robotic disassembler and sorter designed by Apple Engineers in California specifically for recycling outdated or broken iPhones. Reuses and recycles parts from traded in products.

Apple announced on August 16, 2016, that Lens Technology, one of its major suppliers in China, has committed to power all its glass production for Apple with 100 percent renewable energy by 2018. The commitment is a large step in Apple's efforts to help manufacturers lower their carbon footprint in China. Apple also announced that all 14 of its final assembly sites in China are now compliant with UL's Zero Waste to Landfill validation. The standard, which started in January 2015, certifies that all manufacturing waste is reused, recycled, composted, or converted into energy (when necessary). Since the program began, nearly, 140,000 metric tons of waste have been diverted from landfills.

Following further campaigns by Greenpeace, in 2008, Apple became the first electronics manufacturer to fully eliminate all polyvinyl chloride (PVC) and brominated flame retardants (BFRs) in its complete product line. In June 2007, Apple began replacing the cold cathode fluorescent lamp (CCFL) backlit LCD displays in its computers with mercury-free LED-backlit LCD displays and arsenic-free glass, starting with the upgraded MacBook Pro. Apple offers comprehensive and transparent information about the COe, emissions, materials, and electrical usage concerning every product they currently produce or have sold in the past (and which they have enough data needed to produce the report), in their portfolio on their homepage. Allowing consumers to make informed purchasing decisions on the products they offer for sale. In June 2009, Apple's iPhone 3GS was free of PVC, arsenic, and BFRs. All Apple products now have mercury-free LED-backlit LCD displays, arsenic-free glass, and non-PVC cables. All Apple products have EPEAT Gold status and beat the latest Energy Star guidelines in each product's respective regulatory category.

In November 2011, Apple was featured in Greenpeace's Guide to Greener Electronics, which ranks electronics manufacturers on sustainability, climate and energy policy, and how "green" their products are. The company ranked fourth of fifteen electronics companies (moving up five places from the previous year) with a score of 4.6/10. Greenpeace praises Apple's sustainability, noting that the company exceeded its 70% global recycling goal in 2010. It continues to score well on the products rating with all Apple products now being free of PVC plastic and BFRs. However, the guide criticizes Apple on the Energy criteria for not seeking external verification of its greenhouse gas emissions data and for not setting out any targets to reduce emissions. In January 2012, Apple requested that its cable maker, Volex, begin producing halogen-free USB and power cables.

In February 2016, Apple issued a US$1.5 billion green bond (climate bond), the first ever of its kind by a U.S. tech company. The green bond proceeds are dedicated to the financing of environmental projects.

Apple is the world's largest information technology company by revenue, the world's largest technology company by total assets, and the world's second-largest mobile phone manufacturer after Samsung.

In its fiscal year ending in September 2011, Apple Inc. reported a total of $108 billion in annual revenues—a significant increase from its 2010 revenues of $65 billion—and nearly $82 billion in cash reserves. On March 19, 2012, Apple announced plans for a $2.65-per-share dividend beginning in fourth quarter of 2012, per approval by their board of directors.

The company's worldwide annual revenue in 2013 totaled $170 billion. In May 2013, Apple entered the top ten of the Fortune 500 list of companies for the first time, rising 11 places above its 2012 ranking to take the sixth position. , Apple has around US$234 billion of cash and marketable securities, of which 90% is located outside the United States for tax purposes.

Apple amassed 65% of all profits made by the eight largest worldwide smartphone manufacturers in quarter one of 2014, according to a report by Canaccord Genuity. In the first quarter of 2015, the company garnered 92% of all earnings.

On April 30, 2017, "The Wall Street Journal" reported that Apple had cash reserves of $250 billion, officially confirmed by Apple as specifically $256.8 billion a few days later.

, Apple was the largest publicly traded corporation in the world by market capitalization. On August 2, 2018, Apple became the first publicly traded U.S. company to reach a $1 trillion market value. Apple was ranked #4 on the 2018 Fortune 500 rankings of the largest United States corporations by total revenue.

Apple has created subsidiaries in low-tax places such as Ireland, the Netherlands, Luxembourg, and the British Virgin Islands to cut the taxes it pays around the world. According to "The New York Times," in the 1980s Apple was among the first tech companies to designate overseas salespeople in high-tax countries in a manner that allowed the company to sell on behalf of low-tax subsidiaries on other continents, sidestepping income taxes. In the late 1980s, Apple was a pioneer of an accounting technique known as the "Double Irish with a Dutch sandwich," which reduces taxes by routing profits through Irish subsidiaries and the Netherlands and then to the Caribbean.

British Conservative Party Member of Parliament Charlie Elphicke published research on October 30, 2012, which showed that some multinational companies, including Apple Inc., were making billions of pounds of profit in the UK, but were paying an effective tax rate to the UK Treasury of only 3 percent, well below standard corporation tax. He followed this research by calling on the Chancellor of the Exchequer George Osborne to force these multinationals, which also included Google and The Coca-Cola Company, to state the effective rate of tax they pay on their UK revenues. Elphicke also said that government contracts should be withheld from multinationals who do not pay their fair share of UK tax.

Apple Inc. claims to be the single largest taxpayer to the Department of the Treasury of the United States of America with an effective tax rate of approximately of 26% as of the second quarter of the Apple fiscal year 2016. In an interview with the German newspaper FAZ in October 2017, Tim Cook stated, that Apple is the biggest taxpayer worldwide.

In 2015, Reuters reported that Apple had earnings abroad of $54.4 billion which were untaxed by the IRS of the United States. Under U.S. tax law governed by the IRC, corporations don't pay income tax on overseas profits unless the profits are repatriated into the United States and as such Apple argues that to benefit its shareholders it will leave it overseas until a repatriation holiday or comprehensive tax reform takes place in the United States.

On July 12, 2016 the Central Statistics Office of Ireland announced that 2015 Irish GDP had grown by 26.3%, and 2015 Irish GNP had grown by 18.7%. The figures attracted international scorn, and were labelled by Nobel-prize winning economist, Paul Krugman, as leprechaun economics. It was not until 2018 that Irish economists could definitively prove that the 2015 growth was due to Apple restructuring its controversial double Irish subsidiaries (Apple Sales International), which Apple converted into a new Irish capital allowances for intangible assets tax scheme (expires in January 2020). The affair required the Central Bank of Ireland to create a new measure of Irish economic growth, Modified GNI* to replace Irish GDP, given the distortion of Apple's tax schemes. Irish GDP is 143% of Irish Modified GNI*.

On August 30, 2016, after a two-year investigation, the EU Competition Commissioner concluded Apple received "illegal State aid" from Ireland. The EU ordered Apple to pay 13 billion euros ($14.5 billion), plus interest, in unpaid Irish taxes for 2004–2014. It is the largest tax fine in history. The Commission found that Apple had benefitted from a private Irish Revenue Commissioners tax ruling regarding its double Irish tax structure, Apple Sales International (ASI). Instead of using two companies for its double Irish structure, Apple was given a ruling to split ASI into two internal "branches". The Chancellor of Austria, Christian Kern, put this decision into perspective by stating that "every Viennese cafe, every sausage stand pays more tax in Austria than a multinational corporation".

, Apple agreed to start paying €13 billion in back taxes to the Irish government, the repayments will be held in an escrow account while Apple and the Irish government continue their appeals in EU courts.

 the following individuals sit on the board of Apple Inc.

 the management of Apple Inc. includes:

Apple has been a participant in various legal proceedings and claims since it began operation. In particular, Apple is known for and promotes itself as actively and aggressively enforcing its intellectual property interests. Some litigation examples include "Apple v. Samsung", "Apple v. Microsoft", "Motorola Mobility v. Apple Inc.", and "Apple Corps v. Apple Computer". Apple has also had to defend itself against charges on numerous occasions of violating intellectual property rights. Most have been dismissed in the courts as shell companies known as patent trolls, with no evidence of actual use of patents in question. On December 21, 2016, Nokia announced that in the U.S. and Germany, it has filed a suit against Apple, claiming that the latter's products infringe on Nokia's patents. Most recently, in November 2017, the United States International Trade Commission announced an investigation into allegations of patent infringement in regards to Apple's remote desktop technology; Aqua Connect, a company that builds remote desktop software, has claimed that Apple infringed on two of its patents.

Apple has a notable pro-privacy stance, actively making privacy-conscious features and settings part of its conferences, promotional campaigns, and public image. With its iOS 8 mobile operating system in 2014, the company started encryption all contents of iOS devices through users' passcodes, making it impossible for the company to provide customer data to law enforcement requests seeking such information. With the popularity rise of cloud storage solutions, Apple began a technique in 2016 to do deep learning scans for facial data in photos on the user's local device and encrypting the content before uploading it to Apple's iCloud storage system. It also introduced "differential privacy", a way to collect crowdsourced data from many users, while keeping individual users anonymous, in a system that "Wired" described as "trying to learn as much as possible about a group while learning as little as possible about any individual in it". Users are explicitly asked if they want to participate, and can actively opt-in or opt-out.

However, Apple aids law enforcement in criminal investigations by providing iCloud backups of users' devices, and the company's commitment to privacy has been questioned by its efforts to promote biometric authentication technology in its newer iPhone models, which don't have the same level of constitutional privacy as a passcode in the United States.

Apple is a partner of (PRODUCT)RED, a fundraising campaign for AIDS charity. In November 2014, Apple arranged for all App Store revenue in a two-week period to go to the fundraiser, generating more than US$20 million, and in March 2017, it released an iPhone 7 with a red color finish.

Apple contributes financially to fundraisers in times of natural disasters. In November 2012, it donated $2.5 million to the American Red Cross to aid relief efforts after Hurricane Sandy, and in 2017 it donated $5 million to relief efforts for both Hurricane Irma and Hurricane Harvey, as well as for the 2017 Central Mexico earthquake. The company has also used its iTunes platform to encourage donations, including, but not limited to, help the American Red Cross in the aftermath of the 2010 Haiti earthquake, followed by similar procedure in the aftermath of the 2011 Japan earthquake, Typhoon Haiyan in the Philippines in November 2013, and European migrant crisis in September 2015. Apple emphasizes that it does not incur any processing or other fees for iTunes donations, sending 100% of the payments directly to relief efforts, though it also acknowledges that the Red Cross does not receive any personal information on the users donating and that the payments may not be tax deductible.

On April 14, 2016, Apple and the World Wide Fund for Nature (WWF) announced that they have engaged in a partnership to, "help protect life on our planet." Apple released a special page in the iTunes App Store, Apps for Earth. In the arrangement, Apple has committed that through April 24, WWF will receive 100% of the proceeds from the applications participating in the App Store via both the purchases of any paid apps and the In-App Purchases. Apple and WWF's Apps for Earth campaign raised more than $8 million in total proceeds to support WWF's conservation work. WWF announced the results at WWDC 2016 in San Francisco.

Apple has been criticized for alleged unethical business practices such as anti-competitive behavior, rash litigation, dubious tax tactics, production methods involving the use of sweatshop labor, customer service issues involving allegedly misleading warranties and insufficient data security, and its products' environmental footprint. Critics have claimed that Apple products combine stolen and/or purchased designs that Apple claims are its original creations. It has been criticized for its alleged collaboration with the U.S. surveillance program PRISM.

Apple's issues regarding music over the years include those with the European Union regarding iTunes, trouble over updating the Spotify app on Apple devices and collusion with record labels.

Apple has faced scrutiny for its tax practices, including using a Double Irish Arrangement to reduce the amount of taxes it pays. A 2013 US Senate report claimed that Apple hadn't paid corporate taxes for five years due to its deals with the Irish government. In 2016, the European Union ordered Apple to pay a fine for its actions.

In 2018–19, Apple faced criticism for its failure to approve NVIDIA web drivers for GPU installed on legacy Mac Pro machines up to mid 2012 5,1 running macOS Mojave 10.14. Without Apple approved NVIDIA web drivers, Apple users are faced with replacing their NVIDIA cards with a competing supported brand, such as AMD Radeon from the list recommended by Apple.

In June 2019, Apple issued a recall for its 2015 MacBook Pro Retina 15" affecting 432,000 units after reports of batteries catching fire. The recall was criticized as waiting times for replacements were up to 3 weeks and the company didn't provide alternative replacements or repair options.

Ireland's Data Protection Commission in Ireland also launched a privacy investigation to examine whether Apple complied with the EU's GDPR law following an investigation into how the company processes personal data with targeted ads on its platform.

In July 2019, following a campaign by the "right to repair" movement, challenging Apple's tech repair restrictions on devices, the FTC held a workshop to establish the framework of a future nationwide Right to Repair rule. The movement argues Apple is preventing consumers from legitimately fixing their devices at local repair shops which is having a negative impact on consumers.

The Department of Justice also began a review of big tech firms to establish whether they could be unlawfully stifling competition in a broad antitrust probe in 2019.

In December 2019, a report found that the iPhone 11 Pro continues tracking location and collecting user data even after users have disabled location services. In response, an Apple engineer said the Location Services icon "appears for system services that do not have a switch in settings."

In January 2020, US President Donald Trump slammed Apple for refusing to unlock two iPhones of a Saudi national, Mohammed Saeed Alshamrani, who shot and killed three American sailors and injured eight others in the Naval air base. The Pensacola shooting was declared an “act of terrorism” by the FBI, but Apple denied to crack the phones citing its data privacy policy.





</doc>
<doc id="857" url="https://en.wikipedia.org/wiki?curid=857" title="Aberdeenshire">
Aberdeenshire

Aberdeenshire (; ) is one of the 32 council areas of Scotland.

It takes its name from the County of Aberdeen which has substantially different boundaries. The Aberdeenshire council area includes all of the area of the historic counties of Aberdeenshire and Kincardineshire (except the area making up the City of Aberdeen), as well as part of Banffshire. The county boundaries are officially used for a few purposes, namely land registration and lieutenancy.

Aberdeenshire Council is headquartered at Woodhill House, in Aberdeen, making it the only Scottish council whose headquarters are located outside its jurisdiction. Aberdeen itself forms a different council area (Aberdeen City). Aberdeenshire borders onto Angus and Perth and Kinross to the south, Highland and Moray to the west and Aberdeen City to the east.

Traditionally, it has been economically dependent upon the primary sector (agriculture, fishing, and forestry) and related processing industries. Over the last 40 years, the development of the oil and gas industry and associated service sector has broadened Aberdeenshire's economic base, and contributed to a rapid population growth of some 50% since 1975. Its land represents 8% of Scotland's overall territory. It covers an area of .

Aberdeenshire has a rich prehistoric and historic heritage. It is the locus of a large number of Neolithic and Bronze Age archaeological sites, including Longman Hill, Kempstone Hill, Catto Long Barrow and Cairn Lee. The area was settled in the Bronze Age by the Beaker culture, who arrived from the south around 2000–1800 BC. Stone circles and cairns were constructed predominantly in this era. In the Iron Age, hill forts were built. Around the 1st century AD, the Taexali people, who have left little history, were believed to have resided along the coast. The Picts were the next documented inhabitants of the area, and were no later than 800–900 AD. The Romans also were in the area during this period, as they left signs at Kintore. Christianity influenced the inhabitants early on, and there were Celtic monasteries at Old Deer and Monymusk.

Since medieval times there have been a number of traditional paths that crossed the Mounth (a spur of mountainous land that extends from the higher inland range to the North Sea slightly north of Stonehaven) through present-day Aberdeenshire from the Scottish Lowlands to the Highlands. Some of the most well known and historically important trackways are the Causey Mounth and Elsick Mounth.

Aberdeenshire played an important role in the fighting between the Scottish clans. Clan MacBeth and the Clan Canmore were two of the larger clans. Macbeth fell at Lumphanan in 1057. During the Anglo-Norman penetration, other families arrives such as House of Balliol, Clan Bruce, and Clan Cumming (Comyn). When the fighting amongst these newcomers resulted in the Scottish Wars of Independence, the English king Edward I traveled across the area twice, in 1296 and 1303. In 1307, Robert the Bruce was victorious near Inverurie. Along with his victory came new families, namely the Forbeses and the Gordons.

These new families set the stage for the upcoming rivalries during the 14th and 15th centuries. This rivalry grew worse during and after the Protestant Reformation, when religion was another reason for conflict between the clans. The Gordon family adhered to Catholicism and the Forbeses to Protestantism. Aberdeenshire was the historic seat of the clan Dempster. Three universities were founded in the area prior to the 17th century, King's College in Old Aberdeen (1494), Marischal College in Aberdeen (1593), and the University of Fraserburgh (1597).

After the end of the Revolution of 1688, an extended peaceful period was interrupted only by such fleeting events such as the Rising of 1715 and the Rising of 1745. The latter resulted in the end of the ascendancy of Episcopalianism and the feudal power of landowners. An era began of increased agricultural and industrial progress. During the 17th century, Aberdeenshire was the location of more fighting, centered on the Marquess of Montrose and the English Civil Wars. This period also saw increased wealth due to the increase in trade with Germany, Poland, and the Low Countries.

The present council area is named after the historic county of Aberdeenshire, which has different boundaries and was abandoned as an administrative area in 1975 under the Local Government (Scotland) Act 1973. It was replaced by Grampian Regional Council and five district councils: Banff and Buchan, Gordon, Kincardine and Deeside, Moray and the City of Aberdeen. Local government functions were shared between the two levels. In 1996, under the Local Government etc (Scotland) Act 1994, the Banff and Buchan district, Gordon district and Kincardine and Deeside district were merged to form the present Aberdeenshire council area. Moray and the City of Aberdeen were made their own council areas. The present Aberdeenshire council area consists of all of the historic counties of Aberdeenshire and Kincardineshire (except the area of those two counties making up the City of Aberdeen), as well as northeast portions of Banffshire.

The population of the council area has risen over 50% since 1971 to approximately , representing 4.7% of Scotland's total. Aberdeenshire's population has increased by 9.1% since 2001, while Scotland's total population grew by 3.8%.
The census lists a relatively high proportion of under 16s and slightly fewer people of working-age compared with the Scottish average.

Aberdeenshire is one of the most homogeneous regions of the UK. In 2011 82.2% of residents identified as 'White Scottish', followed by 12.3% who are 'White British'. The largest ethnic minority group are Asian Scottish/British at 0.8%.

The fourteen biggest settlements in Aberdeenshire (with 2011 population estimates) are:


Aberdeenshire's Gross Domestic Product (GDP) is estimated at £3,496m (2011), representing 5.2% of the Scottish total. Aberdeenshire's economy is closely linked to Aberdeen City's (GDP £7,906m) and in 2011 the region as a whole was calculated to contribute 16.8% of Scotland's GDP. Between 2012 and 2014 the combined Aberdeenshire and Aberdeen City economic forecast GDP growth rate is 8.6%, the highest growth rate of any local council area in the UK and above the Scottish rate of 4.8%.

A significant proportion of Aberdeenshire's working residents commute to Aberdeen City for work, varying from 11.5% from Fraserburgh to 65% from Westhill.

Average Gross Weekly Earnings (for full-time employees employed in work places in Aberdeenshire in 2011) are £572.60. This is lower than the Scottish average by £2.10 and a fall of 2.6% on the 2010 figure. The average gross weekly pay of people resident in Aberdeenshire is much higher, at £741.90, as many people commute out
of Aberdeenshire, principally into Aberdeen City.

Total employment (excluding farm data) in Aberdeenshire is estimated at 93,700 employees (Business Register and
Employment Survey 2009). The majority of employees work within the service sector, predominantly in public administration, education and health. Almost 19% of employment is within the public sector. Aberdeenshire's economy remains closely linked to Aberdeen City's and the North Sea oil industry, with many employees in oil related jobs.

The average monthly unemployment (claimant count) rate for Aberdeenshire in 2011 was 1.5%. This is lower than the average rates for Aberdeen City (2.3%), Scotland (4.2%) and the UK (3.8%).


The council has 70 councillors, elected in 19 multi-member wards by single transferable vote. The 2017 elections resulted in the following representation:

The overall political composition of the council, following subsequent defections and by-elections, is as follows:
The Council's Revenue Budget for 2012/13 totals approx £548 million. The Education, Learning and Leisure Service takes the largest share of budget (52.3%), followed by Housing and Social Work (24.3%), Infrastructure Services (15.9%), Joint Boards (such as Fire and Police) and Misc services (7.9%) and Trading Activities (0.4%).

21.5% of the revenue is raised locally through the Council Tax. Average Band D Council Tax is £1,141 (2012/13), no change on the previous year.
The current chief executive of the Council is Jim Savege and the elected Council Leader is Jim Gifford. Aberdeenshire also has a Provost, who is Councillor Bill Howatson.

The council has devolved power to six area committees: Banff and Buchan; Buchan; Formartine; Garioch; Marr; and Kincardine and Mearns. Each area committee takes decisions on local issues such as planning applications, and the split is meant to reflect the diverse circumstances of each area. (Boundary map)

The following significant structures or places are within Aberdeenshire:


There are numerous rivers and burns in Aberdeenshire, including Cowie Water, Carron Water, Burn of Muchalls, River Dee, River Don, River Ury, River Ythan, Water of Feugh, Burn of Myrehouse, Laeca Burn and Luther Water. Numerous bays and estuaries are found along the seacoast of Aberdeenshire, including Banff Bay, Ythan Estuary, Stonehaven Bay and Thornyhive Bay. Aberdeenshire is in the rain shadow of the Grampians, therefore it is a generally dry climate, with portions of the coast, receiving of moisture annually. Summers are mild and winters are typically cold in Aberdeenshire; Coastal temperatures are moderated by the North Sea such that coastal areas are typically cooler in the summer and warmer in winter than inland locations. Coastal areas are also subject to haar, or coastal fog.




</doc>
<doc id="859" url="https://en.wikipedia.org/wiki?curid=859" title="Aztlan Underground">
Aztlan Underground

Aztlan Underground is a band from Los Angeles, California that combines Hip-Hop, Punk Rock, Jazz, and electronic music with Chican@ and Native American themes, and indigenous instrumentation. They are often cited as progenitors of Chican@/Native rap and hip-hop.

The band traces its roots to the late-1980s hardcore scene in the Eastside of Los Angeles. They have played rapcore, with elements of punk, hip hop, rock, funk, jazz, indigenous music, and spoken word. Indigenous drums, flutes, and rattles are also commonly used in their music. Their lyrics often address the family and economic issues faced by the Chicano community, and they have been noted as activists for that community.

As an example of the politically active and culturally important artists in Los Angeles in the 1990s, Aztlan Underground appeared on "Culture Clash" on Fox in 1993; and was part of "Breaking Out", a concert on pay per view in 1998, The band was featured in the independent films "Algun Dia" and "Frontierland" in the 1990s, and on the upcoming "Studio 49". The band has been mentioned or featured in various newspapers and magazines: "the Vancouver Sun", "New Times", "BLU Magazine" (an underground hip hop magazine), "BAM Magazine", "La Banda Elastica Magazine", and the "Los Angeles Times" calendar section. The band is also the subject of a chapter in the book "It's Not About a Salary", by Brian Cross.

Aztlan Underground remains active in the community, lending their voice to annual events such as The Farce of July, and the recent movement to recognize Indigenous People's Day in Los Angeles and beyond.

In addition to forming their own label, Xicano Records and Film, Aztlan Underground were signed to the Basque record label Esan Ozenki in 1999 which enabled them to tour Spain extensively and perform in France and Portugal. Aztlan Underground have also performed in Canada, Australia, and Venezuela. The band has been recognized for their music with nominations in the "New Times" 1998 "Best Latin Influenced" category, the "BAM Magazine" 1999 "Best Rock en Español" category, and the "LA Weekly" 1999 "Best Hip Hop" category. The release of their eponymous third album on August 29, 2009 was met with positive reviews and earned the band four Native American Music Award (NAMMY) nominations in 2010.

Year:1995


Year:1998


Year: 2009




</doc>
<doc id="863" url="https://en.wikipedia.org/wiki?curid=863" title="American Civil War">
American Civil War

The American Civil War (also known by other names) was a civil war in the United States from 1861 to 1865, fought between the northern United States (loyal to the Union) and the southern United States (that had seceded from the Union and formed the Confederacy). The civil war began primarily as a result of the long-standing controversy over the enslavement of black people. War broke out in April 1861 when secessionist forces attacked Fort Sumter in South Carolina shortly after Abraham Lincoln had been inaugurated as the President of the United States. The loyalists of the Union in the North, which also included some geographically western and southern states, proclaimed support for the Constitution. They faced secessionists of the Confederate States in the South, who advocated for states' rights in order to uphold slavery.

Of the 34 U.S. states in February 1861, seven Southern "slave states" were declared by their state governments to have seceded from the country, and the Confederate States of America was organized in rebellion against the U.S. constitutional government. The Confederacy grew to control at least a majority of territory in eleven states, and it claimed the additional states of Kentucky and Missouri by assertions from native secessionists fleeing Union authority. These states were given full representation in the Confederate Congress throughout the Civil War. The two remaining "slave states", Delaware and Maryland, were invited to join the Confederacy, but nothing substantial developed due to intervention by federal troops.

The Confederate states were never diplomatically recognized as a joint entity by the government of the United States, nor by that of any foreign country. The states that remained loyal to the U.S. were known as the Union. The Union and the Confederacy quickly raised volunteer and conscription armies that fought mostly in the South over the course of four years. Intense combat left 620,000 to 750,000 people dead, more than the number of U.S. military deaths in all other wars combined.

The war effectively ended April 9, 1865, when General Robert E. Lee surrendered to General Ulysses S. Grant at the Battle of Appomattox Court House. Confederate generals throughout the southern states followed suit, the last surrender on land occurring June 23. Much of the South's infrastructure was destroyed, especially the transportation systems. The Confederacy collapsed, slavery was abolished, and four million black slaves were freed. During the Reconstruction era that followed the war, national unity was slowly restored, the national government expanded its power, and civil and political rights were granted to freed black slaves through amendments to the Constitution and federal legislation. The war is one of the most studied and written about episodes in U.S. history.

In the 1860 presidential election, Republicans, led by Abraham Lincoln, supported banning slavery in all the U.S. territories. The Southern states viewed this as a violation of their constitutional rights, and as the first step in a grander Republican plan to eventually abolish slavery. The three pro-Union candidates together received an overwhelming 82% majority of the votes cast nationally: Republican Lincoln's votes centered in the north, Democrat Stephen A. Douglas' votes were distributed nationally and Constitutional Unionist John Bell's votes centered in Tennessee, Kentucky, and Virginia. The Republican Party, dominant in the North, secured a plurality of the popular votes and a majority of the electoral votes nationally; thus Lincoln was constitutionally elected president. He was the first Republican Party candidate to win the presidency. However, before his inauguration, seven slave states with cotton-based economies declared secession and formed the Confederacy. The first six to declare secession had the highest proportions of slaves in their populations, with an average of 49 percent. Of those states whose legislatures resolved for secession, the first seven voted with split majorities for unionist candidates Douglas and Bell (Georgia with 51% and Louisiana with 55%), or with sizable minorities for those unionists (Alabama with 46%, Mississippi with 40%, Florida with 38%, Texas with 25%, and South Carolina, which cast Electoral College votes without a popular vote for president). Of these, only Texas held a referendum on secession.

Eight remaining slave states continued to reject calls for secession. Outgoing Democratic President James Buchanan and the incoming Republicans rejected secession as illegal. Lincoln's March 4, 1861, inaugural address declared that his administration would not initiate a civil war. Speaking directly to the "Southern States", he attempted to calm their fears of any threats to slavery, reaffirming, "I have no purpose, directly or indirectly to interfere with the institution of slavery in the United States where it exists. I believe I have no lawful right to do so, and I have no inclination to do so." After Confederate forces seized numerous federal forts within territory claimed by the Confederacy, efforts at compromise failed and both sides prepared for war. The Confederates assumed that European countries were so dependent on "King Cotton" that they would intervene, but none did, and none recognized the new Confederate States of America.

Hostilities began on April 12, 1861, when Confederate forces fired upon Fort Sumter. While in the Western Theater the Union made significant permanent gains, in the Eastern Theater, the battle was inconclusive during 1861–1862. Later, in September 1862, Lincoln issued the Emancipation Proclamation, which made ending slavery a war goal. To the west, by summer 1862 the Union destroyed the Confederate river navy, then much of its western armies, and seized New Orleans. The successful 1863 Union siege of Vicksburg split the Confederacy in two at the Mississippi River. In 1863, Robert E. Lee's Confederate incursion north ended at the Battle of Gettysburg. Western successes led to Ulysses S. Grant's command of all Union armies in 1864. Inflicting an ever-tightening naval blockade of Confederate ports, the Union marshaled the resources and manpower to attack the Confederacy from all directions, leading to the fall of Atlanta to William Tecumseh Sherman and his march to the sea. The last significant battles raged around the Siege of Petersburg. Lee's escape attempt ended with his surrender at Appomattox Court House, on April 9, 1865. While the military war was coming to an end, the political reintegration of the nation was to take another 12 years, known as the Reconstruction era.

The American Civil War was among the earliest industrial wars. Railroads, the telegraph, steamships, and iron-clad ships, and mass-produced weapons were employed extensively. The mobilization of civilian factories, mines, shipyards, banks, transportation, and food supplies all foreshadowed the impact of industrialization in World War I, World War II, and subsequent conflicts. It remains the deadliest war in American history. From 1861 to 1865, it is estimated that 620,000 to 750,000 soldiers died, along with an undetermined number of civilians. By one estimate, the war claimed the lives of 10 percent of all Northern men 20–45 years old, and 30 percent of all Southern white men aged 18–40.

The causes of secession were complex and have been controversial since the war began, but most academic scholars identify slavery as a central cause of the war. James C. Bradford wrote that the issue has been further complicated by historical revisionists, who have tried to offer a variety of reasons for the war. Slavery was the central source of escalating political tension in the 1850s. The Republican Party was determined to prevent any spread of slavery, and many Southern leaders had threatened secession if the Republican candidate, Lincoln, won the 1860 election. After Lincoln won, many Southern leaders felt that disunion was their only option, fearing that the loss of representation would hamper their ability to promote pro-slavery acts and policies.

Slavery was a major cause of disunion. Although there were opposing views even in the Union States, most northern soldiers were mostly indifferent on the subject of slavery, while Confederates fought the war mainly to protect a southern society of which slavery was an integral part. From the anti-slavery perspective, the issue was primarily about whether the system of slavery was an anachronistic evil that was incompatible with republicanism. The strategy of the anti-slavery forces was containment—to stop the expansion and thus put slavery on a path to gradual extinction. The slave-holding interests in the South denounced this strategy as infringing upon their Constitutional rights. Southern whites believed that the emancipation of slaves would destroy the South's economy, due to the large amount of capital invested in slaves and fears of integrating the ex-slave black population. In particular, Southerners feared a repeat of "the horrors of Santo Domingo", in which nearly all white people – including men, women, children, and even many sympathetic to abolition – were killed after the successful slave revolt in Haiti. Historian Thomas Fleming points to the historical phrase "a disease in the public mind" used by critics of this idea, and proposes it contributed to the segregation in the Jim Crow era following emancipation. These fears were exacerbated by the recent attempt of John Brown to instigate an armed slave rebellion in the South.

Slavery was illegal in much of the North, having been outlawed in the late 18th and early 19th centuries. It was also fading in the border states and in Southern cities, but it was expanding in the highly profitable cotton districts of the rural South and Southwest. Subsequent writers on the American Civil War looked to several factors explaining the geographic divide.

Between 1803 and 1854, the United States achieved a vast expansion of territory through purchase, negotiation, and conquest. At first, the new states carved out of these territories entering the union were apportioned equally between slave and free states. Pro- and anti-slavery forces collided over the territories west of the Mississippi.

With the conquest of northern Mexico west to California in 1848, slaveholding interests looked forward to expanding into these lands and perhaps Cuba and Central America as well.
Northern "free soil" interests vigorously sought to curtail any further expansion of slave territory. The Compromise of 1850 over California balanced a free-soil state with stronger fugitive slave laws for a political settlement after four years of strife in the 1840s. But the states admitted following California were all free: Minnesota (1858), Oregon (1859) and Kansas (1861). In the Southern states the question of the territorial expansion of slavery westward again became explosive. Both the South and the North drew the same conclusion: "The power to decide the question of slavery for the territories was the power to determine the future of slavery itself."

By 1860, four doctrines had emerged to answer the question of federal control in the territories, and they all claimed they were sanctioned by the Constitution, implicitly or explicitly. The first of these "conservative" theories, represented by the Constitutional Union Party, argued that the Missouri Compromise apportionment of territory north for free soil and south for slavery should become a Constitutional mandate. The Crittenden Compromise of 1860 was an expression of this view.

The second doctrine of Congressional preeminence, championed by Abraham Lincoln and the Republican Party, insisted that the Constitution did not bind legislators to a policy of balance—that slavery could be excluded in a territory as it was done in the Northwest Ordinance of 1787 at the discretion of Congress; thus Congress could restrict human bondage, but never establish it. The Wilmot Proviso announced this position in 1846.

Senator Stephen A. Douglas proclaimed the doctrine of territorial or "popular" sovereignty—which asserted that the settlers in a territory had the same rights as states in the Union to establish or disestablish slavery as a purely local matter. The Kansas–Nebraska Act of 1854 legislated this doctrine. In the Kansas Territory, years of pro and anti-slavery violence and political conflict erupted; the congressional House of Representatives voted to admit Kansas as a free state in early 1860, but its admission in the Senate was delayed until January 1861, after the 1860 elections when Southern states began to leave.

The fourth theory was advocated by Mississippi Senator Jefferson Davis, one of state sovereignty ("states' rights"), also known as the "Calhoun doctrine", named after the South Carolinian political theorist and statesman John C. Calhoun. Rejecting the arguments for federal authority or self-government, state sovereignty would empower states to promote the expansion of slavery as part of the federal union under the U.S. Constitution. "States' rights" was an ideology formulated and applied as a means of advancing slave state interests through federal authority. As historian Thomas L. Krannawitter points out, the "Southern demand for federal slave protection represented a demand for an unprecedented expansion of federal power." These four doctrines comprised the dominant ideologies presented to the American public on the matters of slavery, the territories, and the U.S. Constitution before the 1860 presidential election.

The South argued that just as each state had decided to join the Union, a state had the right to secede—leave the Union—at any time. Northerners (including President Buchanan) rejected that notion as opposed to the will of the Founding Fathers, who said they were setting up a perpetual union. Historian James McPherson writes concerning states' rights and other non-slavery explanations:

Sectionalism resulted from the different economies, social structure, customs, and political values of the North and South. Regional tensions came to a head during the War of 1812, resulting in the Hartford Convention, which manifested Northern dissastisfaction with a foreign trade embargo that affected the industrial North disproportionately, the Three-Fifths Compromise, dilution of Northern power by new states, and a succession of Southern presidents. Sectionalism increased steadily between 1800 and 1860 as the North, which phased slavery out of existence, industrialized, urbanized, and built prosperous farms, while the deep South concentrated on plantation agriculture based on slave labor, together with subsistence agriculture for poor whites. In the 1840s and 1850s, the issue of accepting slavery (in the guise of rejecting slave-owning bishops and missionaries) split the nation's largest religious denominations (the Methodist, Baptist, and Presbyterian churches) into separate Northern and Southern denominations.

Historians have debated whether economic differences between the mainly industrial North and the mainly agricultural South helped cause the war. Most historians now disagree with the economic determinism of historian Charles A. Beard in the 1920s, and emphasize that Northern and Southern economies were largely complementary. While socially different, the sections economically benefited each other.

Slave owners preferred low-cost manual labor with no mechanization. Northern manufacturing interests supported tariffs and protectionism while southern planters demanded free trade. The Democrats in Congress, controlled by Southerners, wrote the tariff laws in the 1830s, 1840s, and 1850s, and kept reducing rates so that the 1857 rates were the lowest since 1816. The Republicans called for an increase in tariffs in the 1860 election. The increases were only enacted in 1861 after Southerners resigned their seats in Congress. The tariff issue was a Northern grievance. However, neo-Confederate writers have claimed it as a Southern grievance. In 1860–61 none of the groups that proposed compromises to head off secession raised the tariff issue. Pamphleteers North and South rarely mentioned the tariff.

Nationalism was a powerful force in the early 19th century, with famous spokesmen such as Andrew Jackson and Daniel Webster. While practically all Northerners supported the Union, Southerners were split between those loyal to the entire United States (called "unionists") and those loyal primarily to the southern region and then the Confederacy. C. Vann Woodward said of the latter group,

Perceived insults to Southern collective honor included the enormous popularity of "Uncle Tom's Cabin" (1852) and the actions of abolitionist John Brown in trying to incite a slave rebellion in 1859. Adding to feelings of Nationalism and honor was the pervasive sense of cultural pessimism for the future of Southern society. Abolitionism, feminism, and socialism all threatened the Southern social order. For some Southern intellectuals, the social rationale for a new nation inaugurated a “proto-fascist” discourse in the United States. 

While the South moved towards a Southern nationalism, leaders in the North were also becoming more nationally minded, and they rejected any notion of splitting the Union. The Republican national electoral platform of 1860 warned that Republicans regarded disunion as treason and would not tolerate it: "We denounce those threats of disunion ... as denying the vital principles of a free government, and as an avowal of contemplated treason, which it is the imperative duty of an indignant people sternly to rebuke and forever silence." The South ignored the warnings: Southerners did not realize how ardently the North would fight to hold the Union together.

The election of Abraham Lincoln in November 1860 was the final trigger for secession. Efforts at compromise, including the "Corwin Amendment" and the "Crittenden Compromise", failed.
Southern leaders feared that Lincoln would stop the expansion of slavery and put it on a course toward extinction. The slave states, which had already become a minority in the House of Representatives, were now facing a future as a perpetual minority in the Senate and Electoral College against an increasingly powerful North. Before Lincoln took office in March 1861, seven slave states had declared their secession and joined to form the Confederacy.

According to Lincoln, the American people had shown that they had been successful in "establishing" and "administering" a republic, but a third challenge faced the nation, "maintaining" a republic based on the people's vote against an attempt to overthrow it.

The election of Lincoln provoked the legislature of South Carolina to call a state convention to consider secession. Prior to the war, South Carolina did more than any other Southern state to advance the notion that a state had the right to nullify federal laws, and even to secede from the United States. The convention summoned unanimously voted to secede on December 20, 1860, and adopted the "Declaration of the Immediate Causes Which Induce and Justify the Secession of South Carolina from the Federal Union". It argued for states' rights for slave owners in the South, but contained a complaint about states' rights in the North in the form of opposition to the Fugitive Slave Act, claiming that Northern states were not fulfilling their federal obligations under the Constitution. The "cotton states" of Mississippi, Florida, Alabama, Georgia, Louisiana, and Texas followed suit, seceding in January and February 1861.

Among the ordinances of secession passed by the individual states, those of three—Texas, Alabama, and Virginia—specifically mentioned the plight of the "slaveholding states" at the hands of northern abolitionists. The rest make no mention of the slavery issue, and are often brief announcements of the dissolution of ties by the legislatures. However, at least four states—South Carolina, Mississippi, Georgia, and Texas—also passed lengthy and detailed explanations of their causes for secession, all of which laid the blame squarely on the movement to abolish slavery and that movement's influence over the politics of the northern states. The southern states believed slaveholding was a constitutional right because of the Fugitive Slave Clause of the Constitution. These states agreed to form a new federal government, the Confederate States of America, on February 4, 1861. They took control of federal forts and other properties within their boundaries with little resistance from outgoing President James Buchanan, whose term ended on March 4, 1861. Buchanan said that the Dred Scott decision was proof that the South had no reason for secession, and that the Union "was intended to be perpetual", but that "The power by force of arms to compel a State to remain in the Union" was not among the "enumerated powers granted to Congress". One quarter of the U.S. Army—the entire garrison in Texas—was surrendered in February 1861 to state forces by its commanding general, David E. Twiggs, who then joined the Confederacy.

As Southerners resigned their seats in the Senate and the House, Republicans were able to pass bills for projects that had been blocked by Southern Senators before the war. These included the Morrill Tariff, land grant colleges (the Morrill Act), a Homestead Act, a transcontinental railroad (the Pacific Railroad Acts), the National Bank Act and the authorization of United States Notes by the Legal Tender Act of 1862. The Revenue Act of 1861 introduced the income tax to help finance the war.

On December 18, 1860, the Crittenden Compromise was proposed to re-establish the Missouri Compromise line by constitutionally banning slavery in territories to the north of the line while guaranteeing it to the south. The adoption of this compromise likely would have prevented the secession of every southern state apart from South Carolina, but Lincoln and the Republicans rejected it. It was then proposed to hold a national referendum on the compromise. The Republicans again rejected the idea, although a majority of both Northerners and Southerners would likely have voted in favor of it. A pre-war February Peace Conference of 1861 met in Washington, proposing a solution similar to that of the Crittenden compromise, it was rejected by Congress. The Republicans proposed an alternative compromise to not interfere with slavery where it existed but the South regarded it as insufficient. Nonetheless, the remaining eight slave states rejected pleas to join the Confederacy following a two-to-one no-vote in Virginia's First Secessionist Convention on April 4, 1861.

On March 4, 1861, Abraham Lincoln was sworn in as President. In his inaugural address, he argued that the Constitution was a "more perfect union" than the earlier Articles of Confederation and Perpetual Union, that it was a binding contract, and called any secession "legally void". He had no intent to invade Southern states, nor did he intend to end slavery where it existed, but said that he would use force to maintain possession of Federal property. The government would make no move to recover post offices, and if resisted, mail delivery would end at state lines. Where popular conditions did not allow peaceful enforcement of Federal law, U.S. marshals and judges would be withdrawn. No mention was made of bullion lost from U.S. mints in Louisiana, Georgia, and North Carolina. He stated that it would be U.S. policy to only collect import duties at its ports; there could be no serious injury to the South to justify armed revolution during his administration. His speech closed with a plea for restoration of the bonds of union, famously calling on "the mystic chords of memory" binding the two regions.

The South sent delegations to Washington and offered to pay for the federal properties and enter into a peace treaty with the United States. Lincoln rejected any negotiations with Confederate agents because he claimed the Confederacy was not a legitimate government, and that making any treaty with it would be tantamount to recognition of it as a sovereign government. Secretary of State William Seward, who at the time saw himself as the real governor or "prime minister" behind the throne of the inexperienced Lincoln, engaged in unauthorized and indirect negotiations that failed. President Lincoln was determined to hold all remaining Union-occupied forts in the Confederacy: Fort Monroe in Virginia, Fort Pickens, Fort Jefferson and Fort Taylor in Florida, and Fort Sumter – located at the cockpit of secession in Charleston, South Carolina.

Fort Sumter was located in the middle of the harbor of Charleston, South Carolina. Its garrison recently moved there to avoid incidents with local militias in the streets of the city. Lincoln told its commander, Maj. Anderson to hold on until fired upon. Confederate president Jefferson Davis ordered the surrender of the fort. Anderson gave a conditional reply that the Confederate government rejected, and Davis ordered General P. G. T. Beauregard to attack the fort before a relief expedition could arrive. He bombarded Fort Sumter on April 12–13, forcing its capitulation.

The attack on Fort Sumter rallied the North to the defense of American nationalism. Historian Allan Nevins underscored the significance of the event:

Union leaders incorrectly assumed that only a minority of Southerners were in favor of secession and that there were large numbers of southern Unionists that could be counted on. Had Northerners realized that most Southerners favored secession, they might have hesitated at attempting the enormous task of conquering a united South.

Lincoln called on all the states to send forces to recapture the fort and other federal properties. The scale of the rebellion appeared to be small, so he called for only 75,000 volunteers for 90 days. The governor of Massachusetts had state regiments on trains headed south the next day. In western Missouri, local secessionists seized Liberty Arsenal. On May 3, 1861, Lincoln called for an additional 42,000 volunteers for a period of three years.

Four states in the middle and upper South had repeatedly rejected Confederate overtures, but now Virginia, Tennessee, Arkansas, and North Carolina refused to send forces against their neighbors, declared their secession, and joined the Confederacy. To reward Virginia, the Confederate capital was moved to Richmond.

Maryland, Delaware, Missouri, and Kentucky were slave states that were opposed to both secession and coercing the South. West Virginia then joined them as an additional border state after it separated from Virginia and became a state of the Union in 1863.

Maryland's territory surrounded the United States' capital of Washington, D.C., and could cut it off from the North. It had numerous anti-Lincoln officials who tolerated anti-army rioting in Baltimore and the burning of bridges, both aimed at hindering the passage of troops to the South. Maryland's legislature voted overwhelmingly (53–13) to stay in the Union, but also rejected hostilities with its southern neighbors, voting to close Maryland's rail lines to prevent them from being used for war. Lincoln responded by establishing martial law and unilaterally suspending habeas corpus in Maryland, along with sending in militia units from the North. Lincoln rapidly took control of Maryland and the District of Columbia by seizing many prominent figures, including arresting 1/3 of the members of the Maryland General Assembly on the day it reconvened. All were held without trial, ignoring a ruling by the Chief Justice of the U.S. Supreme Court Roger Taney, a Maryland native, that only Congress (and not the president) could suspend habeas corpus (Ex parte Merryman). Indeed, federal troops imprisoned a prominent Baltimore newspaper editor, Frank Key Howard, Francis Scott Key's grandson, after he criticized Lincoln in an editorial for ignoring the Supreme Court Chief Justice's ruling.

In Missouri, an elected convention on secession voted decisively to remain within the Union. When pro-Confederate Governor Claiborne F. Jackson called out the state militia, it was attacked by federal forces under General Nathaniel Lyon, who chased the governor and the rest of the State Guard to the southwestern corner of the state ("see also": Missouri secession). In the resulting vacuum, the convention on secession reconvened and took power as the Unionist provisional government of Missouri.

Kentucky did not secede; for a time, it declared itself neutral. When Confederate forces entered the state in September 1861, neutrality ended and the state reaffirmed its Union status, while trying to maintain slavery. During a brief invasion by Confederate forces in 1861, Confederate sympathizers organized a secession convention, formed the shadow Confederate Government of Kentucky, inaugurated a governor, and gained recognition from the Confederacy. Its jurisdiction extended only as far as Confederate battle lines in the Commonwealth and went into exile for good after October 1862.

After Virginia's secession, a Unionist government in Wheeling asked 48 counties to vote on an ordinance to create a new state on October 24, 1861. A voter turnout of 34 percent approved the statehood bill (96 percent approving). The inclusion of 24 secessionist counties in the state and the ensuing guerrilla war engaged about 40,000 Federal troops for much of the war. Congress admitted West Virginia to the Union on June 20, 1863. West Virginia provided about 20,000–22,000 soldiers to both the Confederacy and the Union.

A Unionist secession attempt occurred in East Tennessee, but was suppressed by the Confederacy, which arrested over 3,000 men suspected of being loyal to the Union. They were held without trial.

The Civil War was a contest marked by the ferocity and frequency of battle. Over four years, 237 named battles were fought, as were many more minor actions and skirmishes, which were often characterized by their bitter intensity and high casualties. In his book "The American Civil War", John Keegan writes that "The American Civil War was to prove one of the most ferocious wars ever fought". In many cases, without geographic objectives, the only target for each side was the enemy's soldier.

As the first seven states began organizing a Confederacy in Montgomery, the entire U.S. army numbered 16,000. However, Northern governors had begun to mobilize their militias. The Confederate Congress authorized the new nation up to 100,000 troops sent by governors as early as February. By May, Jefferson Davis was pushing for 100,000 men under arms for one year or the duration, and that was answered in kind by the U.S. Congress.

In the first year of the war, both sides had far more volunteers than they could effectively train and equip. After the initial enthusiasm faded, reliance on the cohort of young men who came of age every year and wanted to join was not enough. Both sides used a draft law—conscription—as a device to encourage or force volunteering; relatively few were drafted and served. The Confederacy passed a draft law in April 1862 for young men aged 18 to 35; overseers of slaves, government officials, and clergymen were exempt. The U.S. Congress followed in July, authorizing a militia draft within a state when it could not meet its quota with volunteers. European immigrants joined the Union Army in large numbers, including 177,000 born in Germany and 144,000 born in Ireland.

When the Emancipation Proclamation went into effect in January 1863, ex-slaves were energetically recruited by the states, and used to meet the state quotas. States and local communities offered higher and higher cash bonuses for white volunteers. Congress tightened the law in March 1863. Men selected in the draft could provide substitutes or, until mid-1864, pay commutation money. Many eligibles pooled their money to cover the cost of anyone drafted. Families used the substitute provision to select which man should go into the army and which should stay home. There was much evasion and overt resistance to the draft, especially in Catholic areas. The draft riot in New York City in July 1863 involved Irish immigrants who had been signed up as citizens to swell the vote of the city's Democratic political machine, not realizing it made them liable for the draft. Of the 168,649 men procured for the Union through the draft, 117,986 were substitutes, leaving only 50,663 who had their personal services conscripted.

In both the North and South, the draft laws were highly unpopular. In the North, some 120,000 men evaded conscription, many of them fleeing to Canada, and another 280,000 soldiers deserted during the war. At least 100,000 Southerners deserted, or about 10 percent. In the South, many men deserted temporarily to take care of their distressed families, then returned to their units. In the North, "bounty jumpers" enlisted to get the generous bonus, deserted, then went back to a second recruiting station under a different name to sign up again for a second bonus; 141 were caught and executed.

From a tiny frontier force in 1860, the Union and Confederate armies had grown into the "largest and most efficient armies in the world" within a few years. European observers at the time dismissed them as amateur and unprofessional, but British historian John Keegan concluded that each outmatched the French, Prussian and Russian armies of the time, and but for the Atlantic, would have threatened any of them with defeat.

The number of women who served as soldiers during the war is estimated at between 400 and 750, although an accurate count is impossible because the women had to disguise themselves as men.

Women also served on the Union hospital ship "Red Rover" and nursed Union and Confederate troops at field hospitals.

Mary Edwards Walker, the only woman to ever receive the Medal of Honor, served in the Union Army and was given the medal for her efforts to treat the wounded during the war. Her name was deleted from the Army Medal of Honor Roll in 1917 (along with over 900 other, male MOH recipients); however, it was restored in 1977.

Perman and Taylor (2010) write that historians are of two minds on why millions of men seemed so eager to fight, suffer and die over four years:

At the start of the civil war, a system of paroles operated. Captives agreed not to fight until they were officially exchanged. Meanwhile, they were held in camps run by their army. They were paid, but they were not allowed to perform any military duties. The system of exchanges collapsed in 1863 when the Confederacy refused to exchange black prisoners. After that, about 56,000 of the 409,000 POWs died in prisons during the war, accounting for nearly 10 percent of the conflict's fatalities.

The small U.S. Navy of 1861 was rapidly enlarged to 6,000 officers and 45,000 men in 1865, with 671 vessels, having a tonnage of 510,396. Its mission was to blockade Confederate ports, take control of the river system, defend against Confederate raiders on the high seas, and be ready for a possible war with the British Royal Navy. Meanwhile, the main riverine war was fought in the West, where a series of major rivers gave access to the Confederate heartland. The U.S. Navy eventually gained control of the Red, Tennessee, Cumberland, Mississippi, and Ohio rivers. In the East, the Navy supplied and moved army forces about, and occasionally shelled Confederate installations.

The Civil War occurred during the early stages of the industrial revolution. Many naval innovations emerged during this time, most notably the advent of the ironclad warship. It began when the Confederacy, knowing they had to meet or match the Union's naval superiority, responded to the Union blockade by building or converting more than 130 vessels, including twenty-six ironclads and floating batteries. Only half of these saw active service. Many were equipped with ram bows, creating "ram fever" among Union squadrons wherever they threatened. But in the face of overwhelming Union superiority and the Union's ironclad warships, they were unsuccessful.

In addition to ocean-going warships coming up the Mississippi, the Union Navy used timberclads, tinclads, and armored gunboats. Shipyards at Cairo, Illinois, and St. Louis built new boats or modified steamboats for action.

The Confederacy experimented with the submarine , which did not work satisfactorily, and with building an ironclad ship, , which was based on rebuilding a sunken Union ship, . On its first foray on March 8, 1862, "Virginia" inflicted significant damage to the Union's wooden fleet, but the next day the first Union ironclad, , arrived to challenge it in the Chesapeake Bay. The resulting three hour Battle of Hampton Roads was a draw, but it proved that ironclads were effective warships. Not long after the battle the Confederacy was forced to scuttle the "Virginia" to prevent its capture, while the Union built many copies of the "Monitor". Lacking the technology and infrastructure to build effective warships, the Confederacy attempted to obtain warships from Britain.

By early 1861, General Winfield Scott had devised the Anaconda Plan to win the war with as little bloodshed as possible. Scott argued that a Union blockade of the main ports would weaken the Confederate economy. Lincoln adopted parts of the plan, but he overruled Scott's caution about 90-day volunteers. Public opinion, however, demanded an immediate attack by the army to capture Richmond.

In April 1861, Lincoln announced the Union blockade of all Southern ports; commercial ships could not get insurance and regular traffic ended. The South blundered in embargoing cotton exports in 1861 before the blockade was effective; by the time they realized the mistake, it was too late. "King Cotton" was dead, as the South could export less than 10 percent of its cotton. The blockade shut down the ten Confederate seaports with railheads that moved almost all the cotton, especially New Orleans, Mobile, and Charleston. By June 1861, warships were stationed off the principal Southern ports, and a year later nearly 300 ships were in service.

British investors built small, fast, steam-driven blockade runners that traded arms and luxuries brought in from Britain through Bermuda, Cuba, and the Bahamas in return for high-priced cotton. Many of the ships were designed for speed and were so small that only a small amount of cotton went out. When the Union Navy seized a blockade runner, the ship and cargo were condemned as a Prize of war and sold, with the proceeds given to the Navy sailors; the captured crewmen were mostly British, and they were released.

The Southern economy nearly collapsed during the war. There were multiple reasons for this: the severe deterioration of food supplies, especially in cities, the failure of Southern railroads, the loss of control of the main rivers, foraging by Northern armies, and the seizure of animals and crops by Confederate armies.

Most historians agree that the blockade was a major factor in ruining the Confederate economy; however, Wise argues that the blockade runners provided just enough of a lifeline to allow Lee to continue fighting for additional months, thanks to fresh supplies of 400,000 rifles, lead, blankets, and boots that the homefront economy could no longer supply.

Surdam argues that the blockade was a powerful weapon that eventually ruined the Southern economy, at the cost of few lives in combat. Practically, the entire Confederate cotton crop was useless (although it was sold to Union traders), costing the Confederacy its main source of income. Critical imports were scarce and the coastal trade was largely ended as well. The measure of the blockade's success was not the few ships that slipped through, but the thousands that never tried it. Merchant ships owned in Europe could not get insurance and were too slow to evade the blockade, so they stopped calling at Confederate ports.

To fight an offensive war, the Confederacy purchased ships from Britain, converted them to warships, and raided American merchant ships in the Atlantic and Pacific oceans. Insurance rates skyrocketed and the American flag virtually disappeared from international waters. However, the same ships were reflagged with European flags and continued unmolested. After the war, the U.S. demanded that Britain pay for the damage done, and Britain paid the U.S. $15 million in 1871.

Although the Confederacy hoped that Britain and France would join them against the Union, this was never likely, and so they instead tried to bring Britain and France in as mediators. The Union, under Lincoln and Secretary of State William H. Seward worked to block this, and threatened war if any country officially recognized the existence of the Confederate States of America. In 1861, Southerners voluntarily embargoed cotton shipments, hoping to start an economic depression in Europe that would force Britain to enter the war to get cotton, but this did not work. Worse, Europe developed other cotton suppliers, which they found superior, hindering the South's recovery after the war.

Cotton diplomacy proved a failure as Europe had a surplus of cotton, while the 1860–62 crop failures in Europe made the North's grain exports of critical importance. It also helped to turn European opinion further away from the Confederacy. It was said that "King Corn was more powerful than King Cotton", as U.S. grain went from a quarter of the British import trade to almost half. When Britain did face a cotton shortage, it was temporary, being replaced by increased cultivation in Egypt and India. Meanwhile, the war created employment for arms makers, ironworkers, and British ships to transport weapons.

Lincoln's administration failed to appeal to European public opinion. Diplomats explained that the United States was not committed to the ending of slavery, and instead repeated legalistic arguments about the unconstitutionality of secession. Confederate representatives, on the other hand, were much more successful by ignoring slavery and instead focusing on their struggle for liberty, their commitment to free trade, and the essential role of cotton in the European economy. The European aristocracy was "absolutely gleeful in pronouncing the American debacle as proof that the entire experiment in popular government had failed. European government leaders welcomed the fragmentation of the ascendant American Republic."

U.S. minister to Britain Charles Francis Adams proved particularly adept and convinced Britain not to boldly challenge the blockade. The Confederacy purchased several warships from commercial shipbuilders in Britain (, , , , , and some others). The most famous, the , did considerable damage and led to serious postwar disputes. However, public opinion against slavery created a political liability for politicians in Britain, where the antislavery movement was powerful.

War loomed in late 1861 between the U.S. and Britain over the "Trent" affair, involving the U.S. Navy's boarding of the British ship and seizure of two Confederate diplomats. However, London and Washington were able to smooth over the problem after Lincoln released the two. In 1862, the British considered mediation between North and South, though even such an offer would have risked war with the United States. British Prime Minister Lord Palmerston reportedly read "Uncle Tom's Cabin" three times when deciding on this.

The Union victory in the Battle of Antietam caused them to delay this decision. The Emancipation Proclamation over time would reinforce the political liability of supporting the Confederacy. Despite sympathy for the Confederacy, France's seizure of Mexico ultimately deterred them from war with the Union. Confederate offers late in the war to end slavery in return for diplomatic recognition were not seriously considered by London or Paris. After 1863, the Polish revolt against Russia further distracted the European powers, and ensured that they would remain neutral.

The Eastern theater refers to the military operations east of the Appalachian Mountains, including the states of Virginia, West Virginia, Maryland, and Pennsylvania, the District of Columbia, and the coastal fortifications and seaports of North Carolina.

Maj. Gen. George B. McClellan took command of the Union Army of the Potomac on July 26 (he was briefly general-in-chief of all the Union armies, but was subsequently relieved of that post in favor of Maj. Gen. Henry W. Halleck), and the war began in earnest in 1862. The 1862 Union strategy called for simultaneous advances along four axes:



The primary Confederate force in the Eastern theater was the Army of Northern Virginia. The Army originated as the (Confederate) Army of the Potomac, which was organized on June 20, 1861, from all operational forces in northern Virginia. On July 20 and 21, the Army of the Shenandoah and forces from the District of Harpers Ferry were added. Units from the Army of the Northwest were merged into the Army of the Potomac between March 14 and May 17, 1862. The Army of the Potomac was renamed "Army of Northern Virginia" on March 14. The Army of the Peninsula was merged into it on April 12, 1862.

When Virginia declared its secession in April 1861, Robert E. Lee chose to follow his home state, despite his desire for the country to remain intact and an offer of a senior Union command.

Lee's biographer, Douglas S. Freeman, asserts that the army received its final name from Lee when he issued orders assuming command on June 1, 1862. However, Freeman does admit that Lee corresponded with Brigadier General Joseph E. Johnston, his predecessor in army command, prior to that date and referred to Johnston's command as the Army of Northern Virginia. Part of the confusion results from the fact that Johnston commanded the Department of Northern Virginia (as of October 22, 1861) and the name Army of Northern Virginia can be seen as an informal consequence of its parent department's name. Jefferson Davis and Johnston did not adopt the name, but it is clear that the organization of units as of March 14 was the same organization that Lee received on June 1, and thus it is generally referred to today as the Army of Northern Virginia, even if that is correct only in retrospect. On July 4 at Harper's Ferry, Colonel Thomas J. Jackson assigned Jeb Stuart to command all the cavalry companies of the Army of the Shenandoah. He eventually commanded the Army of Northern Virginia's cavalry.


In one of the first highly visible battles, in July 1861, a march by Union troops under the command of Maj. Gen. Irvin McDowell on the Confederate forces led by Gen. P. G. T. Beauregard near Washington was repulsed at the First Battle of Bull Run (also known as First Manassas). 

The Union had the upper hand at first, nearly pushing confederate forces holding a defensive position into a rout, but Confederate reinforcements under. Joseph E. Johnston arrived from the Shenandoah Valley by railroad, and the course of the battle quickly changed. A brigade of Virginians under the relatively unknown brigadier general from the Virginia Military Institute, Thomas J. Jackson, stood its ground, which resulted in Jackson receiving his famous nickname, "Stonewall".


Upon the strong urging of President Lincoln to begin offensive operations, McClellan attacked Virginia in the spring of 1862 by way of the peninsula between the York River and James River, southeast of Richmond. McClellan's army reached the gates of Richmond in the Peninsula Campaign,

Also in the spring of 1862, in the Shenandoah Valley, Stonewall Jackson led his Valley Campaign. Employing audacity and rapid, unpredictable movements on interior lines, Jackson's 17,000 men marched 646 miles (1,040 km) in 48 days and won several minor battles as they successfully engaged three Union armies (52,000 men), including those of Nathaniel P. Banks and John C. Fremont, preventing them from reinforcing the Union offensive against Richmond. The swiftness of Jackson's men earned them the nickname of "foot cavalry".

Johnston halted McClellan's advance at the Battle of Seven Pines, but he was wounded in the battle, and Robert E. Lee assumed his position of command. General Lee and top subordinates James Longstreet and Stonewall Jackson defeated McClellan in the Seven Days Battles and forced his retreat.

The Northern Virginia Campaign, which included the Second Battle of Bull Run, ended in yet another victory for the South. McClellan resisted General-in-Chief Halleck's orders to send reinforcements to John Pope's Union Army of Virginia, which made it easier for Lee's Confederates to defeat twice the number of combined enemy troops.

Emboldened by Second Bull Run, the Confederacy made its first invasion of the North with the Maryland Campaign. General Lee led 45,000 men of the Army of Northern Virginia across the Potomac River into Maryland on September 5. Lincoln then restored Pope's troops to McClellan. McClellan and Lee fought at the Battle of Antietam near Sharpsburg, Maryland, on September 17, 1862, the bloodiest single day in United States military history. Lee's army, checked at last, returned to Virginia before McClellan could destroy it. Antietam is considered a Union victory because it halted Lee's invasion of the North and provided an opportunity for Lincoln to announce his Emancipation Proclamation.

When the cautious McClellan failed to follow up on Antietam, he was replaced by Maj. Gen. Ambrose Burnside. Burnside was soon defeated at the Battle of Fredericksburg on December 13, 1862, when more than 12,000 Union soldiers were killed or wounded during repeated futile frontal assaults against Marye's Heights. After the battle, Burnside was replaced by Maj. Gen. Joseph Hooker.

Hooker, too, proved unable to defeat Lee's army; despite outnumbering the Confederates by more than two to one, his Chancellorsville Campaign proved ineffective and he was humiliated in the Battle of Chancellorsville in May 1863. Chancellorsville is known as Lee's "perfect battle" because his risky decision to divide his army in the presence of a much larger enemy force resulted in a significant Confederate victory. Gen. Stonewall Jackson was shot in the arm by accidental friendly fire during the battle and subsequently died of complications. Lee famously said "He has lost his left arm; but I have lost my right arm."

The fiercest fighting of the battle—and the second bloodiest day of the Civil War—occurred on May 3 as Lee launched multiple attacks against the Union position at Chancellorsville. That same day, John Sedgwick advanced across the Rappahannock River, defeated the small Confederate force at Marye's Heights in the Second Battle of Fredericksburg, and then moved to the west. The Confederates fought a successful delaying action at the Battle of Salem Church.


Gen. Hooker was replaced by Maj. Gen. George Meade during Lee's second invasion of the North, in June. Meade defeated Lee at the Battle of Gettysburg (July 1 to 3, 1863). This was the bloodiest battle of the war, and has been called the war's turning point. Pickett's Charge on July 3 is often considered the high-water mark of the Confederacy because it signaled the collapse of serious Confederate threats of victory. Lee's army suffered 28,000 casualties (versus Meade's 23,000). However, Lincoln was angry that Meade failed to intercept Lee's retreat.

The Western theater refers to military operations between the Appalachian Mountains and the Mississippi River, including the states of Alabama, Georgia, Florida, Mississippi, North Carolina, Kentucky, South Carolina and Tennessee, as well as parts of Louisiana.


The primary Union forces in the Western theater were the Army of the Tennessee and the Army of the Cumberland, named for the two rivers, the Tennessee River and Cumberland River. After Meade's inconclusive fall campaign, Lincoln turned to the Western Theater for new leadership. At the same time, the Confederate stronghold of Vicksburg surrendered, giving the Union control of the Mississippi River, permanently isolating the western Confederacy, and producing the new leader Lincoln needed, Ulysses S. Grant.

The primary Confederate force in the Western theater was the Army of Tennessee. The army was formed on November 20, 1862, when General Braxton Bragg renamed the former Army of Mississippi. While the Confederate forces had numerous successes in the Eastern Theater, they were defeated many times in the West.


The Union's key strategist and tactician in the West was Ulysses S. Grant, who won victories at Forts Henry (February 6, 1862) and Donelson (February 11 to 16, 1862), earning him the nickname of "Unconditional Surrender" Grant, by which the Union seized control of the Tennessee and Cumberland Rivers. Nathan Bedford Forrest rallied nearly 4,000 Confederate troops and led them to escape across the Cumberland. Nashville and central Tennessee thus fell to the Union, leading to attrition of local food supplies and livestock and a breakdown in social organization.

Leonidas Polk's invasion of Columbus ended Kentucky's policy of neutrality and turned it against the Confederacy. Grant used river transport and Andrew Foote's gunboats of the Western Flotilla to threaten the Confederacy's "Gibraltar of the West" at Columbus, Kentucky. Although rebuffed at Belmont, Grant cut off Columbus. The Confederates, lacking their own gunboats, were forced to retreat and the Union took control of western Kentucky and opened Tennessee in March 1862.
At the Battle of Shiloh (Pittsburg Landing), in Tennessee in April 1862, the Confederates made a surprise attack that pushed Union forces against the river as night fell. Overnight, the Navy landed additional reinforcements, and Grant counter-attacked. Grant and the Union won a decisive victory—the first battle with the high casualty rates that would repeat over and over. The Confederates lost Albert Sidney Johnston, considered their finest general before the emergence of Lee.


One of the early Union objectives in the war was the capture of the Mississippi River, in order to cut the Confederacy in half. The Mississippi River was opened to Union traffic to the southern border of Tennessee with the taking of Island No. 10 and New Madrid, Missouri, and then Memphis, Tennessee.

In April 1862, the Union Navy captured New Orleans. "The key to the river was New Orleans, the South's largest port [and] greatest industrial center." U.S. Naval forces under Farragut ran past Confederate defenses south of New Orleans. Confederate forces abandoned the city, giving the Union a critical anchor in the deep South. which allowed Union forces to begin moving up the Mississippi. Memphis fell to Union forces on June 6, 1862, and became a key base for further advances south along the Mississippi River. Only the fortress city of Vicksburg, Mississippi, prevented Union control of the entire river.

Bragg's second invasion of Kentucky in the Confederate Heartland Offensive included initial successes such as Kirby Smith's triumph at the Battle of Richmond and the capture of the Kentucky capital of Frankfort on September 3, 1862. However, the campaign ended with a meaningless victory over Maj. Gen. Don Carlos Buell at the Battle of Perryville. Bragg was forced to end his attempt at invading Kentucky and retreat due to lack of logistical support and lack of infantry recruits for the Confederacy in that state.

Bragg was narrowly defeated by Maj. Gen. William Rosecrans at the Battle of Stones River in Tennessee, the culmination of the Stones River Campaign.

Naval forces assisted Grant in the long, complex Vicksburg Campaign that resulted in the Confederates surrendering at the Battle of Vicksburg in July 1863, which cemented Union control of the Mississippi River and is considered one of the turning points of the war.

The one clear Confederate victory in the West was the Battle of Chickamauga. After Rosecrans successful Tullahoma Campaign, Bragg, reinforced by Lt. Gen. James Longstreet's corps (from Lee's army in the east), defeated Rosecrans, despite the heroic defensive stand of Maj. Gen. George Henry Thomas.

Rosecrans retreated to Chattanooga, which Bragg then besieged in the Chattanooga Campaign. Grant marched to the relief of Rosecrans and defeated Bragg at the Third Battle of Chattanooga, eventually causing Longstreet to abandon his Knoxville Campaign and driving Confederate forces out of Tennessee and opening a route to Atlanta and the heart of the Confederacy.

The Trans-Mississippi theater refers to military operations west of the Mississippi River, not including the areas bordering the Pacific Ocean.


The first battle of the Trans-Mississippi theater was the Battle of Wilson's Creek. The Confederates were driven from Missouri early in the war as a result of the Battle of Pea Ridge.

Extensive guerrilla warfare characterized the trans-Mississippi region, as the Confederacy lacked the troops and the logistics to support regular armies that could challenge Union control. Roving Confederate bands such as Quantrill's Raiders terrorized the countryside, striking both military installations and civilian settlements. The "Sons of Liberty" and "Order of the American Knights" attacked pro-Union people, elected officeholders, and unarmed uniformed soldiers. These partisans could not be entirely driven out of the state of Missouri until an entire regular Union infantry division was engaged. By 1864, these violent activities harmed the nationwide anti-war movement organizing against the re-election of Lincoln. Missouri not only stayed in the Union, but Lincoln took 70 percent of the vote for re-election.

Numerous small-scale military actions south and west of Missouri sought to control Indian Territory and New Mexico Territory for the Union. The Battle of Glorieta Pass was the decisive battle of the New Mexico Campaign. The Union repulsed Confederate incursions into New Mexico in 1862, and the exiled Arizona government withdrew into Texas. In the Indian Territory, civil war broke out within tribes. About 12,000 Indian warriors fought for the Confederacy, and smaller numbers for the Union. The most prominent Cherokee was Brigadier General Stand Watie, the last Confederate general to surrender.

After the fall of Vicksburg in July 1863, General Kirby Smith in Texas was informed by Jefferson Davis that he could expect no further help from east of the Mississippi River. Although he lacked resources to beat Union armies, he built up a formidable arsenal at Tyler, along with his own Kirby Smithdom economy, a virtual "independent fiefdom" in Texas, including railroad construction and international smuggling. The Union in turn did not directly engage him. Its 1864 Red River Campaign to take Shreveport, Louisiana, was a failure and Texas remained in Confederate hands throughout the war.

The Lower Seaboard theater refers to military and naval operations that occurred near the coastal areas of the Southeast (Alabama, Florida, Louisiana, Mississippi, South Carolina, and Texas) as well as southern part of the Mississippi River (Port Hudson and south). Union Naval activities were dictated by the Anaconda Plan.

One of the earliest battles of the war was fought at Port Royal Sound, south of Charleston. Much of the war along the South Carolina coast concentrated on capturing Charleston. In attempting to capture Charleston, the Union military tried two approaches, by land over James or Morris Islands or through the harbor. However, the Confederates were able to drive back each Union attack. One of the most famous of the land attacks was the Second Battle of Fort Wagner, in which the 54th Massachusetts Infantry took part. The Federals suffered a serious defeat in this battle, losing 1,500 men while the Confederates lost only 175.

Fort Pulaski on the Georgia coast was an early target for the Union navy. Following the capture of Port Royal, an expedition was organized with engineer troops under the command of Captain Quincy A. Gillmore, forcing a Confederate surrender. The Union army occupied the fort for the rest of the war after making repair.


In April 1862, a Union naval task force commanded by Commander David D. Porter attacked Forts Jackson and St. Philip, which guarded the river approach to New Orleans from the south. While part of the fleet bombarded the forts, other vessels forced a break in the obstructions in the river and enabled the rest of the fleet to steam upriver to the city. A Union army force commanded by Major General Benjamin Butler landed near the forts and forced their surrender. Butler's controversial command of New Orleans earned him the nickname "Beast".

The following year, the Union Army of the Gulf commanded by Major General Nathaniel P. Banks laid siege to Port Hudson for nearly eight weeks, the longest siege in US military history. The Confederates attempted to defend with the Bayou Teche Campaign, but surrendered after Vicksburg. These two surrenders gave the Union control over the entire Mississippi.

Several small skirmishes were fought in Florida, but no major battles. The biggest was the Battle of Olustee in early 1864.

The Pacific Coast theater refers to military operations on the Pacific Ocean and in the states and Territories west of the Continental Divide.

At the beginning of 1864, Lincoln made Grant commander of all Union armies. Grant made his headquarters with the Army of the Potomac, and put Maj. Gen. William Tecumseh Sherman in command of most of the western armies. Grant understood the concept of total war and believed, along with Lincoln and Sherman, that only the utter defeat of Confederate forces and their economic base would end the war. This was total war not in killing civilians but rather in taking provisions and forage and destroying homes, farms, and railroads, that Grant said "would otherwise have gone to the support of secession and rebellion. This policy I believe exercised a material influence in hastening the end." Grant devised a coordinated strategy that would strike at the entire Confederacy from multiple directions. Generals George Meade and Benjamin Butler were ordered to move against Lee near Richmond, General Franz Sigel (and later Philip Sheridan) were to attack the Shenandoah Valley, General Sherman was to capture Atlanta and march to the sea (the Atlantic Ocean), Generals George Crook and William W. Averell were to operate against railroad supply lines in West Virginia, and Maj. Gen. Nathaniel P. Banks was to capture Mobile, Alabama.
Grant's army set out on the Overland Campaign with the goal of drawing Lee into a defense of Richmond, where they would attempt to pin down and destroy the Confederate army. The Union army first attempted to maneuver past Lee and fought several battles, notably at the Wilderness, Spotsylvania, and Cold Harbor. These battles resulted in heavy losses on both sides, and forced Lee's Confederates to fall back repeatedly. At the Battle of Yellow Tavern, the Confederates lost Jeb Stuart.

An attempt to outflank Lee from the south failed under Butler, who was trapped inside the Bermuda Hundred river bend. Each battle resulted in setbacks for the Union that mirrored what they had suffered under prior generals, though unlike those prior generals, Grant fought on rather than retreat. Grant was tenacious and kept pressing Lee's Army of Northern Virginia back to Richmond. While Lee was preparing for an attack on Richmond, Grant unexpectedly turned south to cross the James River and began the protracted Siege of Petersburg, where the two armies engaged in trench warfare for over nine months.
Grant finally found a commander, General Philip Sheridan, aggressive enough to prevail in the Valley Campaigns of 1864. Sheridan was initially repelled at the Battle of New Market by former U.S. Vice President and Confederate Gen. John C. Breckinridge. The Battle of New Market was the Confederacy's last major victory of the war, and included a charge by teenage VMI cadets. After redoubling his efforts, Sheridan defeated Maj. Gen. Jubal A. Early in a series of battles, including a final decisive defeat at the Battle of Cedar Creek. Sheridan then proceeded to destroy the agricultural base of the Shenandoah Valley, a strategy similar to the tactics Sherman later employed in Georgia.

Meanwhile, Sherman maneuvered from Chattanooga to Atlanta, defeating Confederate Generals Joseph E. Johnston and John Bell Hood along the way. The fall of Atlanta on September 2, 1864, guaranteed the reelection of Lincoln as president. Hood left the Atlanta area to swing around and menace Sherman's supply lines and invade Tennessee in the Franklin–Nashville Campaign. Union Maj. Gen. John Schofield defeated Hood at the Battle of Franklin, and George H. Thomas dealt Hood a massive defeat at the Battle of Nashville, effectively destroying Hood's army.

Leaving Atlanta, and his base of supplies, Sherman's army marched with an unknown destination, laying waste to about 20 percent of the farms in Georgia in his "March to the Sea". He reached the Atlantic Ocean at Savannah, Georgia, in December 1864. Sherman's army was followed by thousands of freed slaves; there were no major battles along the March. Sherman turned north through South Carolina and North Carolina to approach the Confederate Virginia lines from the south, increasing the pressure on Lee's army.

Lee's army, thinned by desertion and casualties, was now much smaller than Grant's. One last Confederate attempt to break the Union hold on Petersburg failed at the decisive Battle of Five Forks (sometimes called "the Waterloo of the Confederacy") on April 1. This meant that the Union now controlled the entire perimeter surrounding Richmond-Petersburg, completely cutting it off from the Confederacy. Realizing that the capital was now lost, Lee decided to evacuate his army. The Confederate capital fell to the Union XXV Corps, composed of black troops. The remaining Confederate units fled west after a defeat at Sayler's Creek.

Initially, Lee did not intend to surrender, but planned to regroup at the village of Appomattox Court House, where supplies were to be waiting, and then continue the war. Grant chased Lee and got in front of him, so that when Lee's army reached Appomattox Court House, they were surrounded. After an initial battle, Lee decided that the fight was now hopeless, and surrendered his Army of Northern Virginia on April 9, 1865, at the McLean House. In an untraditional gesture and as a sign of Grant's respect and anticipation of peacefully restoring Confederate states to the Union, Lee was permitted to keep his sword and his horse, Traveller.

On April 14, 1865, President Lincoln was shot by John Wilkes Booth, a Southern sympathizer. Lincoln died early the next morning, and Andrew Johnson became the president. Meanwhile, Confederate forces across the South surrendered as news of Lee's surrender reached them. On April 26, 1865, General Joseph E. Johnston surrendered nearly 90,000 men of the Army of Tennessee to Major General William Tecumseh Sherman at the Bennett Place near present-day Durham, North Carolina. It proved to be the largest surrender of Confederate forces, effectively bringing the war to an end. President Johnson officially declared a virtual end to the insurrection on May 9, 1865; President Jefferson Davis was captured the following day. On June 2, Kirby Smith officially surrendered his troops in the Trans-Mississippi Department. On June 23, Cherokee leader Stand Watie became the last Confederate general to surrender his forces.

The causes of the war, the reasons for its outcome, and even the name of the war itself are subjects of lingering contention today. The North and West grew rich while the once-rich South became poor for a century. The national political power of the slaveowners and rich Southerners ended. Historians are less sure about the results of the postwar Reconstruction, especially regarding the second-class citizenship of the Freedmen and their poverty.

Historians have debated whether the Confederacy could have won the war. Most scholars, including James McPherson, argue that Confederate victory was at least possible. McPherson argues that the North's advantage in population and resources made Northern victory likely but not guaranteed. He also argues that if the Confederacy had fought using unconventional tactics, they would have more easily been able to hold out long enough to exhaust the Union.

Confederates did not need to invade and hold enemy territory to win, but only needed to fight a defensive war to convince the North that the cost of winning was too high. The North needed to conquer and hold vast stretches of enemy territory and defeat Confederate armies to win. Lincoln was not a military dictator, and could continue to fight the war only as long as the American public supported a continuation of the war. The Confederacy sought to win independence by out-lasting Lincoln; however, after Atlanta fell and Lincoln defeated McClellan in the election of 1864, all hope for a political victory for the South ended. At that point, Lincoln had secured the support of the Republicans, War Democrats, the border states, emancipated slaves, and the neutrality of Britain and France. By defeating the Democrats and McClellan, he also defeated the Copperheads and their peace platform.
Many scholars argue that the Union held an insurmountable long-term advantage over the Confederacy in industrial strength and population. Confederate actions, they argue, only delayed defeat. Civil War historian Shelby Foote expressed this view succinctly: "I think that the North fought that war with one hand behind its back ... If there had been more Southern victories, and a lot more, the North simply would have brought that other hand out from behind its back. I don't think the South ever had a chance to win that War."

A minority view among historians is that the Confederacy lost because, as E. Merton Coulter put it, "people did not will hard enough and long enough to win." According to Charles H. Wilson, in "The Collapse of the Confederacy", "internal conflict should figure prominently in any explanation of Confederate defeat." Marxist historian Armstead Robinson agrees, pointing to a class conflict in the Confederate army between the slave owners and the larger number of non-owners. He argues that the non-owner soldiers grew embittered about fighting to preserve slavery, and fought less enthusiastically. He attributes the major Confederate defeats in 1863 at Vicksburg and Missionary Ridge to this class conflict. However, most historians reject the argument. James M. McPherson, after reading thousands of letters written by Confederate soldiers, found strong patriotism that continued to the end; they truly believed they were fighting for freedom and liberty. Even as the Confederacy was visibly collapsing in 1864–65, he says most Confederate soldiers were fighting hard. Historian Gary Gallagher cites General Sherman who in early 1864 commented, "The devils seem to have a determination that cannot but be admired." Despite their loss of slaves and wealth, with starvation looming, Sherman continued, "yet I see no sign of let up—some few deserters—plenty tired of war, but the masses determined to fight it out."

Also important were Lincoln's eloquence in rationalizing the national purpose and his skill in keeping the border states committed to the Union cause. The Emancipation Proclamation was an effective use of the President's war powers. The Confederate government failed in its attempt to get Europe involved in the war militarily, particularly Britain and France. Southern leaders needed to get European powers to help break up the blockade the Union had created around the Southern ports and cities. Lincoln's naval blockade was 95 percent effective at stopping trade goods; as a result, imports and exports to the South declined significantly. The abundance of European cotton and Britain's hostility to the institution of slavery, along with Lincoln's Atlantic and Gulf of Mexico naval blockades, severely decreased any chance that either Britain or France would enter the war.

Historian Don Doyle has argued that the Union victory had a major impact on the course of world history. The Union victory energized popular democratic forces. A Confederate victory, on the other hand, would have meant a new birth of slavery, not freedom. Historian Fergus Bordewich, following Doyle, argues that:

Scholars have debated what the effects of the war were on political and economic power in the South. The prevailing view is that the southern planter elite retained its powerful position in the South. However, a 2017 study challenges this, noting that while some Southern elites retained their economic status, the turmoil of the 1860s created greater opportunities for economic mobility in the South than in the North.

The war resulted in at least 1,030,000 casualties (3 percent of the population), including about 620,000 soldier deaths—two-thirds by disease, and 50,000 civilians. Binghamton University historian J. David Hacker believes the number of soldier deaths was approximately 750,000, 20 percent higher than traditionally estimated, and possibly as high as 850,000. The war accounted for more American deaths than in all other U.S. wars combined.

Based on 1860 census figures, 8 percent of all white men aged 13 to 43 died in the war, including 6 percent in the North and 18 percent in the South. About 56,000 soldiers died in prison camps during the War. An estimated 60,000 men lost limbs in the war.

Union army dead, amounting to 15 percent of the over two million who served, was broken down as follows:

In addition there were 4,523 deaths in the Navy (2,112 in battle) and 460 in the Marines (148 in battle).

Black troops made up 10 percent of the Union death toll, they amounted to 15 percent of disease deaths but less than 3 percent of those killed in battle. Losses among African Americans were high, in the last year and a half and from all reported casualties, approximately 20 percent of all African Americans enrolled in the military lost their lives during the Civil War. Notably, their mortality rate was significantly higher than white soldiers:
Confederate records compiled by historian William F. Fox list 74,524 killed and died of wounds and 59,292 died of disease. Including Confederate estimates of battle losses where no records exist would bring the Confederate death toll to 94,000 killed and died of wounds. Fox complained, however, that records were incomplete, especially during the last year of the war, and that battlefield reports likely under-counted deaths (many men counted as wounded in battlefield reports subsequently died of their wounds). Thomas L. Livermore, using Fox's data, put the number of Confederate non-combat deaths at 166,000, using the official estimate of Union deaths from disease and accidents and a comparison of Union and Confederate enlistment records, for a total of 260,000 deaths. However, this excludes the 30,000 deaths of Confederate troops in prisons, which would raise the minimum number of deaths to 290,000.

The United States National Park Service uses the following figures in its official tally of war losses:

Union: 853,838

Confederate: 914,660

While the figures of 360,000 army deaths for the Union and 260,000 for the Confederacy remained commonly cited, they are incomplete. In addition to many Confederate records being missing, partly as a result of Confederate widows not reporting deaths due to being ineligible for benefits, both armies only counted troops who died during their service, and not the tens of thousands who died of wounds or diseases after being discharged. This often happened only a few days or weeks later. Francis Amasa Walker, superintendent of the 1870 census, used census and surgeon general data to estimate a minimum of 500,000 Union military deaths and 350,000 Confederate military deaths, for a total death toll of 850,000 soldiers. While Walker's estimates were originally dismissed because of the 1870 census's undercounting, it was later found that the census was only off by 6.5%, and that the data Walker used would be roughly accurate.

Analyzing the number of dead by using census data to calculate the deviation of the death rate of men of fighting age from the norm suggests that at least 627,000 and at most 888,000, but most likely 761,000 soldiers, died in the war. This would break down to approximately 350,000 Confederate and 411,000 Union military deaths, going by the proportion of Union to Confederate battle losses.

Deaths among former slaves has proven much harder to estimate, due to the lack of reliable census data at the time, though they were known to be considerable, as former slaves were set free or escaped in massive numbers in an area where the Union army did not have sufficient shelter, doctors, or food for them. University of Connecticut Professor James Downs states that tens to hundreds of thousands of slaves died during the war from disease, starvation, or exposure, and that if these deaths are counted in the war's total, the death toll would exceed 1 million.

Losses were far higher than during the recent defeat of Mexico, which saw roughly thirteen thousand American deaths, including fewer than two thousand killed in battle, between 1846 and 1848. One reason for the high number of battle deaths during the war was the continued use of tactics similar to those of the Napoleonic Wars at the turn of the century, such as charging. With the advent of more accurate rifled barrels, Minié balls, and (near the end of the war for the Union army) repeating firearms such as the Spencer Repeating Rifle and the Henry Repeating Rifle, soldiers were mowed down when standing in lines in the open. This led to the adoption of trench warfare, a style of fighting that defined much of World War I.

The wealth amassed in slaves and slavery for the Confederacy's 3.5 million blacks effectively ended when Union armies arrived; they were nearly all freed by the Emancipation Proclamation. Slaves in the border states and those located in some former Confederate territory occupied before the Emancipation Proclamation were freed by state action or (on December 6, 1865) by the Thirteenth Amendment.

The war destroyed much of the wealth that had existed in the South. All accumulated investment Confederate bonds was forfeit; most banks and railroads were bankrupt. Income per person in the South dropped to less than 40 percent of that of the North, a condition that lasted until well into the 20th century. Southern influence in the U.S. federal government, previously considerable, was greatly diminished until the latter half of the 20th century. The full restoration of the Union was the work of a highly contentious postwar era known as Reconstruction.

While not all Southerners saw themselves as fighting to preserve slavery, most of the officers and over a third of the rank and file in Lee's army had close family ties to slavery. To Northerners, in contrast, the motivation was primarily to preserve the Union, not to abolish slavery. Abraham Lincoln consistently made preserving the Union the central goal of the war, though he increasingly saw slavery as a crucial issue and made ending it an additional goal. Lincoln's decision to issue the Emancipation Proclamation angered both Peace Democrats ("Copperheads") and War Democrats, but energized most Republicans. By warning that free blacks would flood the North, Democrats made gains in the 1862 elections, but they did not gain control of Congress. The Republicans' counterargument that slavery was the mainstay of the enemy steadily gained support, with the Democrats losing decisively in the 1863 elections in the northern state of Ohio when they tried to resurrect anti-black sentiment.

The Emancipation Proclamation enabled African-Americans, both free blacks and escaped slaves, to join the Union Army. About 190,000 volunteered, further enhancing the numerical advantage the Union armies enjoyed over the Confederates, who did not dare emulate the equivalent manpower source for fear of fundamentally undermining the legitimacy of slavery.

During the Civil War, sentiment concerning slaves, enslavement and emancipation in the United States was divided. In 1861, Lincoln worried that premature attempts at emancipation would mean the loss of the border states, and that "to lose Kentucky is nearly the same as to lose the whole game." Copperheads and some War Democrats opposed emancipation, although the latter eventually accepted it as part of total war needed to save the Union.

At first, Lincoln reversed attempts at emancipation by Secretary of War Simon Cameron and Generals John C. Frémont (in Missouri) and David Hunter (in South Carolina, Georgia and Florida) to keep the loyalty of the border states and the War Democrats. Lincoln warned the border states that a more radical type of emancipation would happen if his gradual plan based on compensated emancipation and voluntary colonization was rejected. But only the District of Columbia accepted Lincoln's gradual plan, which was enacted by Congress. When Lincoln told his cabinet about his proposed emancipation proclamation, Seward advised Lincoln to wait for a victory before issuing it, as to do otherwise would seem like "our last shriek on the retreat". Lincoln laid the groundwork for public support in an open letter published in abolitionist Horace Greeley's newspaper.

In September 1862, the Battle of Antietam provided this opportunity, and the subsequent War Governors' Conference added support for the proclamation. Lincoln issued his preliminary Emancipation Proclamation on September 22, 1862, and his final Emancipation Proclamation on January 1, 1863. In his letter to Albert G. Hodges, Lincoln explained his belief that "If slavery is not wrong, nothing is wrong ... And yet I have never understood that the Presidency conferred upon me an unrestricted right to act officially upon this judgment and feeling ... I claim not to have controlled events, but confess plainly that events have controlled me."

Lincoln's moderate approach succeeded in inducing border states, War Democrats and emancipated slaves to fight for the Union. The Union-controlled border states (Kentucky, Missouri, Maryland, Delaware and West Virginia) and Union-controlled regions around New Orleans, Norfolk and elsewhere, were not covered by the Emancipation Proclamation. All abolished slavery on their own, except Kentucky and Delaware.

Since the Emancipation Proclamation was based on the President's war powers, it only included territory held by Confederates at the time. However, the Proclamation became a symbol of the Union's growing commitment to add emancipation to the Union's definition of liberty. The Emancipation Proclamation greatly reduced the Confederacy's hope of getting aid from Britain or France. By late 1864, Lincoln was playing a leading role in getting Congress to vote for the Thirteenth Amendment, which made emancipation universal and permanent.

In "Texas v. White", the United States Supreme Court ruled that Texas had remained a state ever since it first joined the Union, despite claims that it joined the Confederate States; the court further held that the Constitution did not permit states to unilaterally secede from the United States, and that the ordinances of secession, and all the acts of the legislatures within seceding states intended to give effect to such ordinances, were "absolutely null", under the constitution.

Reconstruction began during the war, with the Emancipation Proclamation of January 1, 1863, and it continued until 1877. It comprised multiple complex methods to resolve the outstanding issues of the war's aftermath, the most important of which were the three "Reconstruction Amendments" to the Constitution, which remain in effect to the present time: the 13th (1865), the 14th (1868) and the 15th (1870). From the Union perspective, the goals of Reconstruction were to consolidate the Union victory on the battlefield by reuniting the Union; to guarantee a "republican form of government for the ex-Confederate states; and to permanently end slavery—and prevent semi-slavery status.

President Johnson took a lenient approach and saw the achievement of the main war goals as realized in 1865, when each ex-rebel state repudiated secession and ratified the Thirteenth Amendment. Radical Republicans demanded proof that Confederate nationalism was dead and that the slaves were truly free. They came to the fore after the 1866 elections and undid much of Johnson's work. In 1872 the "Liberal Republicans" argued that the war goals had been achieved and that Reconstruction should end. They ran a presidential ticket in 1872 but were decisively defeated. In 1874, Democrats, primarily Southern, took control of Congress and opposed any more reconstruction. The Compromise of 1877 closed with a national consensus that the Civil War had finally ended. With the withdrawal of federal troops, however, whites retook control of every Southern legislature; the Jim Crow period of disenfranchisement and legal segregation was about to begin.

The Civil War would have a huge impact on American politics in the years to come. Many veterans on the both sides were subsequently elected to political office, including five U. S. Presidents: U. S. Grant, Rutherford B. Hayes, James Garfield, Benjamin Harrison, and William McKinley.

The Civil War is one of the central events in American collective memory. There are innumerable statues, commemorations, books and archival collections. The memory includes the home front, military affairs, the treatment of soldiers, both living and dead, in the war's aftermath, depictions of the war in literature and art, evaluations of heroes and villains, and considerations of the moral and political lessons of the war. The last theme includes moral evaluations of racism and slavery, heroism in combat and heroism behind the lines, and the issues of democracy and minority rights, as well as the notion of an "Empire of Liberty" influencing the world.

Professional historians have paid much more attention to the causes of the war, than to the war itself. Military history has largely developed outside academia, leading to a proliferation of studies by non-scholars who nevertheless are familiar with the primary sources and pay close attention to battles and campaigns, and who write for the general public, rather than the scholarly community. Bruce Catton and Shelby Foote are among the best-known writers. Practically every major figure in the war, both North and South, has had a serious biographical study. Deeply religious Southerners saw the hand of God in history, which demonstrated His wrath at their sinfulness, or His rewards for their suffering. Historian Wilson Fallin has examined the sermons of white and black Baptist preachers after the War. Southern white preachers said:
In sharp contrast, Black preachers interpreted the Civil War as:

Memory of the war in the white South crystallized in the myth of the "Lost Cause": that the Confederate cause was a just and heroic one. The myth shaped regional identity and race relations for generations. Alan T. Nolan notes that the Lost Cause was expressly "a rationalization, a cover-up to vindicate the name and fame" of those in rebellion. Some claims revolve around the insignificance of slavery; some appeals highlight cultural differences between North and South; the military conflict by Confederate actors is idealized; in any case, secession was said to be lawful. Nolan argues that the adoption of the Lost Cause perspective facilitated the reunification of the North and the South while excusing the "virulent racism" of the 19th century, sacrificing African-American progress to a white man's reunification. He also deems the Lost Cause "a caricature of the truth. This caricature wholly misrepresents and distorts the facts of the matter" in every instance.

The economic and political-power determinism forcefully presented by Charles A. Beard and Mary R. Beard in "The Rise of American Civilization" (1927) was highly influential among historians and the general public until the civil rights movement of the 1950s and 1960s. The Beards downplayed slavery, abolitionism, and issues of morality. They ignored constitutional issues of states' rights and even ignored American nationalism as the force that finally led to victory in the war. Indeed, the ferocious combat itself was passed over as merely an ephemeral event. Much more important was the calculus of class conflict. The Beards announced that the Civil War was really:

The Beards themselves abandoned their interpretation by the 1940s and it became defunct among historians in the 1950s, when scholars shifted to an emphasis on slavery. However, Beardian themes still echo among Lost Cause writers.

The first efforts at Civil War battlefield preservation and memorialization came during the war itself with the establishment of National Cemeteries at Gettysburg, Mill Springs and Chattanooga. Soldiers began erecting markers on battlefields beginning with the First Battle of Bull Run in July 1861, but the oldest surviving monument is the Hazen monument, erected at Stones River near Murfreesboro, Tennessee, in the summer of 1863 by soldiers in Union Col. William B. Hazen's brigade to mark the spot where they buried their dead in the Battle of Stones River. In the 1890s, the United States government established five Civil War battlefield parks under the jurisdiction of the War Department, beginning with the creation of the Chickamauga and Chattanooga National Military Park in Tennessee and the Antietam National Battlefield in Maryland in 1890. The Shiloh National Military Park was established in 1894, followed by the Gettysburg National Military Park in 1895 and Vicksburg National Military Park in 1899. In 1933, these five parks and other national monuments were transferred to the jurisdiction of the National Park Service.

The modern Civil War battlefield preservation movement began in 1987 with the founding of the Association for the Preservation of Civil War Sites (APCWS), a grassroots organization created by Civil War historians and others to preserve battlefield land by acquiring it. In 1991, the original Civil War Trust was created in the mold of the Statue of Liberty/Ellis Island Foundation, but failed to attract corporate donors and soon helped manage the disbursement of U.S. Mint Civil War commemorative coin revenues designated for battlefield preservation. Although the two non-profit organizations joined forces on a number of battlefield acquisitions, ongoing conflicts prompted the boards of both organizations to facilitate a merger, which happened in 1999 with the creation of the Civil War Preservation Trust. In 2011, the organization was renamed, again becoming the Civil War Trust. After expanding its mission in 2014 to include battlefields of the Revolutionary War and War of 1812, the non-profit became the American Battlefield Trust in May 2018, operating with two divisions, the Civil War Trust and the Revolutionary War Trust. From 1987 through May 2018, the Trust and its predecessor organizations, along with their partners, preserved 49,893 acres of battlefield land through acquisition of property or conservation easements at more than 130 battlefields in 24 states.

The five major Civil War battlefield parks operated by the National Park Service (Gettysburg, Antietam, Shiloh, Chickamauga/Chattanooga and Vicksburg) had a combined 3.1 million visitors in 2018, down 70% from 10.2 million in 1970. Attendance at Gettysburg in 2018 was 950,000, a decline of 86% since 1970.

The American Civil War has been commemorated in many capacities ranging from the reenactment of battles, to statues and memorial halls erected, to films being produced, to stamps and coins with Civil War themes being issued, all of which helped to shape public memory. This varied advent occurred in greater proportions on the 100th and 150th anniversary.

Hollywood's take on the war has been especially influential in shaping public memory, as seen in such film classics as "Birth of a Nation" (1915), "Gone with the Wind" (1939), and more recently "Lincoln" (2012). Ken Burns produced a notable PBS series on television titled "The Civil War" (1990). It was digitally remastered and re-released in 2015.

Numerous technological innovations during the Civil War had a great impact on 19th-century science. The Civil War was one of the earliest examples of an "industrial war", in which technological might is used to achieve military supremacy in a war. New inventions, such as the train and telegraph, delivered soldiers, supplies and messages at a time when horses were considered to be the fastest way to travel. It was also in this war when countries first used aerial warfare, in the form of reconnaissance balloons, to a significant effect. It saw the first action involving steam-powered ironclad warships in naval warfare history. Repeating firearms such as the Henry rifle, Spencer rifle, Colt revolving rifle, Triplett & Scott carbine and others, first appeared during the Civil War; they were a revolutionary invention that would soon replace muzzle-loading and single-shot firearms in warfare, as well as the first appearances of rapid-firing weapons and machine guns such as the Agar gun and the Gatling gun.














</doc>
<doc id="864" url="https://en.wikipedia.org/wiki?curid=864" title="Andy Warhol">
Andy Warhol

Andy Warhol (; born Andrew Warhola; August 6, 1928 – February 22, 1987) was an American artist, film director, and producer who was a leading figure in the visual art movement known as pop art. His works explore the relationship between artistic expression, advertising, and celebrity culture that flourished by the 1960s, and span a variety of media, including painting, silkscreening, photography, film, and sculpture. Some of his best known works include the silkscreen paintings "Campbell's Soup Cans" (1962) and "Marilyn Diptych" (1962), the experimental film "Chelsea Girls" (1966), and the multimedia events known as the "Exploding Plastic Inevitable" (1966–67).

Born and raised in Pittsburgh, Warhol initially pursued a successful career as a commercial illustrator. After exhibiting his work in several galleries in the late 1950s, he began to receive recognition as an influential and controversial artist. His New York studio, The Factory, became a well-known gathering place that brought together distinguished intellectuals, drag queens, playwrights, Bohemian street people, Hollywood celebrities, and wealthy patrons. He promoted a collection of personalities known as Warhol superstars, and is credited with inspiring the widely used expression "15 minutes of fame." In the late 1960s, he managed and produced the experimental rock band The Velvet Underground and founded "Interview" magazine. He authored numerous books, including "The Philosophy of Andy Warhol" and "". He lived openly as a gay man before the gay liberation movement. After gallbladder surgery, Warhol died of cardiac arrhythmia in February 1987 at the age of 58.

Warhol has been the subject of numerous retrospective exhibitions, books, and feature and documentary films. The Andy Warhol Museum in his native city of Pittsburgh, which holds an extensive permanent collection of art and archives, is the largest museum in the United States dedicated to a single artist. Many of his creations are very collectible and highly valuable. The highest price ever paid for a Warhol painting is US$105 million for a 1963 canvas titled "Silver Car Crash (Double Disaster)"; his works include some of the most expensive paintings ever sold. A 2009 article in "The Economist" described Warhol as the "bellwether of the art market".

Warhol was born on August 6, 1928, in Pittsburgh, Pennsylvania. He was the fourth child of Ondrej Warhola (Americanized as Andrew Warhola, Sr., 1889–1942) and Julia ("née" Zavacká, 1892–1972), whose first child was born in their homeland and died before their move to the U.S.

His parents were working-class Lemko emigrants from Mikó, Austria-Hungary (now called Miková, located in today's northeastern Slovakia). Warhol's father emigrated to the United States in 1914, and his mother joined him in 1921, after the death of Warhol's grandparents. Warhol's father worked in a coal mine. The family lived at 55 Beelen Street and later at 3252 Dawson Street in the Oakland neighborhood of Pittsburgh. The family was Ruthenian Catholic and attended St. John Chrysostom Byzantine Catholic Church. Andy Warhol had two older brothers—Pavol (Paul), the oldest, was born before the family emigrated; Ján was born in Pittsburgh. Pavol's son, James Warhola, became a successful children's book illustrator.

In third grade, Warhol had Sydenham's chorea (also known as St. Vitus' Dance), the nervous system disease that causes involuntary movements of the extremities, which is believed to be a complication of scarlet fever which causes skin pigmentation blotchiness. At times when he was confined to bed, he drew, listened to the radio and collected pictures of movie stars around his bed. Warhol later described this period as very important in the development of his personality, skill-set and preferences. When Warhol was 13, his father died in an accident.

As a teenager, Warhol graduated from Schenley High School in 1945. Also as a teen, Warhol won a Scholastic Art and Writing Award. After graduating from high school, his intentions were to study art education at the University of Pittsburgh in the hope of becoming an art teacher, but his plans changed and he enrolled in the Carnegie Institute of Technology, now Carnegie Mellon University in Pittsburgh, where he studied commercial art. During his time there, Warhol joined the campus Modern Dance Club and Beaux Arts Society. He also served as art director of the student art magazine, "Cano", illustrating a cover in 1948 and a full-page interior illustration in 1949. These are believed to be his first two published artworks. Warhol earned a Bachelor of Fine Arts in pictorial design in 1949. Later that year, he moved to New York City and began a career in magazine illustration and advertising.

Warhol's early career was dedicated to commercial and advertising art, where his first commission had been to draw shoes for "Glamour" magazine in the late 1940s. In the 1950s, Warhol worked as a designer for shoe manufacturer Israel Miller. American photographer John Coplans recalled that

Warhol's "whimsical" ink drawings of shoe advertisements figured in some of his earliest showings at the Bodley Gallery in New York.

Warhol was an early adopter of the silk screen printmaking process as a technique for making paintings. A young Warhol was taught silk screen printmaking techniques by Max Arthur Cohn at his graphic arts business in Manhattan. While working in the shoe industry, Warhol developed his "blotted line" technique, applying ink to paper and then blotting the ink while still wet, which was akin to a printmaking process on the most rudimentary scale. His use of tracing paper and ink allowed him to repeat the basic image and also to create endless variations on the theme, a method that prefigures his 1960s silk-screen canvas. In his book "", Warhol writes: "When you do something exactly wrong, you always turn up something."

Warhol habitually used the expedient of tracing photographs projected with an epidiascope. Using prints by Edward Wallowitch, his 'first boyfriend' the photographs would undergo a subtle transformation during Warhol's often cursory tracing of contours and hatching of shadows. Warhol used Wallowitch's photograph "Young Man Smoking a Cigarette" (c.1956), for a 1958 design for a book cover he submitted to Simon and Schuster for the Walter Ross pulp novel "The Immortal", and later used others for his dollar bill series, and for "Big Campbell's Soup Can with Can Opener (Vegetable)", of 1962 which initiated Warhol's most sustained motif, the soup can.

With the rapid expansion of the record industry, RCA Records hired Warhol, along with another freelance artist, Sid Maurer, to design album covers and promotional materials.

He began exhibiting his work during the 1950s. He held exhibitions at the Hugo Gallery and the Bodley Gallery in New York City; in California, his first West Coast gallery exhibition was on July 9, 1962, in the Ferus Gallery of Los Angeles with Campbell's Soup Cans. The exhibition marked his West Coast debut of pop art.
Andy Warhol's first New York solo pop art exhibition was hosted at Eleanor Ward's Stable Gallery November 6–24, 1962. The exhibit included the works "Marilyn Diptych", "100 Soup Cans", "100 Coke Bottles", and "100 Dollar Bills". At the Stable Gallery exhibit, the artist met for the first time poet John Giorno who would star in Warhol's first film, "Sleep", in 1963.

It was during the 1960s that Warhol began to make paintings of iconic American objects such as dollar bills, mushroom clouds, electric chairs, Campbell's Soup Cans, Coca-Cola bottles, celebrities such as Marilyn Monroe, Elvis Presley, Marlon Brando, Troy Donahue, Muhammad Ali, and Elizabeth Taylor, as well as newspaper headlines or photographs of police dogs attacking African-American protesters during the Birmingham campaign in the civil rights movement. During these years, he founded his studio, "The Factory" and gathered about him a wide range of artists, writers, musicians, and underground celebrities. His work became popular and controversial. Warhol had this to say about Coca-Cola:

New York City's Museum of Modern Art hosted a Symposium on pop art in December 1962 during which artists such as Warhol were attacked for "capitulating" to consumerism. Critics were scandalized by Warhol's open embrace of market culture. This symposium set the tone for Warhol's reception.

A pivotal event was the 1964 exhibit "The American Supermarket", a show held in Paul Bianchini's Upper East Side gallery. The show was presented as a typical U.S. small supermarket environment, except that everything in it—from the produce, canned goods, meat, posters on the wall, etc.—was created by six prominent pop artists of the time, among them the controversial (and like-minded) Billy Apple, Mary Inman, and Robert Watts. Warhol's painting of a can of Campbell's soup cost $1,500 while each autographed can sold for $6. The exhibit was one of the first mass events that directly confronted the general public with both pop art and the perennial question of what art is.

As an advertisement illustrator in the 1950s, Warhol used assistants to increase his productivity. Collaboration would remain a defining (and controversial) aspect of his working methods throughout his career; this was particularly true in the 1960s. One of the most important collaborators during this period was Gerard Malanga. Malanga assisted the artist with the production of silkscreens, films, sculpture, and other works at "The Factory", Warhol's aluminum foil-and-silver-paint-lined studio on 47th Street (later moved to Broadway). Other members of Warhol's Factory crowd included Freddie Herko, Ondine, Ronald Tavel, Mary Woronov, Billy Name, and Brigid Berlin (from whom he apparently got the idea to tape-record his phone conversations).

During the 1960s, Warhol also groomed a retinue of bohemian and counterculture eccentrics upon whom he bestowed the designation "Superstars", including Nico, Joe Dallesandro, Edie Sedgwick, Viva, Ultra Violet, Holly Woodlawn, Jackie Curtis, and Candy Darling. These people all participated in the Factory films, and some—like Berlin—remained friends with Warhol until his death. Important figures in the New York underground art/cinema world, such as writer John Giorno and film-maker Jack Smith, also appear in Warhol films (many premiering at the New Andy Warhol Garrick Theatre and 55th Street Playhouse) of the 1960s, revealing Warhol's connections to a diverse range of artistic scenes during this time. Less well known was his support and collaboration with several teenagers during this era, who would achieve prominence later in life including writer David Dalton, photographer Stephen Shore and artist Bibbe Hansen (mother of pop musician Beck).

On June 3, 1968, radical feminist writer Valerie Solanas shot Warhol and Mario Amaya, art critic and curator, at Warhol's studio. Before the shooting, Solanas had been a marginal figure in the Factory scene. She authored in 1967 the "S.C.U.M. Manifesto", a separatist feminist tract that advocated the elimination of men; and appeared in the 1968 Warhol film "I, a Man". Earlier on the day of the attack, Solanas had been turned away from the Factory after asking for the return of a script she had given to Warhol. The script had apparently been misplaced.

Amaya received only minor injuries and was released from the hospital later the same day. Warhol was seriously wounded by the attack and barely survived: surgeons opened his chest and massaged his heart to help stimulate its movement again. He suffered physical effects for the rest of his life, including being required to wear a surgical corset. The shooting had a profound effect on Warhol's life and art.

Solanas was arrested the day after the assault, after turning herself in to police. By way of explanation, she said that Warhol "had too much control over my life." She was subsequently diagnosed with paranoid schizophrenia and eventually sentenced to three years under the control of the Department of Corrections. After the shooting, the Factory scene heavily increased security, and for many the "Factory 60s" ended.

Warhol had this to say about the attack: "Before I was shot, I always thought that I was more half-there than all-there—I always suspected that I was watching TV instead of living life. People sometimes say that the way things happen in movies is unreal, but actually it's the way things happen in life that's unreal. The movies make emotions look so strong and real, whereas when things really do happen to you, it's like watching television—you don't feel anything. Right when I was being shot and ever since, I knew that I was watching television. The channels switch, but it's all television."

Compared to the success and scandal of Warhol's work in the 1960s, the 1970s were a much quieter decade, as he became more entrepreneurial. According to Bob Colacello, Warhol devoted much of his time to rounding up new, rich patrons for portrait commissions—including Shah of Iran Mohammad Reza Pahlavi, his wife Empress Farah Pahlavi, his sister Princess Ashraf Pahlavi, Mick Jagger, Liza Minnelli, John Lennon, Diana Ross, and Brigitte Bardot. Warhol's famous portrait of Chinese Communist leader Mao Zedong was created in 1973. He also founded, with Gerard Malanga, "Interview" magazine, and published "The Philosophy of Andy Warhol" (1975). An idea expressed in the book: "Making money is art, and working is art and good business is the best art."

Warhol socialized at various nightspots in New York City, including Max's Kansas City; and, later in the 1970s, Studio 54. He was generally regarded as quiet, shy, and a meticulous observer. Art critic Robert Hughes called him "the white mole of Union Square."

In 1979, along with his longtime friend Stuart Pivar, Warhol founded the New York Academy of Art.

Warhol had a re-emergence of critical and financial success in the 1980s, partially due to his affiliation and friendships with a number of prolific younger artists, who were dominating the "bull market" of 1980s New York art: Jean-Michel Basquiat, Julian Schnabel, David Salle and other so-called Neo-Expressionists, as well as members of the Transavantgarde movement in Europe, including Francesco Clemente and Enzo Cucchi. Before the 1984 Sarajevo Winter Olympics, he teamed with 15 other artists, including David Hockney and Cy Twombly, and contributed a Speed Skater print to the Art and Sport collection. The Speed Skater was used for the official Sarajevo Winter Olympics poster.

By this time, graffiti artist Fab Five Freddy paid homage to Warhol when he painted an entire train with Campbell soup cans. This was instrumental in Freddy becoming involved in the underground NYC art scene and becoming an affiliate of Basquiat.

By this period, Warhol was being criticized for becoming merely a "business artist". In 1979, reviewers disliked his exhibits of portraits of 1970s personalities and celebrities, calling them superficial, facile and commercial, with no depth or indication of the significance of the subjects. They also criticized his 1980 exhibit of 10 portraits at the Jewish Museum in Manhattan, entitled "Jewish Geniuses", which Warhol—who was uninterested in Judaism and Jews—had described in his diary as "They're going to sell." In hindsight, however, some critics have come to view Warhol's superficiality and commerciality as "the most brilliant mirror of our times," contending that "Warhol had captured something irresistible about the zeitgeist of American culture in the 1970s."

Warhol also had an appreciation for intense Hollywood glamour. He once said: "I love Los Angeles. I love Hollywood. They're so beautiful. Everything's plastic, but I love plastic. I want to be plastic."

In 1984 Vanity Fair commissioned Warhol to produce a portrait of Prince, in order to accompany an article that celebrated the success of "Purple Rain" and its accompanying movie. Referencing the many celebrity portraits produced by Warhol across his career, "Orange" "Prince (1984)" was created using a similar composition to the Marilyn "Flavors" series from 1962, among some of Warhol's very first celebrity portraits. Prince is depicted in a pop color palette commonly used by Warhol, in bright orange with highlights of bright green and blue. The facial features and hair are screen-printed in black over the orange background.

In the "Andy Warhol Diaries", Warhol recorded how excited he was to see Prince and Billy Idol together at a party in the mid 1980s, and he compared them to the Hollywood movie stars of the 1950s and 1960s who also inspired his portraits: "...seeing these two glamour boys, its like "boys" are the new Hollywood glamour girls, like Jean Harlow and Marilyn Monroe".

Warhol died in Manhattan at 6:32 a.m. on February 22, 1987 at age 58. According to news reports, he had been making a good recovery from gallbladder surgery at New York Hospital before dying in his sleep from a sudden post-operative irregular heartbeat. Prior to his diagnosis and operation, Warhol delayed having his recurring gallbladder problems checked, as he was afraid to enter hospitals and see doctors. His family sued the hospital for inadequate care, saying that the arrhythmia was caused by improper care and water intoxication. The malpractice case was quickly settled out of court; Warhol's family received an undisclosed sum of money.

Shortly before Warhol's death, doctors expected Warhol to survive the surgery, though a re-evaluation of the case about thirty years after his death showed many indications that Warhol's surgery was in fact riskier than originally thought. It was widely reported at the time that Warhol died of a "routine" surgery, though when considering factors such as his age, a family history of gallbladder problems, his previous gunshot wound, and his medical state in the weeks leading up to the procedure, the potential risk of death following the surgery appeared to have been significant.

Warhol's brothers took his body back to Pittsburgh, where an open-coffin wake was held at the Thomas P. Kunsak Funeral Home. The solid bronze casket had gold-plated rails and white upholstery. Warhol was dressed in a black cashmere suit, a paisley tie, a platinum wig, and sunglasses. He was laid out holding a small prayer book and a red rose. The funeral liturgy was held at the Holy Ghost Byzantine Catholic Church on Pittsburgh's North Side. The eulogy was given by Monsignor Peter Tay. Yoko Ono and John Richardson were speakers. The coffin was covered with white roses and asparagus ferns. After the liturgy, the coffin was driven to St. John the Baptist Byzantine Catholic Cemetery in Bethel Park, a south suburb of Pittsburgh.

At the grave, the priest said a brief prayer and sprinkled holy water on the casket. Before the coffin was lowered, Paige Powell dropped a copy of "Interview" magazine, an "Interview" T-shirt, and a bottle of the Estee Lauder perfume "Beautiful" into the grave. Warhol was buried next to his mother and father. A memorial service was held in Manhattan for Warhol on April 1, 1987, at St. Patrick's Cathedral, New York.

By the beginning of the 1960s, pop art was an experimental form that several artists were independently adopting; some of these pioneers, such as Roy Lichtenstein, would later become synonymous with the movement. Warhol, who would become famous as the "Pope of Pop", turned to this new style, where popular subjects could be part of the artist's palette. His early paintings show images taken from cartoons and advertisements, hand-painted with paint drips. Marilyn Monroe was a pop art painting that Warhol had done and it was very popular. Those drips emulated the style of successful abstract expressionists (such as Willem de Kooning). Warhol's first pop art paintings were displayed in April 1961, serving as the backdrop for New York Department Store Bonwit Teller's window display. This was the same stage his Pop Art contemporaries Jasper Johns, James Rosenquist and Robert Rauschenberg had also once graced.
It was the gallerist Muriel Latow who came up with the ideas for both the soup cans and Warhol's dollar paintings. On November 23, 1961, Warhol wrote Latow a check for $50 which, according to the 2009 Warhol biography, "Pop, The Genius of Warhol", was payment for coming up with the idea of the soup cans as subject matter. For his first major exhibition, Warhol painted his famous cans of Campbell's soup, which he claimed to have had for lunch for most of his life. A 1964 "Large Campbell's Soup Can" was sold in a 2007 Sotheby's auction to a South American collector for £5.1 million ($7.4 million).

He loved celebrities, so he painted them as well. From these beginnings he developed his later style and subjects. Instead of working on a signature subject matter, as he started out to do, he worked more and more on a signature style, slowly eliminating the handmade from the artistic process. Warhol frequently used silk-screening; his later drawings were traced from slide projections. At the height of his fame as a painter, Warhol had several assistants who produced his silk-screen multiples, following his directions to make different versions and variations.

In 1979, Warhol was commissioned by BMW to paint a Group-4 race version of the then "elite supercar" BMW M1 for the fourth installment in the BMW Art Car Project. It was reported at the time that, unlike the three artists before him, Warhol opted to paint directly onto the automobile himself instead of letting technicians transfer his scale-model design to the car. It was indicated that Warhol spent only a total of 23 minutes to paint the entire car.

Warhol produced both comic and serious works; his subject could be a soup can or an electric chair. Warhol used the same techniques—silkscreens, reproduced serially, and often painted with bright colors—whether he painted celebrities, everyday objects, or images of suicide, car crashes, and disasters, as in the 1962–63 "Death and Disaster" series. The "Death and Disaster" paintings included "Red Car Crash", "Purple Jumping Man", and "Orange Disaster." One of these paintings, the diptych "Silver Car Crash", became the highest priced work of his when it sold at Sotheby's Contemporary Art Auction on Wednesday, November 13, 2013, for $105.4 million.

Some of Warhol's work, as well as his own personality, has been described as being Keatonesque. Warhol has been described as playing dumb to the media. He sometimes refused to explain his work. He has suggested that all one needs to know about his work is "already there 'on the surface'."

His Rorschach inkblots are intended as pop comments on art and what art could be. His cow wallpaper (literally, wallpaper with a cow motif) and his oxidation paintings (canvases prepared with copper paint that was then oxidized with urine) are also noteworthy in this context. Equally noteworthy is the way these works—and their means of production—mirrored the atmosphere at Andy's New York "Factory". Biographer Bob Colacello provides some details on Andy's "piss paintings":

Warhol's first portrait of "Basquiat" (1982) is a black photo-silkscreen over an oxidized copper "piss painting".

After many years of silkscreen, oxidation, photography, etc., Warhol returned to painting with a brush in hand in a series of more than 50 large collaborative works done with Jean-Michel Basquiat between 1984 and 1986. Despite negative criticism when these were first shown, Warhol called some of them "masterpieces," and they were influential for his later work.

Andy Warhol was commissioned in 1984 by collector and gallerist Alexander Iolas to produce work based on Leonardo da Vinci's "The Last Supper" for an exhibition at the old refectory of the Palazzo delle Stelline in Milan, opposite from the Santa Maria delle Grazie where Leonardo da Vinci's mural can be seen. Warhol exceeded the demands of the commission and produced nearly 100 variations on the theme, mostly silkscreens and paintings, and among them a collaborative sculpture with Basquiat, the "Ten Punching Bags (Last Supper)".
The Milan exhibition that opened in January 1987 with a set of 22 silk-screens, was the last exhibition for both the artist and the gallerist. The series of "The Last Supper" was seen by some as "arguably his greatest," but by others as "wishy-washy, religiose" and "spiritless." It is the largest series of religious-themed works by any U.S. artist.

Artist Maurizio Cattelan describes that it is difficult to separate daily encounters from the art of Andy Warhol: "That's probably the greatest thing about Warhol: the way he penetrated and summarized our world, to the point that distinguishing between him and our everyday life is basically impossible, and in any case useless." Warhol was an inspiration towards Cattelan's magazine and photography compilations, such as "Permanent Food, Charley", and "Toilet Paper".

In the period just before his death, Warhol was working on "Cars", a series of paintings for Mercedes-Benz.

A self-portrait by Andy Warhol (1963–64), which sold in New York at the May Post-War and Contemporary evening sale in Christie's, fetched $38.4 million.

On May 9, 2012, his classic painting "Double Elvis (Ferus Type)" sold at auction at Sotheby's in New York for US$33 million. With commission, the sale price totaled US$37,042,500, short of the $50 million that Sotheby's had predicted the painting might bring. The piece (silkscreen ink and spray paint on canvas) shows Elvis Presley in a gunslinger pose. It was first exhibited in 1963 at the Ferus Gallery in Los Angeles. Warhol made 22 versions of the "Double Elvis", nine of which are held in museums.

In November 2013, his "Silver Car Crash (Double Disaster)" diptych sold at Sotheby's Contemporary Art Auction for $105.4 million, a new record for the pop artist (pre-auction estimates were at $80 million). Created in 1963, this work had rarely been seen in public in the previous years. In November 2014, "Triple Elvis" sold for $81.9m (£51.9m) at an auction in New York.

Warhol worked across a wide range of media—painting, photography, drawing, and sculpture. In addition, he was a highly prolific filmmaker. Between 1963 and 1968, he made more than 60 films, plus some 500 short black-and-white "screen test" portraits of Factory visitors. One of his most famous films, "Sleep", monitors poet John Giorno sleeping for six hours. The 35-minute film "Blow Job" is one continuous shot of the face of DeVeren Bookwalter supposedly receiving oral sex from filmmaker Willard Maas, although the camera never tilts down to see this. Another, "Empire" (1964), consists of eight hours of footage of the Empire State Building in New York City at dusk. The film "Eat" consists of a man eating a mushroom for 45 minutes. Warhol attended the 1962 premiere of the static composition by LaMonte Young called "Trio for Strings" and subsequently created his famous series of static films including "Kiss", "Eat", and "Sleep" (for which Young initially was commissioned to provide music). Uwe Husslein cites filmmaker Jonas Mekas, who accompanied Warhol to the Trio premiere, and who claims Warhol's static films were directly inspired by the performance.

"Batman Dracula" is a 1964 film that was produced and directed by Warhol, without the permission of DC Comics. It was screened only at his art exhibits. A fan of the "Batman" series, Warhol's movie was an "homage" to the series, and is considered the first appearance of a blatantly campy Batman. The film was until recently thought to have been lost, until scenes from the picture were shown at some length in the 2006 documentary "Jack Smith and the Destruction of Atlantis".

Warhol's 1965 film "Vinyl" is an adaptation of Anthony Burgess' popular dystopian novel "A Clockwork Orange". Others record improvised encounters between Factory regulars such as Brigid Berlin, Viva, Edie Sedgwick, Candy Darling, Holly Woodlawn, Ondine, Nico, and Jackie Curtis. Legendary underground artist Jack Smith appears in the film "Camp".

His most popular and critically successful film was "Chelsea Girls" (1966). The film was highly innovative in that it consisted of two 16 mm-films being projected simultaneously, with two different stories being shown in tandem. From the projection booth, the sound would be raised for one film to elucidate that "story" while it was lowered for the other. The multiplication of images evoked Warhol's seminal silk-screen works of the early 1960s.

Warhol was a fan of filmmaker Radley Metzger film work and commented that Metzger's film, "The Lickerish Quartet", was "an outrageously kinky masterpiece". "Blue Movie"—a film in which Warhol superstar Viva makes love in bed with Louis Waldon, another Warhol superstar—was Warhol's last film as director. The film, a seminal film in the Golden Age of Porn, was, at the time, controversial for its frank approach to a sexual encounter. "Blue Movie" was publicly screened in New York City in 2005, for the first time in more than 30 years.

After his June 3, 1968 shooting, a reclusive Warhol relinquished his personal involvement in filmmaking. His acolyte and assistant director, Paul Morrissey, took over the film-making chores for the Factory collective, steering Warhol-branded cinema towards more mainstream, narrative-based, B-movie exploitation fare with "Flesh", "Trash", and "Heat". All of these films, including the later "Andy Warhol's Dracula" and "Andy Warhol's Frankenstein", were far more mainstream than anything Warhol as a director had attempted. These latter "Warhol" films starred Joe Dallesandro—more of a Morrissey star than a true Warhol superstar.

In the early 1970s, most of the films directed by Warhol were pulled out of circulation by Warhol and the people around him who ran his business. After Warhol's death, the films were slowly restored by the Whitney Museum and are occasionally projected at museums and film festivals. Few of the Warhol-directed films are available on video or DVD.


In the mid-1960s, Warhol adopted the band the Velvet Underground, making them a crucial element of the Exploding Plastic Inevitable multimedia performance art show. Warhol, with Paul Morrissey, acted as the band's manager, introducing them to Nico (who would perform with the band at Warhol's request). While managing The Velvet Underground, Andy would have them dressed in all black to perform in front of movies that he was also presenting. In 1966 he "produced" their first album "The Velvet Underground & Nico", as well as providing its album art. His actual participation in the album's production amounted to simply paying for the studio time. After the band's first album, Warhol and band leader Lou Reed started to disagree more about the direction the band should take, and their artistic friendship ended. In 1989, after Warhol's death, Reed and John Cale re-united for the first time since 1972 to write, perform, record and release the concept album "Songs for Drella", a tribute to Warhol. In October 2019, an audio tape of publicly unknown music by Reed, based on Warhols' 1975 book, “"The Philosophy of Andy Warhol: From A to B and Back Again"”, was reported to have been discovered in an archive at the Andy Warhol Museum in Pittsburgh.

Warhol designed many album covers for various artists starting with the photographic cover of John Wallowitch's debut album, "This Is John Wallowitch!!!" (1964). He designed the cover art for The Rolling Stones' albums "Sticky Fingers" (1971) and "Love You Live" (1977), and the John Cale albums "The Academy in Peril" (1972) and "Honi Soit" in 1981. One of Warhol's last works was a portrait of Aretha Franklin for the cover of her 1986 gold album "Aretha", which was done in the style of the "Reigning Queens" series he had completed the year before.

Warhol strongly influenced the new wave/punk rock band Devo, as well as David Bowie. Bowie recorded a song called "Andy Warhol" for his 1971 album "Hunky Dory". Lou Reed wrote the song "Andy's Chest", about Valerie Solanas, the woman who shot Warhol, in 1968. He recorded it with the Velvet Underground, and this version was released on the "VU" album in 1985. Bowie would later play Warhol in the 1996 movie, "Basquiat". Bowie recalled how meeting Warhol in real life helped him in the role, and recounted his early meetings with him:

The band Triumph also wrote a song about Andy Warhol, "Stranger In A Strange Land" off their 1984 album Thunder Seven.

Beginning in the early 1950s, Warhol produced several unbound portfolios of his work.

The first of several bound self-published books by Warhol was "25 Cats Name Sam and One Blue Pussy", printed in 1954 by Seymour Berlin on Arches brand watermarked paper using his blotted line technique for the lithographs. The original edition was limited to 190 numbered, hand colored copies, using Dr. Martin's ink washes. Most of these were given by Warhol as gifts to clients and friends. Copy No. 4, inscribed "Jerry" on the front cover and given to Geraldine Stutz, was used for a facsimile printing in 1987, and the original was auctioned in May 2006 for US$35,000 by Doyle New York.

Other self-published books by Warhol include:

Warhol's book "A La Recherche du Shoe Perdu" (1955) marked his "transition from commercial to gallery artist". (The title is a play on words by Warhol on the title of French author Marcel Proust's "À la recherche du temps perdu".)

After gaining fame, Warhol "wrote" several books that were commercially published:

Warhol created the fashion magazine "Interview" that is still published today. The loopy title script on the cover is thought to be either his own handwriting or that of his mother, Julia Warhola, who would often do text work for his early commercial pieces.

Although Andy Warhol is most known for his paintings and films, he authored works in many different media.



He founded the gossip magazine "Interview", a stage for celebrities he "endorsed" and a business staffed by his friends. He collaborated with others on all of his books (some of which were written with Pat Hackett.) He adopted the young painter Jean-Michel Basquiat, and the band The Velvet Underground, presenting them to the public as his latest interest, and collaborating with them. One might even say that he produced people (as in the Warholian "Superstar" and the Warholian portrait). He endorsed products, appeared in commercials, and made frequent celebrity guest appearances on television shows and in films (he appeared in everything from "Love Boat" to "Saturday Night Live" and the Richard Pryor movie "Dynamite Chicken").

In this respect Warhol was a fan of "Art Business" and "Business Art"—he, in fact, wrote about his interest in thinking about art as business in "The Philosophy of Andy Warhol from A to B and Back Again".

Warhol was homosexual. Interviewed in 1980, he indicated that he was still a virgin. Biographer Bob Colacello, who was present at the interview, felt it was probably true and that what little sex he had was probably "a mixture of voyeurism and masturbation—to use [Andy's] word "abstract"". Warhol's assertion of virginity would seem to be contradicted by his hospital treatment in 1960 for condylomata, a sexually transmitted disease. It has also been contradicted by his lovers, including Warhol muse BillyBoy, who has said they had sex to orgasm: "When he wasn't being Andy Warhol and when you were just alone with him he was an incredibly generous and very kind person. What seduced me was the Andy Warhol who I saw alone. In fact when I was with him in public he kind of got on my nerves...I'd say: 'You're just obnoxious, I can't bear you." Billy Name also denied that Warhol was only a voyeur, saying: "He was the essence of sexuality. It permeated everything. Andy exuded it, along with his great artistic creativity...It brought a joy to the whole art world in New York." "But his personality was so vulnerable that it became a defense to put up the blank front." Warhol's lovers included John Giorno, Billy Name, Charles Lisanby, and Jon Gould. His boyfriend of 12 years was Jed Johnson, whom he met in 1968, and who later achieved fame as an interior designer.

The fact that Warhol's homosexuality influenced his work and shaped his relationship to the art world is a major subject of scholarship on the artist and is an issue that Warhol himself addressed in interviews, in conversation with his contemporaries, and in his publications ("e.g.", "Popism: The Warhol 1960s"). Throughout his career, Warhol produced erotic photography and drawings of male nudes. Many of his most famous works (portraits of Liza Minnelli, Judy Garland, and Elizabeth Taylor, and films such as "Blow Job", "My Hustler" and "Lonesome Cowboys") draw from gay underground culture or openly explore the complexity of sexuality and desire. As has been addressed by a range of scholars, many of his films premiered in gay porn theaters, including the New Andy Warhol Garrick Theatre and 55th Street Playhouse, in the late 1960s.

The first works that Warhol submitted to a fine art gallery, homoerotic drawings of male nudes, were rejected for being too openly gay. In "Popism", furthermore, the artist recalls a conversation with the film maker Emile de Antonio about the difficulty Warhol had being accepted socially by the then-more-famous (but closeted) gay artists Jasper Johns and Robert Rauschenberg. De Antonio explained that Warhol was "too swish and that upsets them." In response to this, Warhol writes, "There was nothing I could say to that. It was all too true. So I decided I just wasn't going to care, because those were all the things that I didn't want to change anyway, that I didn't think I 'should' want to change ... Other people could change their attitudes but not me". In exploring Warhol's biography, many turn to this period—the late 1950s and early 1960s—as a key moment in the development of his persona. Some have suggested that his frequent refusal to comment on his work, to speak about himself (confining himself in interviews to responses like "Um, no" and "Um, yes", and often allowing others to speak for him)—and even the evolution of his pop style—can be traced to the years when Warhol was first dismissed by the inner circles of the New York art world.

Warhol was a practicing Ruthenian Catholic. He regularly volunteered at homeless shelters in New York City, particularly during the busier times of the year, and described himself as a religious person. Many of Warhol's later works depicted religious subjects, including two series, "Details of Renaissance Paintings" (1984) and "The Last Supper" (1986). In addition, a body of religious-themed works was found posthumously in his estate.

During his life, Warhol regularly attended Liturgy, and the priest at Warhol's church, Saint Vincent Ferrer, said that the artist went there almost daily, although he was not observed taking Communion or going to Confession and sat or knelt in the pews at the back. The priest thought he was afraid of being recognized; Warhol said he was self-conscious about being seen in a Roman Rite church crossing himself "in the Orthodox way" (right to left instead of the reverse).

His art is noticeably influenced by the Eastern Christian tradition which was so evident in his places of worship.

Warhol's brother has described the artist as "really religious, but he didn't want people to know about that because [it was] private". Despite the private nature of his faith, in Warhol's eulogy John Richardson depicted it as devout: "To my certain knowledge, he was responsible for at least one conversion. He took considerable pride in financing his nephew's studies for the priesthood".

Warhol was an avid collector. His friends referred to his numerous collections, which filled not only his four-story townhouse, but also a nearby storage unit, as "Andy's Stuff." The true extent of his collections was not discovered until after his death, when The Andy Warhol Museum in Pittsburgh took in 641 boxes of his "Stuff."

Warhol's collections included a Coca-Cola memorabilia sign, and 19th century paintings along with airplane menus, unpaid invoices, pizza dough, pornographic pulp novels, newspapers, stamps, supermarket flyers, and cookie jars, among other eccentricities. It also included significant works of art, such as George Bellows's "Miss Bentham". One of his main collections was his wigs. Warhol owned more than 40 and felt very protective of his hairpieces, which were sewn by a New York wig-maker from hair imported from Italy. In 1985 a girl snatched Warhol's wig off his head. It was later discovered in Warhol's diary entry for that day that he wrote: "I don't know what held me back from pushing her over the balcony."

Another item found in Warhol's boxes at the museum in Pittsburgh was a mummified human foot from Ancient Egypt. The curator of anthropology at Carnegie Museum of Natural History felt that Warhol most likely found it at a flea market.

Warhol's will dictated that his entire estate—with the exception of a few modest legacies to family members—would go to create a foundation dedicated to the "advancement of the visual arts". Warhol had so many possessions that it took Sotheby's nine days to auction his estate after his death; the auction grossed more than US$20 million.

In 1987, in accordance with Warhol's will, the Andy Warhol Foundation for the Visual Arts began. The foundation serves as the estate of Andy Warhol, but also has a mission "to foster innovative artistic expression and the creative process" and is "focused primarily on supporting work of a challenging and often experimental nature."

The Artists Rights Society is the U.S. copyright representative for the Andy Warhol Foundation for the Visual Arts for all Warhol works with the exception of Warhol film stills. The U.S. copyright representative for Warhol film stills is the Warhol Museum in Pittsburgh. Additionally, the Andy Warhol Foundation for the Visual Arts has agreements in place for its image archive. All digital images of Warhol are exclusively managed by Corbis, while all transparency images of Warhol are managed by Art Resource.

The Andy Warhol Foundation released its "20th Anniversary Annual Report" as a three-volume set in 2007: Vol. I, 1987–2007; Vol. II, Grants & Exhibitions; and Vol. III, Legacy Program. The Foundation remains one of the largest grant-giving organizations for the visual arts in the U.S.

Many of Warhol's works and possessions are on display at The Andy Warhol Museum in Pittsburgh. The foundation donated more than 3,000 works of art to the museum.

Warhol appeared as himself in the film "Cocaine Cowboys" (1979) and in the film "Tootsie" (1982).

After his death, Warhol was portrayed by Crispin Glover in Oliver Stone's film "The Doors" (1991), by David Bowie in Julian Schnabel's film "Basquiat" (1996), and by Jared Harris in Mary Harron's film "I Shot Andy Warhol" (1996). Warhol appears as a character in Michael Daugherty's opera "Jackie O" (1997). Actor Mark Bringleson makes a brief cameo as Warhol in "" (1997). Many films by avant-garde cineast Jonas Mekas have caught the moments of Warhol's life. Sean Gregory Sullivan depicted Warhol in the film "54" (1998). Guy Pearce portrayed Warhol in the film "Factory Girl" (2007) about Edie Sedgwick's life. Actor Greg Travis portrays Warhol in a brief scene from the film "Watchmen" (2009).

In the movie "Highway to Hell" a group of Andy Warhols are part of the "Good Intentions Paving Company" where good-intentioned souls are ground into pavement. In the film "Men in Black 3" (2012) Andy Warhol turns out to really be undercover MIB Agent W (played by Bill Hader). Warhol is throwing a party at The Factory in 1969, where he is looked up by MIB Agents K and J (J from the future). Agent W is desperate to end his undercover job ("I'm so out of ideas I'm painting soup cans and bananas, for Christ sakes!", "You gotta fake my death, okay? I can't listen to sitar music anymore." and "I can't tell the girls from the boys.").

Andy Warhol (portrayed by Tom Meeten) is one of main characters of the 2012 British television show "Noel Fielding's Luxury Comedy". The character is portrayed as having robot-like mannerisms. In the 2017 feature "The Billionaire Boys Club" Cary Elwes portrays Warhol in a film based on the true story about Ron Levin (portrayed by Kevin Spacey) a friend of Warhol's who was murdered in 1986. In September 2016, it was announced that Jared Leto would portray the title character in "Warhol", an upcoming American biographical drama film produced by Michael De Luca and written by Terence Winter, based on the book "Warhol: The Biography" by Victor Bockris.


Warhol appeared as a recurring character in TV series "Vinyl", played by John Cameron Mitchell. Warhol was portrayed by Evan Peters in the "" episode "". The episode depicts the attempted assassination of Warhol by Valerie Solanas (Lena Dunham).

In early 1969, Andy Warhol was commissioned by Braniff International to appear in two television commercials to promote the luxury Airline's new When You Got It - Flaunt It Campaign. The campaign was created by Braniff's new advertising agency Lois Holland Calloway, which was led by famed advertiser George Lois, creator of a famed series of Esquire Magazine covers. The first commercial series involved pairing the most unlikely people but who shared the fact that they both flew Braniff Airways. Mr. Warhol was paired with boxing legend Sonny Liston. The odd commercial worked as did the others that featured unlikely fellow travelers such as painter Salvador Dali and baseball legend Whitey Ford.

Two additional commercials for Braniff were created that featured famous persons entering a Braniff jet and being greeted a Braniff Hostess while espousing their like for flying Braniff. Mr. Warhol was also featured in the first of these commercials that were also produced by Mr. Lois and were released in the summer of 1969. Mr. Lois has incorrectly stated that he was commissioned by Braniff in 1967 for representation during that year but at that time Madison Avenue advertising doyenne Mary Wells Lawrence, who was married to Braniff's charismatic Chairman and President Harding Lawrence, was representing the Dallas-based carrier at that time. Mr. Lois succeeded Wells Rich Greene Agency on December 1, 1968. The rights to Mr. Warhol's films for Braniff and his signed contracts are owned by a private Trust and are administered by Braniff Airways Foundation in Dallas, Texas.

In 2002, the U.S. Postal Service issued an 18-cent stamp commemorating Warhol. Designed by Richard Sheaff of Scottsdale, Arizona, the stamp was unveiled at a ceremony at The Andy Warhol Museum and features Warhol's painting "Self-Portrait, 1964". In March 2011, a chrome statue of Andy Warhol and his Polaroid camera was revealed at Union Square in New York City.





</doc>
<doc id="868" url="https://en.wikipedia.org/wiki?curid=868" title="Alp Arslan">
Alp Arslan

Alp Arslan (honorific in Turkish meaning "Heroic Lion"; in ; full name: "Diyā ad-Dunyā wa ad-Dīn Adud ad-Dawlah Abu Shujā’ Muhammad Ālp Ārslan ibn Dawūd" ; 20 January 1029 – 15 December 1072), real name Muhammad bin Dawud Chaghri, was the second Sultan of the Seljuk Empire and great-grandson of Seljuk, the eponymous founder of the dynasty. As Sultan, Alp Arslan greatly expanded Seljuk territory and consolidated power, defeating rivals to his south and northwest. His victory over the Byzantines at the Battle of Manzikert in 1071 ushered in the Turkish settlement of Anatolia. For his military prowess and fighting skills he obtained the name "Alp Arslan", which means "Heroic Lion" in Turkish.

Alp Arslan accompanied his uncle, Tughril Bey, on campaigns in the south against the Shia Fatimids while his father, Çağrı Bey, remained in Khorasan. Upon Alp Arslan's return to Khorasan, he began his work in administration at his father's suggestion. While there, his father introduced him to Nizam al-Mulk, one of the most eminent statesmen in early Muslim history and Alp Arslan's future vizier.

After the death of his father, Alp Arslan succeeded him as governor of Khorasan in 1059. His uncle Tughril died in 1063 and was succeeded by Suleiman, Arslan's brother. Arslan and his uncle Kutalmish both contested this succession. ("see" Battle of Damghan (1063)) Arslan defeated Kutalmish for the throne and succeeded on 27 April 1064 as sultan of Great Seljuq, thus becoming sole monarch of Persia from the river Oxus to the Tigris.

In consolidating his empire and subduing contending factions, Arslan was ably assisted by Nizam al-Mulk, and the two are credited with helping to stabilize the empire after the death of Tughril. With peace and security established in his dominions, Arslan convoked an assembly of the states and in 1066, he declared his son Malik Shah I his heir and successor. With the hope of capturing Caesarea Mazaca, the capital of Cappadocia, he placed himself at the head of the Turkish cavalry, crossed the Euphrates, and entered and invaded the city. Along with Nizam al-Mulk, he then marched into Armenia and Georgia, which he conquered in 1064. After a siege of 25 days, the Seljuks captured Ani, the capital city of Armenia. An account of the sack and massacres in Ani is given by the historian Sibt ibn al-Jawzi, who quotes an eyewitness saying:

En route to fight the Fatimids in Syria in 1068, Alp Arslan invaded the Byzantine Empire. The Emperor Romanos IV Diogenes, assuming command in person, met the invaders in Cilicia. In three arduous campaigns, the Turks were defeated in detail and driven across the Euphrates in 1070. The first two campaigns were conducted by the emperor himself, while the third was directed by Manuel Comnenos, great-uncle of Emperor Manuel Comnenos. During this time, Arslan gained the allegiance of Rashid al-Dawla Mahmud, the Mirdasid emir of Aleppo.

In 1071 Romanos again took the field and advanced into Armenia with possibly 30,000 men, including a contingent of Cuman Turks as well as contingents of Franks and Normans, under Ursel de Baieul. Alp Arslan, who had moved his troops south to fight the Fatimids, quickly reversed to meet the Byzantines. At Manzikert, on the Murat River, north of Lake Van, the two forces waged the Battle of Manzikert. The Cuman mercenaries among the Byzantine forces immediately defected to the Turkish side. Seeing this, "the Western mercenaries rode off and took no part in the battle." To be exact, Romanos was betrayed by general Andronikos Doukas, son of the Caesar (Romanos's stepson), who pronounced him dead and rode off with a large part of the Byzantine forces at a critical moment. The Byzantines were totally routed.
Emperor Romanos IV was himself taken prisoner and conducted into the presence of Alp Arslan. After a ritual humiliation, Arslan treated him with generosity. After peace terms were agreed to, Arslan dismissed the Emperor, loaded with presents and respectfully attended by a military guard. The following conversation is said to have taken place after Romanos was brought as a prisoner before the Sultan:

Alp Arslan's victories changed the balance in near Asia completely in favour of the Seljuq Turks and Sunni Muslims. While the Byzantine Empire was to continue for nearly four more centuries, and the Crusades would contest the issue for some time, the victory at Manzikert signalled the beginning of Turkish ascendancy in Anatolia. Most historians, including Edward Gibbon, date the defeat at Manzikert as the beginning of the end of the Eastern Roman Empire.

Alp Arslan's strength lay in the military realm. Domestic affairs were handled by his able vizier, Nizam al-Mulk, the founder of the administrative organization that characterized and strengthened the sultanate during the reigns of Alp Arslan and his son, Malik Shah. Military fiefs, governed by Seljuq princes, were established to provide support for the soldiery and to accommodate the nomadic Turks to the established Anatolian agricultural scene. This type of military fiefdom enabled the nomadic Turks to draw on the resources of the sedentary Persians, Turks, and other established cultures within the Seljuq realm, and allowed Alp Arslan to field a huge standing army without depending on tribute from conquest to pay his soldiers. He not only had enough food from his subjects to maintain his military, but the taxes collected from traders and merchants added to his coffers sufficiently to fund his continuous wars.

According to the poet Saadi Shirazi:
Suleiman ibn Kutalmish was the son of the contender for Arslan's throne; he was appointed governor of the north-western provinces and assigned to completing the invasion of Anatolia. An explanation for this choice can only be conjectured from Ibn al-Athir's account of the battle between Alp-Arslan and Kutalmish, in which he writes that Alp-Arslan wept for the latter's death and greatly mourned the loss of his kinsman.

After Manzikert, the dominion of Alp Arslan extended over much of western Asia. He soon prepared to march for the conquest of Turkestan, the original seat of his ancestors. With a powerful army he advanced to the banks of the Oxus. Before he could pass the river with safety, however, it was necessary to subdue certain fortresses, one of which was for several days vigorously defended by the governor, Yussuf al-Kharezmi, a Khwarezmian. He was obliged to surrender, however, and was carried as a prisoner before the sultan, who condemned him to death. Yussuf, in desperation, drew his dagger and rushed upon the sultan. Alp Arslan, who took great pride in his reputation as an archer, motioned to his guards not to interfere. He drew his bow, but his foot slipped, the arrow glanced aside, and he received the assassin's dagger in his chest. Alp Arslan died from this wound four days later, on 25 November 1072, in his 42nd year, and he was taken to Merv to be buried next to his father, Chaghri Beg.

Alp Arslan is widely regarded as having begun Anatolianism, although unintentionally. His victory at Manzikert is often cited as the beginning of the end of Byzantine power in Anatolia, and the beginning of Turkish identity there.

Alp Arslan's conquest of Anatolia from the Byzantines is also seen as one of the pivotal precursors to the launch of the crusades.

From 2002 to July 2008 under Turkmen calendar reform, the month of August was named after Alp Arslan.



</doc>
<doc id="869" url="https://en.wikipedia.org/wiki?curid=869" title="American Film Institute">
American Film Institute

The American Film Institute (AFI) is an American film organization that educates filmmakers and honors the heritage of the motion picture arts in the United States. AFI is supported by private funding and public membership fees.

The institute is composed of leaders from the film, entertainment, business, and academic communities. A board of trustees chaired by Sir Howard Stringer and a board of directors chaired by Robert A. Daly guide the organization, which is led by President and CEO, film historian Bob Gazzale. Prior leaders were founding director George Stevens, Jr. (from the organization's inception in 1967 until 1980) and Jean Picker Firstenberg (from 1980 to 2007).

The American Film Institute was founded by a 1965 presidential mandate announced in the Rose Garden of the White House by Lyndon B. Johnson—to establish a national arts organization to preserve the legacy of American film heritage, educate the next generation of filmmakers, and honor the artists and their work. Two years later, in 1967, AFI was established, supported by the National Endowment for the Arts, the Motion Picture Association of America and the Ford Foundation.

The original 22-member Board of Trustees included actor Gregory Peck as chairman and actor Sidney Poitier as vice-chairman, as well as director Francis Ford Coppola, film historian Arthur Schlesinger, Jr., lobbyist Jack Valenti, and other representatives from the arts and academia.

The institute established a training program for filmmakers known then as the Center for Advanced Film Studies. Also created in the early years were a repertory film exhibition program at the Kennedy Center for the Performing Arts and the AFI Catalog of Feature Films — a scholarly source for American film history. The institute moved to its current eight-acre Hollywood campus in 1981. The film training program grew into the AFI Conservatory, an accredited graduate school.

AFI moved its presentation of first-run and auteur films from the Kennedy Center to the historic AFI Silver Theatre and Cultural Center, which hosts the AFI DOCS film festival, making AFI the largest nonprofit film exhibitor in the world. AFI educates audiences and recognizes artistic excellence through its awards programs and 10 Top 10 Lists.

In 2017, then-aspiring filmmaker Ilana Bar-Din Giannini claimed that the AFI expelled her after she accused Dezso Magyar of sexually harassing her in the early 1980s.

AFI educational and cultural programs include:

In 1969, the institute established the AFI Conservatory for Advanced Film Studies at Greystone, the Doheny Mansion in Beverly Hills, California. The first class included filmmakers Terrence Malick, Caleb Deschanel, and Paul Schrader. That program grew into the AFI Conservatory, an accredited graduate film school located in the hills above Hollywood, California, providing training in six filmmaking disciplines: cinematography, directing, editing, producing, production design, and screenwriting. Mirroring a professional production environment, Fellows collaborate to make more films than any other graduate level program. Admission to AFI Conservatory is highly selective, with a maximum of 140 graduates per year.

In 2013, Emmy and Oscar-winning director, producer, and screenwriter James L. Brooks ("As Good as It Gets", "Broadcast News", "Terms of Endearment") joined AFI as Artistic Director of the AFI Conservatory where he provides leadership for the film program. Brooks' artistic role at the AFI Conservatory has a rich legacy that includes Daniel Petrie, Jr., Robert Wise, and Frank Pierson. Award-winning director Bob Mandel served as Dean of the AFI Conservatory for nine years. Jan Schuette took over as Dean in 2014 and served until 2017. Film Producer Richard Gladstein became Dean on July 1, 2017.

AFI Conservatory's alumni have careers in film, television and on the web. They have been recognized with all of the major industry awards—Academy Award, Emmy Award, guild awards, and the Tony Award.

Among the alumni of AFI are Andrea Arnold, ("Red Road", "Fish Tank"), Darren Aronofsky ("Requiem for a Dream", "Black Swan"), Carl Colpaert ("Gas Food Lodging", "Hurlyburly", "Swimming with Sharks"), Doug Ellin ("Entourage"), Todd Field ("In the Bedroom", "Little Children"), Jack Fisk ("Badlands", "Days of Heaven", "There Will Be Blood"), Carl Franklin ("One False Move", "Devil in a Blue Dress", "House of Cards"), Michael Greenspan ("Wrecked", "Kill for Me"), Patty Jenkins ("Monster", "Wonder Woman"), Janusz Kamiński ("Lincoln", "Schindler's List", "Saving Private Ryan"), Matthew Libatique ("Noah", "Black Swan"), David Lynch ("Mulholland Drive", "Blue Velvet"), Terrence Malick ("Days of Heaven", "The Thin Red Line", "The Tree of Life"), Victor Nuñez, ("Ruby in Paradise", "Ulee's Gold"), Wally Pfister ("Memento", "The Dark Knight", "Inception"), Robert Richardson ("Platoon", "JFK", "Django Unchained"), and many others.

The AFI Catalog, started in 1968, is a web-based filmographic database. A research tool for film historians, the catalog consists of entries on more than 60,000 feature films and 17,000 short films produced from 1893–2011, as well as AFI Awards Outstanding Movies of the Year from 2000 through 2010. Early print copies of this catalog may also be found at local libraries.

Created in 2000, the AFI Awards honor the ten outstanding films ("Movies of the Year") and ten outstanding television programs ("TV Programs of the Year"). The awards are a non-competitive acknowledgment of excellence.

The awards are announced in December, and a private luncheon for award honorees takes place the following January.

The AFI 100 Years... series, which ran from 1998 to 2008 and created jury-selected lists of America's best movies in categories such as Musicals, Laughs and Thrills, prompted new generations to experience classic American films. The juries consisted of over 1,500 artists, scholars, critics, and historians. "Citizen Kane" was voted the greatest American film twice.

AFI operates two film festivals: AFI Fest in Los Angeles, and AFI Docs (formally known as Silverdocs) in Silver Spring, Maryland, and Washington, D.C.

AFI Fest is the American Film Institute's annual celebration of artistic excellence. It is a showcase for the best festival films of the year and an opportunity for master filmmakers and emerging artists to come together with audiences in the movie capital of the world. It is the only festival of its stature that is free to the public. The Academy of Motion Picture Arts and Sciences recognizes AFI Fest as a qualifying festival for the Short Films category for the annual Academy Awards.

The festival has paid tribute to numerous influential filmmakers and artists over the years, including Agnès Varda, Pedro Almodóvar and David Lynch as guest artistic directors, and has screened scores of films that have produced Oscar nominations and wins. The American Film Market (AFM) is the market partner of AFI Fest. Audi is the festival's presenting sponsor. Additional sponsors include American Airlines and Stella Artois.

Held annually in June, AFI Docs (formerly Silverdocs) is a documentary festival in Washington, D.C. The festival attracts over 27,000 documentary enthusiasts.

The AFI Silver Theatre and Cultural Center is a moving image exhibition, education and cultural center located in Silver Spring, Maryland. Anchored by the restoration of noted architect John Eberson's historic 1938 Silver Theatre, it features 32,000 square feet of new construction housing two stadium theatres, office and meeting space, and reception and exhibit areas.

The AFI Silver Theatre and Cultural Center presents film and video programming, augmented by filmmaker interviews, panels, discussions, and musical performances.

The Directing Workshop for Women is a training program committed to educating and mentoring participants in an effort to increase the number of women working professionally in screen directing. In this tuition-free program, each participant is required to complete a short film by the end of the year-long program.

Alumnae of the program include Maya Angelou, Anne Bancroft, Dyan Cannon, Ellen Burstyn, Jennifer Getzinger, Lesli Linka Glatter, and Nancy Malone.

AFI released a set of hour-long programs reviewing the career of acclaimed directors. The Directors Series content was copyrighted in 1997 by Media Entertainment Inc and The American Film Institute, and the VHS and DVDs were released between 1999 and 2001 on Winstar TV and Video.

Directors featured included:





</doc>
<doc id="872" url="https://en.wikipedia.org/wiki?curid=872" title="Akira Kurosawa">
Akira Kurosawa

Akira Kurosawa ( or or "Kurosawa Akira"; March 23, 1910 – September 6, 1998) was a Japanese film director and screenwriter, who directed 30 films in a career spanning 57 years. He is regarded as one of the most important and influential filmmakers in the history of cinema.

Kurosawa entered the Japanese film industry in 1936, following a brief stint as a painter. After years of working on numerous films as an assistant director and scriptwriter, he made his debut as a director during World War II with the popular action film "Sanshiro Sugata" (a.k.a. "Judo Saga"). After the war, the critically acclaimed "Drunken Angel" (1948), in which Kurosawa cast then-unknown actor Toshiro Mifune in a starring role, cemented the director's reputation as one of the most important young filmmakers in Japan. The two men would go on to collaborate on another 15 films.

"Rashomon", which premiered in Tokyo, became the surprise winner of the Golden Lion at the 1951 Venice Film Festival. The commercial and critical success of that film opened up Western film markets for the first time to the products of the Japanese film industry, which in turn led to international recognition for other Japanese filmmakers. Kurosawa directed approximately one film per year throughout the 1950s and early 1960s, including a number of highly regarded (and often adapted) films, such as "Ikiru" (1952), "Seven Samurai" (1954) and "Yojimbo" (1961). After the 1960s he became much less prolific; even so, his later work—including his final two epics, "Kagemusha" (1980) and "Ran" (1985)—continued to win awards, though more often abroad than in Japan.

In 1990, he accepted the Academy Award for Lifetime Achievement. Posthumously, he was named "Asian of the Century" in the "Arts, Literature, and Culture" category by "AsianWeek" magazine and CNN, cited there as being among the five people who most prominently contributed to the improvement of Asia in the 20th century. His career has been honored by many retrospectives, critical studies and biographies in both print and video, and by releases in many consumer media formats.

Kurosawa was born on March 23, 1910, in Ōimachi in the Ōmori district of Tokyo. His father Isamu (1864–1948), a member of a samurai family from Akita Prefecture, worked as the director of the Army's Physical Education Institute's lower secondary school, while his mother Shima (1870–1952) came from a merchant's family living in Osaka. Akira was the eighth and youngest child of the moderately wealthy family, with two of his siblings already grown up at the time of his birth and one deceased, leaving Kurosawa to grow up with three sisters and a brother.

In addition to promoting physical exercise, Isamu Kurosawa was open to Western traditions and considered theater and motion pictures to have educational merit. He encouraged his children to watch films; young Akira viewed his first movies at the age of six. An important formative influence was his elementary school teacher Mr. Tachikawa, whose progressive educational practices ignited in his young pupil first a love of drawing and then an interest in education in general. During this time, the boy also studied calligraphy and Kendo swordsmanship.

Another major childhood influence was Heigo Kurosawa, Akira's older brother by four years. In the aftermath of the Great Kantō earthquake of 1923, which devastated Tokyo, Heigo took the 13-year-old Akira to view the devastation. When the younger brother wanted to look away from the human corpses and animal carcasses scattered everywhere, Heigo forbade him to do so, encouraging Akira instead to face his fears by confronting them directly. Some commentators have suggested that this incident would influence Kurosawa's later artistic career, as the director was seldom hesitant to confront unpleasant truths in his work.

Heigo was academically gifted, but soon after failing to secure a place in Tokyo's foremost high school, he began to detach himself from the rest of the family, preferring to concentrate on his interest in foreign literature. In the late 1920s, Heigo became a benshi (silent film narrator) for Tokyo theaters showing foreign films and quickly made a name for himself. Akira, who at this point planned to become a painter, moved in with him, and the two brothers became inseparable. With Heigo's guidance, Akira devoured not only films but also theater and circus performances, while exhibiting his paintings and working for the left-wing Proletarian Artists' League. However, he was never able to make a living with his art, and, as he began to perceive most of the proletarian movement as "putting unfulfilled political ideals directly onto the canvas", he lost his enthusiasm for painting.

With the increasing production of talking pictures in the early 1930s, film narrators like Heigo began to lose work, and Akira moved back in with his parents. In July 1933, Heigo committed suicide. Kurosawa has commented on the lasting sense of loss he felt at his brother's death and the chapter of his autobiography ("Something Like an Autobiography") that describes it—written nearly half a century after the event—is titled, "A Story I Don't Want to Tell". Only four months later, Kurosawa's eldest brother also died, leaving Akira, at age 23, the only one of the Kurosawa brothers still living, together with his three surviving sisters.

In 1935, the new film studio Photo Chemical Laboratories, known as P.C.L. (which later became the major studio, Toho), advertised for assistant directors. Although he had demonstrated no previous interest in film as a profession, Kurosawa submitted the required essay, which asked applicants to discuss the fundamental deficiencies of Japanese films and find ways to overcome them. His half-mocking view was that if the deficiencies were fundamental, there was no way to correct them. Kurosawa's essay earned him a call to take the follow-up exams, and director Kajirō Yamamoto, who was among the examiners, took a liking to Kurosawa and insisted that the studio hire him. The 25-year-old Kurosawa joined P.C.L. in February 1936.

During his five years as an assistant director, Kurosawa worked under numerous directors, but by far the most important figure in his development was Yamamoto. Of his 24 films as A.D., he worked on 17 under Yamamoto, many of them comedies featuring the popular actor Ken'ichi Enomoto, known as "Enoken". Yamamoto nurtured Kurosawa's talent, promoting him directly from third assistant director to chief assistant director after a year. Kurosawa's responsibilities increased, and he worked at tasks ranging from stage construction and film development to location scouting, script polishing, rehearsals, lighting, dubbing, editing, and second-unit directing. In the last of Kurosawa's films as an assistant director for Yamamoto, "Horse" ("Uma", 1941), Kurosawa took over most of the production, as his mentor was occupied with the shooting of another film.

One important piece of advice Yamamoto gave Kurosawa was that a good director needed to master screenwriting. Kurosawa soon realized that the potential earnings from his scripts were much higher than what he was paid as an assistant director. Kurosawa would later write or co-write all of his own films. He also frequently wrote screenplays for other directors such as for the film director Satsuo Yamamoto's film, "A Triumph of Wings" ("Tsubasa no gaika", 1942). This outside scriptwriting would serve Kurosawa as a lucrative sideline lasting well into the 1960s, long after he became world-famous.

In the two years following the release of "Horse" in 1941, Kurosawa searched for a story he could use to launch his directing career. Towards the end of 1942, about a year after the Japanese attack on Pearl Harbor, novelist Tsuneo Tomita published his Musashi Miyamoto-inspired judo novel, "Sanshiro Sugata", the advertisements for which intrigued Kurosawa. He bought the book on its publication day, devoured it in one sitting, and immediately asked Toho to secure the film rights. Kurosawa's initial instinct proved correct as, within a few days, three other major Japanese studios also offered to buy the rights. Toho prevailed, and Kurosawa began pre-production on his debut work as director.
Shooting of "Sanshiro Sugata" began on location in Yokohama in December 1942. Production proceeded smoothly, but getting the completed film past the censors was an entirely different matter. The censorship office considered the work to be objectionably "British-American" by the standards of wartime Japan, and it was only through the intervention of director Yasujirō Ozu, who championed the film, that "Sanshiro Sugata" was finally accepted for release on March 25, 1943. (Kurosawa had just turned 33.) The movie became both a critical and commercial success. Nevertheless, the censorship office would later decide to cut out some 18 minutes of footage, much of which is now considered lost.

He next turned to the subject of wartime female factory workers in "The Most Beautiful", a propaganda film which he shot in a semi-documentary style in early 1944. In order to coax realistic performances from his actresses, the director had them live in a real factory during the shoot, eat the factory food and call each other by their character names. He would use similar methods with his performers throughout his career.

During production, the actress playing the leader of the factory workers, Yōko Yaguchi, was chosen by her colleagues to present their demands to the director. She and Kurosawa were constantly at loggerheads, and it was through these arguments that the two, paradoxically, became close. They married on May 21, 1945, with Yaguchi two months pregnant (she never resumed her acting career), and the couple would remain together until her death in 1985. They had two children, both surviving Kurosawa : a son, Hisao, born December 20, 1945, who served as producer on some of his father's last projects, and Kazuko, a daughter, born April 29, 1954, who became a costume designer.

Shortly before his marriage, Kurosawa was pressured by the studio against his will to direct a sequel to his debut film. The often blatantly propagandistic "Sanshiro Sugata Part II", which premiered in May 1945, is generally considered one of his weakest pictures.

Kurosawa decided to write the script for a film that would be both censor-friendly and less expensive to produce. "The Men Who Tread on the Tiger's Tail", based on the Kabuki play "Kanjinchō" and starring the comedian Enoken, with whom Kurosawa had often worked during his assistant director days, was completed in September 1945. By this time, Japan had surrendered and the occupation of Japan had begun. The new American censors interpreted the values allegedly promoted in the picture as overly "feudal" and banned the work. (It was not released until 1952, the year another Kurosawa film, "Ikiru", was also released.) Ironically, while in production, the film had already been savaged by Japanese wartime censors as too Western and "democratic" (they particularly disliked the comic porter played by Enoken), so the movie most probably would not have seen the light of day even if the war had continued beyond its completion.

After the war, Kurosawa, influenced by the democratic ideals of the Occupation, sought to make films that would establish a new respect towards the individual and the self. The first such film, "No Regrets for Our Youth" (1946), inspired by both the 1933 Takigawa incident and the Hotsumi Ozaki wartime spy case, criticized Japan's prewar regime for its political oppression. Atypically for the director, the heroic central character is a woman, Yukie (Setsuko Hara), who, born into upper-middle-class privilege, comes to question her values in a time of political crisis. The original script had to be extensively rewritten and, because of its controversial theme (and because the protagonist was a woman), the completed work divided critics, but it nevertheless managed to win the approval of audiences, who turned variations on the film's title into a postwar catchphrase.

His next film, "One Wonderful Sunday" premiered in July 1947 to mixed reviews. It is a relatively uncomplicated and sentimental love story dealing with an impoverished postwar couple trying to enjoy, within the devastation of postwar Tokyo, their one weekly day off. The movie bears the influence of Frank Capra, D. W. Griffith and F. W. Murnau, each of whom was among Kurosawa's favorite directors. Another film released in 1947 with Kurosawa's involvement was the action-adventure thriller, "Snow Trail", directed by Senkichi Taniguchi from Kurosawa's screenplay. It marked the debut of the intense young actor Toshiro Mifune. It was Kurosawa who, with his mentor Yamamoto, had intervened to persuade Toho to sign Mifune, during an audition in which the young man greatly impressed Kurosawa, but managed to alienate most of the other judges.
"Drunken Angel" is often considered the director's first major work. Although the script, like all of Kurosawa's occupation-era works, had to go through forced rewrites due to American censorship, Kurosawa felt that this was the first film in which he was able to express himself freely. A grittily realistic story of a doctor who tries to save a gangster (yakuza) with tuberculosis, it was also the director's first film with Toshiro Mifune, who would proceed to play either the main or a major character in all but one ("Ikiru") of the director's next 16 films. While Mifune was not cast as the protagonist in "Drunken Angel", his explosive performance as the gangster so dominates the drama that he shifted the focus from the title character, the alcoholic doctor played by Takashi Shimura, who had already appeared in several Kurosawa movies. However, Kurosawa did not want to smother the young actor's immense vitality, and Mifune's rebellious character electrified audiences in much the way that Marlon Brando's defiant stance would startle American film audiences a few years later. The film premiered in Tokyo in April 1948 to rave reviews and was chosen by the prestigious Kinema Junpo critics poll as the best film of its year, the first of three Kurosawa movies to be so honored.

Kurosawa, with producer Sōjirō Motoki and fellow directors and friends Kajiro Yamamoto, Mikio Naruse and Senkichi Taniguchi, formed a new independent production unit called Film Art Association (Eiga Geijutsu Kyōkai). For this organization's debut work, and first film for Daiei studios, Kurosawa turned to a contemporary play by Kazuo Kikuta and, together with Taniguchi, adapted it for the screen. "The Quiet Duel" starred Toshiro Mifune as an idealistic young doctor struggling with syphilis, a deliberate attempt by Kurosawa to break the actor away from being typecast as gangsters. Released in March 1949, it was a box office success, but is generally considered one of the director's lesser achievements.

His second film of 1949, also produced by Film Art Association and released by Shintoho, was "Stray Dog". It is a detective movie (perhaps the first important Japanese film in that genre) that explores the mood of Japan during its painful postwar recovery through the story of a young detective, played by Mifune, and his fixation on the recovery of his handgun, which was stolen by a penniless war veteran who proceeds to use it to rob and murder. Adapted from an unpublished novel by Kurosawa in the style of a favorite writer of his, Georges Simenon, it was the director's first collaboration with screenwriter Ryuzo Kikushima, who would later help to script eight other Kurosawa films. A famous, virtually wordless sequence, lasting over eight minutes, shows the detective, disguised as an impoverished veteran, wandering the streets in search of the gun thief; it employed actual documentary footage of war-ravaged Tokyo neighborhoods shot by Kurosawa's friend, Ishirō Honda, the future director of "Godzilla". The film is considered a precursor to the contemporary police procedural and buddy cop film genres.

"Scandal", released by Shochiku in April 1950, was inspired by the director's personal experiences with, and anger towards, Japanese yellow journalism. The work is an ambitious mixture of courtroom drama and social problem film about free speech and personal responsibility, but even Kurosawa regarded the finished product as dramatically unfocused and unsatisfactory, and almost all critics agree. However, it would be Kurosawa's second film of 1950, "Rashomon", that would ultimately win him, and Japanese cinema, a whole new international audience.

After finishing "Scandal", Kurosawa was approached by Daiei studios, which asked the director to make another film for them. Kurosawa picked a script by an aspiring young screenwriter, Shinobu Hashimoto, who would eventually work on nine of his films. Their first joint effort was based on Ryūnosuke Akutagawa's experimental short story "In a Grove", which recounts the murder of a samurai and the rape of his wife from various different and conflicting points-of-view. Kurosawa saw potential in the script, and with Hashimoto's help, polished and expanded it and then pitched it to Daiei, who were happy to accept the project due to its low budget.

The shooting of "Rashomon" began on July 7, 1950, and, after extensive location work in the primeval forest of Nara, wrapped on August 17. Just one week was spent in hurried post-production, hampered by a studio fire, and the finished film premiered at Tokyo's Imperial Theatre on August 25, expanding nationwide the following day. The movie was met by lukewarm reviews, with many critics puzzled by its unique theme and treatment, but it was nevertheless a moderate financial success for Daiei.
Kurosawa's next film, for Shochiku, was "The Idiot", an adaptation of the novel by the director's favorite writer, Fyodor Dostoyevsky. The filmmaker relocated the story from Russia to Hokkaido, but it is otherwise very faithful to the original, a fact seen by many critics as detrimental to the work. A studio-mandated edit shortened it from Kurosawa's original cut of 265 minutes (nearly four-and-a-half hours) to just 166 minutes, making the resulting narrative exceedingly difficult to follow. The severely edited film version is widely considered today to be one of the director's least successful works and the original full length version no longer exists. Contemporary reviews of the much shortened edited version were very negative, but the film was a moderate success at the box office, largely because of the popularity of one of its stars, Setsuko Hara.

Meanwhile, unbeknownst to Kurosawa, "Rashomon" had been entered in the prestigious Venice Film Festival, due to the efforts of Giuliana Stramigioli, a Japan-based representative of an Italian film company, who had seen and admired the movie and convinced Daiei to submit it. On September 10, 1951, "Rashomon" was awarded the festival's highest prize, the Golden Lion, shocking not only Daiei but the international film world, which at the time was largely unaware of Japan's decades-old cinematic tradition.

After Daiei very briefly exhibited a subtitled print of the film in Los Angeles, RKO purchased distribution rights to "Rashomon" in the United States. The company was taking a considerable gamble. It had put out only one prior subtitled film in the American market, and the only previous Japanese talkie commercially released in New York had been Mikio Naruse's comedy, "Wife! Be Like a Rose", in 1937: a critical and box-office flop. However, "Rashomon"s commercial run, greatly helped by strong reviews from critics and even the columnist Ed Sullivan, was very successful. (It earned $35,000 in its first three weeks at a single New York theater, an almost unheard-of sum at the time.)

This success in turn led to a vogue in America and the West for Japanese movies throughout the 1950s, replacing the enthusiasm for Italian neorealist cinema. For example, by the end of 1952 "Rashomon" was released in Japan, the United States, and most of Europe. Among the Japanese filmmakers whose work, as a result, began to win festival prizes and commercial release in the West were Kenji Mizoguchi ("The Life of Oharu", "Ugetsu", "Sansho the Bailiff") and, somewhat later, Yasujirō Ozu ("Tokyo Story", "An Autumn Afternoon")—artists highly respected in Japan but, prior to this period, almost totally unknown in the West. Kurosawa's growing reputation among Western audiences in the 1950s would make Western audiences more sympathetic to the reception of later generations of Japanese filmmakers ranging from Kon Ichikawa, Masaki Kobayashi, Nagisa Oshima and Shohei Imamura to Juzo Itami, Takeshi Kitano and Takashi Miike.

His career boosted by his sudden international fame, Kurosawa, now reunited with his original film studio, Toho (which would go on to produce his next 11 films), set to work on his next project, "Ikiru". The movie stars Takashi Shimura as a cancer-ridden Tokyo bureaucrat, Watanabe, on a final quest for meaning before his death. For the screenplay, Kurosawa brought in Hashimoto as well as writer Hideo Oguni, who would go on to co-write 12 Kurosawa films. Despite the work's grim subject matter, the screenwriters took a satirical approach, which some have compared to the work of Brecht, to both the bureaucratic world of its hero and the U.S. cultural colonization of Japan. (American pop songs figure prominently in the film.) Because of this strategy, the filmmakers are usually credited with saving the picture from the kind of sentimentality common to dramas about characters with terminal illnesses. "Ikiru" opened in October 1952 to rave reviews—it won Kurosawa his second Kinema Junpo "Best Film" award—and enormous box office success. It remains the most acclaimed of all the artist's films set in the modern era.

In December 1952, Kurosawa took his "Ikiru" screenwriters, Shinobu Hashimoto and Hideo Oguni, for a forty-five-day secluded residence at an inn to create the screenplay for his next movie, "Seven Samurai". The ensemble work was Kurosawa's first proper samurai film, the genre for which he would become most famous. The simple story, about a poor farming village in Sengoku period Japan that hires a group of samurai to defend it against an impending attack by bandits, was given a full epic treatment, with a huge cast (largely consisting of veterans of previous Kurosawa productions) and meticulously detailed action, stretching out to almost three-and-a-half hours of screen time.

Three months were spent in pre-production and a month in rehearsals. Shooting took up 148 days spread over almost a year, interrupted by production and financing troubles and Kurosawa's health problems. The film finally opened in April 1954, half a year behind its original release date and about three times over budget, making it at the time the most expensive Japanese film ever made. (However, by Hollywood standards, it was a quite modestly budgeted production, even for that time). The film received positive critical reaction and became a big hit, quickly making back the money invested in it and providing the studio with a product that they could, and did, market internationally—though with extensive edits. Over time—and with the theatrical and home video releases of the uncut version—its reputation has steadily grown. It is now regarded by some commentators as the greatest Japanese film ever made, and in 1979, a poll of Japanese film critics also voted it the best Japanese film ever made. In the most recent (2012) version of the widely respected British Film Institute (BFI) "Sight & Sound" "Greatest Films of All Time" poll, "Seven Samurai" placed 17th among all films from all countries in both the critics' and the directors' polls, receiving a place in the Top Ten lists of 48 critics and 22 directors.

In 1954, nuclear tests in the Pacific were causing radioactive rainstorms in Japan and one particular incident in March had exposed a Japanese fishing boat to nuclear fallout, with disastrous results. It is in this anxious atmosphere that Kurosawa's next film, "Record of a Living Being", was conceived. The story concerned an elderly factory owner (Toshiro Mifune) so terrified of the prospect of a nuclear attack that he becomes determined to move his entire extended family (both legal and extra-marital) to what he imagines is the safety of a farm in Brazil. Production went much more smoothly than the director's previous film, but a few days before shooting ended, Kurosawa's composer, collaborator and close friend Fumio Hayasaka died (of tuberculosis) at the age of 41. The film's score was finished by Hayasaka's student, Masaru Sato, who would go on to score all of Kurosawa's next eight films. "Record of a Living Being" opened in November 1955 to mixed reviews and muted audience reaction, becoming the first Kurosawa film to lose money during its original theatrical run. Today, it is considered by many to be among the finest films dealing with the psychological effects of the global nuclear stalemate.

Kurosawa's next project, "Throne of Blood", an adaptation of William Shakespeare's "Macbeth"—set, like "Seven Samurai", in the Sengoku Era—represented an ambitious transposition of the English work into a Japanese context. Kurosawa instructed his leading actress, Isuzu Yamada, to regard the work as if it were a cinematic version of a "Japanese" rather than a European literary classic. Given Kurosawa's appreciation of traditional Japanese stage acting, the acting of the players, particularly Yamada, draws heavily on the stylized techniques of the Noh theater. It was filmed in 1956 and released in January 1957 to a slightly less negative domestic response than had been the case with the director's previous film. Abroad, "Throne of Blood", regardless of the liberties it takes with its source material, quickly earned a place among the most celebrated Shakespeare adaptations.

Another adaptation of a classic European theatrical work followed almost immediately, with production of "The Lower Depths", based on a play by Maxim Gorky, taking place in May and June 1957. In contrast to the Shakespearean sweep of "Throne of Blood", "The Lower Depths" was shot on only two confined sets, in order to emphasize the restricted nature of the characters' lives. Though faithful to the play, this adaptation of Russian material to a completely Japanese setting—in this case, the late Edo period—unlike his earlier "The Idiot", was regarded as artistically successful. The film premiered in September 1957, receiving a mixed response similar to that of "Throne of Blood". However, some critics rank it among the director's most underrated works.

Kurosawa's three consecutive movies after "Seven Samurai" had not managed to capture Japanese audiences in the way that that film had. The mood of the director's work had been growing increasingly pessimistic and dark, with the possibility of redemption through personal responsibility now very much questioned, particularly in "Throne of Blood" and "The Lower Depths". He recognized this, and deliberately aimed for a more light-hearted and entertaining film for his next production, while switching to the new widescreen format that had been gaining popularity in Japan. The resulting film, "The Hidden Fortress", is an action-adventure comedy-drama about a medieval princess, her loyal general and two peasants who all need to travel through enemy lines in order to reach their home region. Released in December 1958, "The Hidden Fortress" became an enormous box office success in Japan and was warmly received by critics both in Japan and abroad. Today, the film is considered one of Kurosawa's most lightweight efforts, though it remains popular, not least because it is one of several major influences on George Lucas's 1977 space opera, "Star Wars".

Starting with "Rashomon", Kurosawa's productions had become increasingly large in scope and so had the director's budgets. Toho, concerned about this development, suggested that he might help finance his own works, therefore making the studio's potential losses smaller, while in turn allowing himself more artistic freedom as co-producer. Kurosawa agreed, and the Kurosawa Production Company was established in April 1959, with Toho as the majority shareholder.

Despite risking his own money, Kurosawa chose a story that was more directly critical of the Japanese business and political elites than any previous work. "The Bad Sleep Well", based on a script by Kurosawa's nephew Mike Inoue, is a revenge drama about a young man who is able to infiltrate the hierarchy of a corrupt Japanese company with the intention of exposing the men responsible for his father's death. Its theme proved topical: while the film was in production, mass demonstrations were held against the new U.S.–Japan Security treaty, which was seen by many Japanese, particularly the young, as threatening the country's democracy by giving too much power to corporations and politicians. The film opened in September 1960 to positive critical reaction and modest box office success. The 25-minute opening sequence depicting a corporate wedding reception is widely regarded as one of Kurosawa's most skillfully executed set pieces, but the remainder of the film is often perceived as disappointing by comparison. The movie has also been criticized for employing the conventional Kurosawan hero to combat a social evil that cannot be resolved through the actions of individuals, however courageous or cunning.

"Yojimbo" ("The Bodyguard"), Kurosawa Production's second film, centers on a masterless samurai, Sanjuro, who strolls into a 19th-century town ruled by two opposing violent factions and provokes them into destroying each other. The director used this work to play with many genre conventions, particularly the Western, while at the same time offering an unprecedentedly (for the Japanese screen) graphic portrayal of violence. Some commentators have seen the Sanjuro character in this film as a fantasy figure who magically reverses the historical triumph of the corrupt merchant class over the samurai class. Featuring Tatsuya Nakadai in his first major role in a Kurosawa movie, and with innovative photography by Kazuo Miyagawa (who shot "Rashomon") and Takao Saito, the film premiered in April 1961 and was a critically and commercially successful venture, earning more than any previous Kurosawa film. The movie and its blackly comic tone were also widely imitated abroad. Sergio Leone's "A Fistful of Dollars" was a virtual (unauthorized) scene-by-scene remake with Toho filing a lawsuit on Kurosawa's behalf and prevailing.
Following the success of "Yojimbo", Kurosawa found himself under pressure from Toho to create a sequel. Kurosawa turned to a script he had written before "Yojimbo", reworking it to include the hero of his previous film. "Sanjuro" was the first of three Kurosawa films to be adapted from the work of the writer Shūgorō Yamamoto (the others would be "Red Beard" and "Dodeskaden"). It is lighter in tone and closer to a conventional period film than "Yojimbo", though its story of a power struggle within a samurai clan is portrayed with strongly comic undertones. The film opened on January 1, 1962, quickly surpassing "Yojimbo"s box office success and garnering positive reviews.

Kurosawa had meanwhile instructed Toho to purchase the film rights to "King's Ransom", a novel about a kidnapping written by American author and screenwriter Evan Hunter, under his pseudonym of Ed McBain, as one of his 87th Precinct series of crime books. The director intended to create a work condemning kidnapping, which he considered one of the very worst crimes. The suspense film, titled "High and Low", was shot during the latter half of 1962 and released in March 1963. It broke Kurosawa's box office record (the third film in a row to do so), became the highest grossing Japanese film of the year, and won glowing reviews. However, his triumph was somewhat tarnished when, ironically, the film was blamed for a wave of kidnappings which occurred in Japan about this time (he himself received kidnapping threats directed at his young daughter, Kazuko). "High and Low" is considered by many commentators to be among the director's strongest works.

Kurosawa quickly moved on to his next project, "Red Beard". Based on a short story collection by Shūgorō Yamamoto and incorporating elements from Dostoyevsky's novel "The Insulted and Injured", it is a period film, set in a mid-nineteenth century clinic for the poor, in which Kurosawa's humanist themes receive perhaps their fullest statement. A conceited and materialistic, foreign-trained young doctor, Yasumoto, is forced to become an intern at the clinic under the stern tutelage of Doctor Niide, known as "Akahige" ("Red Beard"), played by Mifune. Although he resists Red Beard initially, Yasumoto comes to admire his wisdom and courage, and to perceive the patients at the clinic, whom he at first despised, as worthy of compassion and dignity.

Yūzō Kayama, who plays Yasumoto, was an extremely popular film and music star at the time, particularly for his "Young Guy" ("Wakadaishō") series of musical comedies, so signing him to appear in the film virtually guaranteed Kurosawa strong box-office. The shoot, the filmmaker's longest ever, lasted well over a year (after five months of pre-production), and wrapped in spring 1965, leaving the director, his crew and his actors exhausted. "Red Beard" premiered in April 1965, becoming the year's highest-grossing Japanese production and the third (and last) Kurosawa film to top the prestigious Kinema Jumpo yearly critics poll. It remains one of Kurosawa's best-known and most-loved works in his native country. Outside Japan, critics have been much more divided. Most commentators concede its technical merits and some praise it as among Kurosawa's best, while others insist that it lacks complexity and genuine narrative power, with still others claiming that it represents a retreat from the artist's previous commitment to social and political change.

The film marked something of an end of an era for its creator. The director himself recognized this at the time of its release, telling critic Donald Richie that a cycle of some kind had just come to an end and that his future films and production methods would be different. His prediction proved quite accurate. Beginning in the late 1950s, television began increasingly to dominate the leisure time of the formerly large and loyal Japanese cinema audience. And as film company revenues dropped, so did their appetite for risk—particularly the risk represented by Kurosawa's costly production methods.

"Red Beard" also marked the midway point, chronologically, in the artist's career. During his previous twenty-nine years in the film industry (which includes his five years as assistant director), he had directed twenty-three films, while during the remaining twenty-eight years, for many and complex reasons, he would complete only seven more. Also, for reasons never adequately explained, "Red Beard" would be his final film starring Toshiro Mifune. Yu Fujiki, an actor who worked on "The Lower Depths", observed, regarding the closeness of the two men on the set, "Mr. Kurosawa's heart was in Mr. Mifune's body." Donald Richie has described the rapport between them as a unique "symbiosis".

When Kurosawa's exclusive contract with Toho came to an end in 1966, the 56-year-old director was seriously contemplating change. Observing the troubled state of the domestic film industry, and having already received dozens of offers from abroad, the idea of working outside Japan appealed to him as never before.

For his first foreign project, Kurosawa chose a story based on a "Life" magazine article. The Embassy Pictures action thriller, to be filmed in English and called simply "Runaway Train", would have been his first in color. But the language barrier proved a major problem, and the English version of the screenplay was not even finished by the time filming was to begin in autumn 1966. The shoot, which required snow, was moved to autumn 1967, then canceled in 1968. Almost two decades later, another foreign director working in Hollywood, Andrei Konchalovsky, finally made "Runaway Train" (1985), though from a new script loosely based on Kurosawa's.

The director meanwhile had become involved in a much more ambitious Hollywood project. "Tora! Tora! Tora!", produced by 20th Century Fox and Kurosawa Production, would be a portrayal of the Japanese attack on Pearl Harbor from both the American and the Japanese points-of-view, with Kurosawa helming the Japanese half and an English-speaking filmmaker directing the American half. He spent several months working on the script with Ryuzo Kikushima and Hideo Oguni, but very soon the project began to unravel. The director of the American sequences turned out not to be David Lean, as originally planned, but American Richard Fleischer. The budget was also cut, and the screen time allocated for the Japanese segment would now be no longer than 90 minutes—a major problem, considering that Kurosawa's script ran over four hours. After numerous revisions with the direct involvement of Darryl Zanuck, a more or less finalized cut screenplay was agreed upon in May 1968.

Shooting began in early December, but Kurosawa would last only a little over three weeks as director. He struggled to work with an unfamiliar crew and the requirements of a Hollywood production, while his working methods puzzled his American producers, who ultimately concluded that the director must be mentally ill. Kurosawa was examined at Kyoto University Hospital by a neuropsychologist, Dr. Murakami, whose diagnosis was forwarded to Darryl Zanuck and Richard Zanuck at Fox studios indicating a diagnosis of neurasthenia stating that, "He is suffering from disturbance of sleep, agitated with feelings of anxiety and in manic excitement caused by the above mentioned illness. It is necessary for him to have rest and medical treatment for more than two months." On Christmas Eve 1968, the Americans announced that Kurosawa had left the production due to "fatigue", effectively firing him. He was ultimately replaced, for the film's Japanese sequences, with two directors, Kinji Fukasaku and Toshio Masuda.

"Tora! Tora! Tora!", finally released to unenthusiastic reviews in September 1970, was, as Donald Richie put it, an "almost unmitigated tragedy" in Kurosawa's career. He had spent years of his life on a logistically nightmarish project to which he ultimately did not contribute a foot of film shot by himself. (He had his name removed from the credits, though the script used for the Japanese half was still his and his co-writers'.) He became estranged from his longtime collaborator, writer Ryuzo Kikushima, and never worked with him again. The project had inadvertently exposed corruption in his own production company (a situation reminiscent of his own movie, "The Bad Sleep Well"). His very sanity had been called into question. Worst of all, the Japanese film industry—and perhaps the man himself—began to suspect that he would never make another film.

Knowing that his reputation was at stake following the much publicised "Tora! Tora! Tora!" debacle, Kurosawa moved quickly to a new project to prove he was still viable. To his aid came friends and famed directors Keisuke Kinoshita, Masaki Kobayashi and Kon Ichikawa, who together with Kurosawa established in July 1969 a production company called the Club of the Four Knights (Yonki no kai). Although the plan was for the four directors to create a film each, it has been suggested that the real motivation for the other three directors was to make it easier for Kurosawa to successfully complete a film, and therefore find his way back into the business.

The first project proposed and worked on was a period film to be called "Dora-heita", but when this was deemed too expensive, attention shifted to "Dodesukaden", an adaptation of yet another Shūgorō Yamamoto work, again about the poor and destitute. The film was shot quickly (by Kurosawa's standards) in about nine weeks, with Kurosawa determined to show he was still capable of working quickly and efficiently within a limited budget. For his first work in color, the dynamic editing and complex compositions of his earlier pictures were set aside, with the artist focusing on the creation of a bold, almost surreal palette of primary colors, in order to reveal the toxic environment in which the characters live. It was released in Japan in October 1970, but though a minor critical success, it was greeted with audience indifference. The picture lost money and caused the Club of the Four Knights to dissolve. Initial reception abroad was somewhat more favorable, but "Dodesukaden" has since been typically considered an interesting experiment not comparable to the director's best work.

Unable to secure funding for further work and allegedly suffering from health problems, Kurosawa apparently reached the breaking point: on December 22, 1971, he slit his wrists and throat multiple times. The suicide attempt proved unsuccessful and the director's health recovered fairly quickly, with Kurosawa now taking refuge in domestic life, uncertain if he would ever direct another film.

In early 1973, the Soviet studio Mosfilm approached the filmmaker to ask if he would be interested in working with them. Kurosawa proposed an adaptation of Russian explorer Vladimir Arsenyev's autobiographical work "Dersu Uzala". The book, about a Goldi hunter who lives in harmony with nature until destroyed by encroaching civilization, was one that he had wanted to make since the 1930s. In December 1973, the 63-year-old Kurosawa set off for the Soviet Union with four of his closest aides, beginning a year-and-a-half stay in the country. Shooting began in May 1974 in Siberia, with filming in exceedingly harsh natural conditions proving very difficult and demanding. The picture wrapped in April 1975, with a thoroughly exhausted and homesick Kurosawa returning to Japan and his family in June. "Dersu Uzala" had its world premiere in Japan on August 2, 1975, and did well at the box office. While critical reception in Japan was muted, the film was better reviewed abroad, winning the Golden Prize at the 9th Moscow International Film Festival, as well as an Academy Award for Best Foreign Language Film. Today, critics remain divided over the film: some see it as an example of Kurosawa's alleged artistic decline, while others count it among his finest works.

Although proposals for television projects were submitted to him, he had no interest in working outside the film world. Nevertheless, the hard-drinking director did agree to appear in a series of television ads for Suntory whiskey, which aired in 1976. While fearing that he might never be able to make another film, the director nevertheless continued working on various projects, writing scripts and creating detailed illustrations, intending to leave behind a visual record of his plans in case he would never be able to film his stories.

In 1977, American director George Lucas released "Star Wars", a wildly successful science fiction film influenced by Kurosawa's "The Hidden Fortress", among other works. Lucas, like many other New Hollywood directors, revered Kurosawa and considered him a role model, and was shocked to discover that the Japanese filmmaker was unable to secure financing for any new work. The two met in San Francisco in July 1978 to discuss the project Kurosawa considered most financially viable: "Kagemusha", the epic story of a thief hired as the double of a medieval Japanese lord of a great clan. Lucas, enthralled by the screenplay and Kurosawa's illustrations, leveraged his influence over 20th Century Fox to coerce the studio that had fired Kurosawa just ten years earlier to produce "Kagemusha", then recruited fellow fan Francis Ford Coppola as co-producer.

Production began the following April, with Kurosawa in high spirits. Shooting lasted from June 1979 through March 1980 and was plagued with problems, not the least of which was the firing of the original lead actor, Shintaro Katsu—creator of the very popular Zatoichi character—due to an incident in which the actor insisted, against the director's wishes, on videotaping his own performance. (He was replaced by Tatsuya Nakadai, in his first of two consecutive leading roles in a Kurosawa movie.) The film was completed only a few weeks behind schedule and opened in Tokyo in April 1980. It quickly became a massive hit in Japan. The film was also a critical and box office success abroad, winning the coveted Palme d'Or at the 1980 Cannes Film Festival in May, though some critics, then and now, have faulted the film for its alleged coldness. Kurosawa spent much of the rest of the year in Europe and America promoting "Kagemusha", collecting awards and accolades, and exhibiting as art the drawings he had made to serve as storyboards for the film.
The international success of "Kagemusha" allowed Kurosawa to proceed with his next project, "Ran", another epic in a similar vein. The script, partly based on William Shakespeare's "King Lear", depicted a ruthless, bloodthirsty "daimyō" (warlord), played by Tatsuya Nakadai, who, after foolishly banishing his one loyal son, surrenders his kingdom to his other two sons, who then betray him, thus plunging the entire kingdom into war. As Japanese studios still felt wary about producing another film that would rank among the most expensive ever made in the country, international help was again needed. This time it came from French producer Serge Silberman, who had produced Luis Buñuel's final movies. Filming did not begin until December 1983 and lasted more than a year.

In January 1985, production of "Ran" was halted as Kurosawa's 64-year-old wife Yōko fell ill. She died on February 1. Kurosawa returned to finish his film and "Ran" premiered at the Tokyo Film Festival on 31 May, with a wide release the next day. The film was a moderate financial success in Japan, but a larger one abroad and, as he had done with "Kagemusha", Kurosawa embarked on a trip to Europe and America, where he attended the film's premieres in September and October.

"Ran" won several awards in Japan, but was not quite as honored there as many of the director's best films of the 1950s and 1960s had been. The film world was surprised, however, when Japan passed over the selection of "Ran" in favor of another film as its official entry to compete for an Oscar nomination in the Best Foreign Film category, which was ultimately rejected for competition at the 58th Academy Awards. Both the producer and Kurosawa himself attributed the failure to even submit "Ran" for competition to a misunderstanding: because of the Academy's arcane rules, no one was sure whether "Ran" qualified as a "Japanese" film, a "French" film (due to its financing), or both, so it was not submitted at all. In response to what at least appeared to be a blatant snub by his own countrymen, the director Sidney Lumet led a successful campaign to have Kurosawa receive an Oscar nomination for Best Director that year (Sydney Pollack ultimately won the award for directing "Out of Africa"). "Ran"s costume designer, Emi Wada, won the movie's only Oscar.

"Kagemusha" and "Ran", particularly the latter, are often considered to be among Kurosawa's finest works. After "Ran"s release, Kurosawa would point to it as his best film, a major change of attitude for the director who, when asked which of his works was his best, had always previously answered "my next one".
For his next movie, Kurosawa chose a subject very different from any that he had ever filmed before. While some of his previous pictures (for example, "Drunken Angel" and "Kagemusha") had included brief dream sequences, "Dreams" was to be entirely based upon the director's own dreams. Significantly, for the first time in over forty years, Kurosawa, for this deeply personal project, wrote the screenplay alone. Although its estimated budget was lower than the films immediately preceding it, Japanese studios were still unwilling to back one of his productions, so Kurosawa turned to another famous American fan, Steven Spielberg, who convinced Warner Bros. to buy the international rights to the completed film. This made it easier for Kurosawa's son, Hisao, as co-producer and soon-to-be head of Kurosawa Production, to negotiate a loan in Japan that would cover the film's production costs. Shooting took more than eight months to complete, and "Dreams" premiered at Cannes in May 1990 to a polite but muted reception, similar to the reaction the picture would generate elsewhere in the world. In 1990, he accepted the Academy Award for Lifetime Achievement.
Kurosawa now turned to a more conventional story with "Rhapsody in August"—the director's first film fully produced in Japan since "Dodeskaden" over twenty years before—which explored the scars of the nuclear bombing which destroyed Nagasaki at the very end of World War II. It was adapted from a Kiyoko Murata novel, but the film's references to the Nagasaki bombing came from the director rather than from the book. This was his only movie to include a role for an American movie star: Richard Gere, who plays a small role as the nephew of the elderly heroine. Shooting took place in early 1991, with the film opening on 25 May that year to a largely negative critical reaction, especially in the United States, where the director was accused of promulgating naïvely anti-American sentiments, though Kurosawa rejected these accusations.

Kurosawa wasted no time moving onto his next project: "Madadayo", or "Not Yet". Based on autobiographical essays by Hyakken Uchida, the film follows the life of a Japanese professor of German through the Second World War and beyond. The narrative centers on yearly birthday celebrations with his former students, during which the protagonist declares his unwillingness to die just yet—a theme that was becoming increasingly relevant for the film's 81-year-old creator. Filming began in February 1992 and wrapped by the end of September. Its release on April 17, 1993, was greeted by an even more disappointed reaction than had been the case with his two preceding works.

Kurosawa nevertheless continued to work. He wrote the original screenplays "The Sea is Watching" in 1993 and "After the Rain" in 1995. While putting finishing touches on the latter work in 1995, Kurosawa slipped and broke the base of his spine. Following the accident, he would use a wheelchair for the rest of his life, putting an end to any hopes of him directing another film. His longtime wish—to die on the set while shooting a movie—was never to be fulfilled.

After his accident, Kurosawa's health began to deteriorate. While his mind remained sharp and lively, his body was giving up, and for the last half-year of his life, the director was largely confined to bed, listening to music and watching television at home. On September 6, 1998, Kurosawa died of a stroke in Setagaya, Tokyo, at the age of 88. At the time of his death, Kurosawa had two children, his son Hisao Kurosawa who married Hiroko Hayashi and his daughter Kazuko Kurosawa who married Harayuki Kato, along with several grandchildren. One of his grandchildren, the actor Takayuki Kato and , became a supporting actor in two films posthumously developed from screenplays written by Kurosawa which remained unproduced during his own lifetime, Takashi Koizumi's "After the Rain" (1999) and Kei Kumai's "The Sea is Watching" (2002).

From the beginning, Kurosawa displayed a bold, dynamic style, strongly influenced by Western cinema yet quite distinct from it. Kurosawa was extensively involved with every aspect of film production. He was also a gifted screenwriter, and would usually work in close collaboration with his co-writers from the beginning of the development of a film to ensure a high-quality script, which he insisted was the absolute foundation of a good film. He frequently served as editor of his own films and was regarded by his production team as "the greatest editor in the world". Though it was common in the Japanese film industry of that time for established directors to assemble around them a team, or "-gumi", with people drawn from the same pool of creative technicians, crew members and actors working from film to film (for example, the director Hiroshi Inagaki, who worked at Toho during the same period as Kurosawa, had such a team), Kurosawa's team, known as the "Kurosawa-gumi" (Kurosawa group)—including, for example, the cinematographer Asakazu Nakai, the production assistant Teruyo Nogami and the actor Takashi Shimura—was remarkable for its loyalty to the director and the consistent quality of its work.

Kurosawa's style is marked by a number of devices and techniques which Kurosawa introduced in his films over the decades. In his films of the 1940s and 1950s, Kurosawa frequently employs the "axial cut", in which the camera moves closer to, or further away from, the subject, not through the use of tracking shots or dissolves, but through a series of matched jump cuts. Another stylistic trait which scholars have pointed out is Kurosawa's tendency to "cut on motion": that is, to edit a sequence of a character or characters in motion so that an action is depicted in two or more separate shots, rather than one uninterrupted shot.

A form of cinematic punctuation very strongly identified with Kurosawa is the wipe. This is an effect created through an optical printer, in which, when a scene ends, a line or bar appears to move across the screen, "wiping" away the image while simultaneously revealing the first image of the subsequent scene. As a transitional device, it is used as a substitute for the straight cut or the dissolve (though Kurosawa often used both of those devices as well). In his mature work, Kurosawa employed the wipe so frequently that it became a kind of signature. For example, one blogger has counted no fewer than 12 instances of the wipe in "Drunken Angel". Kurosawa by all accounts always gave great attention to the soundtracks of his films, especially with an emphasis on sound-image counterpoint, in which the music or sound effects would ironically comment upon the image rather than merely reinforcing it. (Teruyo Nogami's memoir gives several such examples from "Drunken Angel" and "Stray Dog".) He was also involved with several of Japan's outstanding contemporary composers, including Fumio Hayasaka (who died in 1955) and the internationally famous Tōru Takemitsu.

Kurosawa employed a number of recurring major themes in his films. These include: (a) the master-disciple relationship between a usually older mentor and one or more novices, which often involves spiritual as well as technical mastery and self-mastery; (b) the heroic champion, the exceptional individual who emerges from the mass of people to produce something or right some injustice; (c) the depiction of extremes of weather as both dramatic devices and symbols of human passion; and (d) the recurrence of cycles of inexorable savage violence within history. According to Stephen Prince, the latter theme began with "Throne of Blood" (1957), and recurred in Kurosawa films of the 1980s. Mr. Prince calls this theme "the countertradition to the committed, heroic mode of Kurosawa's cinema".

Since the early to mid-1950s, a number of critics from the French New Wave championed the films of the older Japanese master, Kenji Mizoguchi, at the expense of Kurosawa's work. New Wave critic-filmmaker Jacques Rivette, said: "You can compare only what is comparable and that which aims high enough ... [Mizoguchi] seems to be the only Japanese director who is completely Japanese and yet is also the only one that achieves a true universality, that of an individual." According to such French commentators, Mizoguchi seemed, of the two artists, the more authentically Japanese. But at least one film scholar has questioned the validity of this dichotomy between "Japanese" Mizoguchi and "Western" Kurosawa by pointing out that "Mizo" had been at least as much influenced by Western cinema and Western culture in general as Kurosawa had ever been, and that this awareness of foreign trends is reflected in his work.

In Japan, there have been critics and other filmmakers who have accused Kurosawa's work of elitism, because of his focus on exceptional, heroic individuals and groups of men. In her DVD commentary on "Seven Samurai", Joan Mellen maintains that certain shots of the samurai characters Kambei and Kyuzo, which as far as she is concerned reveal Kurosawa "privileging" these samurai, "support the argument voiced by several Japanese critics that Kurosawa was an elitist ... Kurosawa was hardly a progressive director, they argued, since his peasants could not discover among their own ranks leaders who might rescue the village ... Kurosawa defended himself against this charge in his interview with me. 'I wanted to say that after everything the peasants were the stronger, closely clinging to the earth ... It was the samurai who were weak because they were being blown by the winds of time.

Owing to Kurosawa's popularity with European and American audiences from the early 1950s onward, he did not escape the charge of deliberately catering to the tastes of Westerners to achieve or maintain that popularity. Joan Mellen, recording the violently negative reaction (in the 1970s) of the left-wing director Nagisa Oshima to Kurosawa and his work, states: "That Kurosawa had brought Japanese film to a Western audience meant [to Oshima] that he must be pandering to Western values and politics." Kurosawa always strongly denied pandering to Western tastes: "He has never catered to a foreign audience" writes Audie Bock, "and has condemned those who do".

Many celebrated directors have been influenced by Kurosawa and have expressed admiration for his work. The filmmakers cited below can be presented according to four categories: (a) those who, like Kurosawa himself, established international critical reputations in the 1950s and early 1960s; (b) the so-called "New Hollywood" directors, that is, American moviemakers who, for the most part, established their reputations in the early to mid-1970s; (c) other Asian directors; and (d) modern-day directors.
The Swedish director Ingmar Bergman called his own film "The Virgin Spring" "touristic, a lousy imitation of Kurosawa", and added, "At that time my admiration for the Japanese cinema was at its height. I was almost a samurai myself!" In Italy, Federico Fellini in an interview declared the director "the greatest living example of all that an author of the cinema should be"—despite admitting to having seen only one of his films, "Seven Samurai". In France, Roman Polanski in 1965 cited Kurosawa as one of his three favorite filmmakers (with Fellini and Orson Welles), singling out "Seven Samurai", "Throne of Blood" and "The Hidden Fortress" for praise. The Italian director Bernardo Bertolucci considered the Japanese master's influence to be seminal: "Kurosawa's movies and "La Dolce Vita" of Fellini are the things that pushed me, sucked me into being a film director." German New Wave director Werner Herzog has cited Kurosawa as one of his greatest influences: "Of the filmmakers with whom I feel some kinship, Griffith ... Buñuel, Kurosawa and Eisenstein's "Ivan the Terrible", all come to mind." When asked to list his favorite directors, Russian director Andrei Tarkovsky cited Kurosawa as one of his favorites and named "Seven Samurai" as one of his ten favorite films.

According to his personal assistant Anthony Frewin, Stanley Kubrick "thought Kurosawa was one of the great film directors and followed him closely. In fact I cannot think of any other director he spoke so consistently and admiringly about. So, if Kubrick was cast away on a desert island and could only take a few films, what would they be? My money would be on "The Battle of Algiers", "Danton", "Rashomon", "Seven Samurai" and "Throne of Blood"."

Kurosawa's New Hollywood admirers have included Robert Altman, Francis Ford Coppola, Steven Spielberg, Martin Scorsese, George Lucas, and John Milius. In his early years while still a television director, Robert Altman stated that when he first saw "Rashomon" he was so impressed by its cinematographer's achievement of shooting several shots with the camera aimed directly at the sun—allegedly it was the first film in which this was done successfully—that he claims he was inspired the very next day to begin incorporating shots of the sun into his television work. It was Coppola who said of Kurosawa, "One thing that distinguishes [him] is that he didn't make one masterpiece or two masterpieces. He made, you know, "eight" masterpieces." Both Spielberg and Scorsese have praised the older man's role as teacher and role model, and Scorsese called him his "sensei", using the Japanese term. Spielberg has declared, "I have learned more from him than from almost any other filmmaker on the face of the earth", while Scorsese remarked, "Let me say it simply: Akira Kurosawa was my master, and ... the master of so many other filmmakers over the years." Several of these moviemakers were also instrumental in helping Kurosawa obtain financing for his late films: Lucas and Coppola served as co-producers on "Kagemusha", while the Spielberg name, lent to the 1990 production, "Dreams", helped bring that picture to fruition.

As the first Asian filmmaker to achieve international prominence, Kurosawa has naturally served as an inspiration for other Asian "auteurs". Of "Rashomon", the most famous director of India, Satyajit Ray, said: "The effect of the film on me [upon first seeing it in Calcutta in 1952] was electric. I saw it three times on consecutive days, and wondered each time if there was another film anywhere which gave such sustained and dazzling proof of a director's command over every aspect of film making." Other Asian admirers include the Japanese actor and director Takeshi Kitano, Hong Kong filmmaker John Woo, Japanese anime director Hayao Miyazaki and mainland Chinese director Zhang Yimou, who called Kurosawa "the quintessential Asian director".

Even today, Kurosawa continues to inspire and influence contemporary filmmakers. Alexander Payne spent the early part of his career watching Kurosawa's films, most notably "Seven Samurai" and "Ikiru". Guillermo del Toro referred to Kurosawa "one of the essential masters", citing "Throne of Blood", "High and Low" and "Ran" as among his favorite films. Kathryn Bigelow praised Kurosawa as one of "high-impact filmmakers" who can create emotionally invested characters. J.J. Abrams says he drew from Kurosawa while making . At the age of 19, Alejandro González Iñárritu remembers being spellbound when he first saw "Ikiru" and praises Kurosawa as "one of the first storytelling geniuses who began to change the narrative structure of films". When Spike Lee posted a list of 87 films every aspiring director should see, he included three Kurosawa movies: "Rashomon", "Yojimbo" and "Ran".

Following Kurosawa's death, several posthumous works based on his unfilmed screenplays have been produced. "After the Rain", directed by Takashi Koizumi, was released in 1999, and "The Sea Is Watching", directed by Kei Kumai, premiered in 2002. A script created by the Yonki no Kai ("Club of the Four Knights") (Kurosawa, Keisuke Kinoshita, Masaki Kobayashi, and Kon Ichikawa), around the time that "Dodeskaden" was made, finally was filmed and released (in 2000) as "Dora-heita", by the only surviving founding member of the club, Kon Ichikawa. Huayi Brothers Media and CKF Pictures in China announced in 2017 plans to produce a film of Kurosawa's posthumous screenplay of "The Masque of the Black Death" by Edgar Allan Poe for 2020. Patrick Frater writing for "Variety" magazine in May 2017 stated that another two unfinished films by Kurosawa were planned, with "Silvering Spear" to start filming in 2018.

In September 2011, it was reported that remake rights to most of Kurosawa's movies and unproduced screenplays were assigned by the Akira Kurosawa 100 Project to the L.A.-based company Splendent. Splendent's chief Sakiko Yamada, stated that he aimed to "help contemporary film-makers introduce a new generation of moviegoers to these unforgettable stories".

Kurosawa Production Co., established in 1959, continues to oversee many of the aspects of Kurosawa's legacy. The director's son, Hisao Kurosawa, is the current head of the company. Its American subsidiary, Kurosawa Enterprises, is located in Los Angeles. Rights to Kurosawa's works were then held by Kurosawa Production and the film studios under which he worked, most notably Toho. These rights were then assigned to the Akira Kurosawa 100 Project before being reassigned in 2011 to the L.A. based company Splendent. Kurosawa Production works closely with the Akira Kurosawa Foundation, established in December 2003 and also run by Hisao Kurosawa. The foundation organizes an annual short film competition and spearheads Kurosawa-related projects, including a recently shelved one to build a memorial museum for the director.

In 1981, the Kurosawa Film Studio was opened in Yokohama; two additional locations have since been launched in Japan. A large collection of archive material, including scanned screenplays, photos and news articles, has been made available through the Akira Kurosawa Digital Archive, a Japanese proprietary website maintained by Ryukoku University Digital Archives Research Center in collaboration with Kurosawa Production. Anaheim University's Akira Kurosawa School of Film was launched in spring 2009 with the backing of Kurosawa Production. It offers online programs in digital film making, with headquarters in Anaheim and a learning center in Tokyo.

Two film awards have also been named in Kurosawa's honor. The Akira Kurosawa Award for Lifetime Achievement in Film Directing is awarded during the San Francisco International Film Festival, while the Akira Kurosawa Award is given during the Tokyo International Film Festival. In 1999 he was named "Asian of the Century" in the "Arts, Literature, and Culture" category by "AsianWeek" magazine and CNN, cited as "one of the [five] people who contributed most to the betterment of Asia in the past 100 years". In commemoration of the 100th anniversary of Kurosawa's birth in 2010, a project called AK100 was launched in 2008. The AK100 Project aims to "expose young people who are the representatives of the next generation, and all people everywhere, to the light and spirit of Akira Kurosawa and the wonderful world he created".

Anaheim University in cooperation with the Kurosawa Family established the Anaheim University Akira Kurosawa School of Film to offer online and blended learning programs on Akira Kurosawa and filmmaking. The animated Wes Anderson film, "Isle of Dogs," is partially inspired by Kurosawa's filming techniques. At the 64th Sydney Film Festival, there was a retrospective of Akira Kurosawa where films of his were screened to remember the great legacy he has created from his work.

A significant number of full-length and short documentaries concerning the life and films of Kurosawa were made during his lifetime and after his death. "AK" was filmed in 1985 and is a French documentary film directed by Chris Marker. Though it was filmed while Kurosawa was working on "Ran", the film focuses more on Kurosawa's remote but polite personality than on the making of the film. The documentary is sometimes seen as being reflective of Marker's fascination with Japanese culture, which he also drew on for one of his best-known films, "Sans Soleil". The film was screened in the Un Certain Regard section at the 1985 Cannes Film Festival. Other documentaries concerning Kurosawa's life and works produced posthumously include:





</doc>
<doc id="874" url="https://en.wikipedia.org/wiki?curid=874" title="Ancient Egypt">
Ancient Egypt

Ancient Egypt was a civilization of ancient North Africa, concentrated along the lower reaches of the Nile River, situated in the place that is now the country Egypt. Ancient Egyptian civilization followed prehistoric Egypt and coalesced around 3100 BC (according to conventional Egyptian chronology) with the political unification of Upper and Lower Egypt under Menes (often identified with Narmer). The history of ancient Egypt occurred as a series of stable kingdoms, separated by periods of relative instability known as Intermediate Periods: the Old Kingdom of the Early Bronze Age, the Middle Kingdom of the Middle Bronze Age and the New Kingdom of the Late Bronze Age.

Egypt reached the pinnacle of its power in the New Kingdom, ruling much of Nubia and a sizable portion of the Near East, after which it entered a period of slow decline. During the course of its history Egypt was invaded or conquered by a number of foreign powers, including the Hyksos, the Libyans, the Nubians, the Assyrians, the Achaemenid Persians, and the Macedonians under the command of Alexander the Great. The Greek Ptolemaic Kingdom, formed in the aftermath of Alexander's death, ruled Egypt until 30 BC, when, under Cleopatra, it fell to the Roman Empire and became a Roman province.

The success of ancient Egyptian civilization came partly from its ability to adapt to the conditions of the Nile River valley for agriculture. The predictable flooding and controlled irrigation of the fertile valley produced surplus crops, which supported a more dense population, and social development and culture. With resources to spare, the administration sponsored mineral exploitation of the valley and surrounding desert regions, the early development of an independent writing system, the organization of collective construction and agricultural projects, trade with surrounding regions, and a military intended to assert Egyptian dominance. Motivating and organizing these activities was a bureaucracy of elite scribes, religious leaders, and administrators under the control of a pharaoh, who ensured the cooperation and unity of the Egyptian people in the context of an elaborate system of religious beliefs.

The many achievements of the ancient Egyptians include the quarrying, surveying and construction techniques that supported the building of monumental pyramids, temples, and obelisks; a system of mathematics, a practical and effective system of medicine, irrigation systems and agricultural production techniques, the first known planked boats, Egyptian faience and glass technology, new forms of literature, and the earliest known peace treaty, made with the Hittites. Ancient Egypt has left a lasting legacy. Its art and architecture were widely copied, and its antiquities carried off to far corners of the world. Its monumental ruins have inspired the imaginations of travelers and writers for centuries. A new-found respect for antiquities and excavations in the early modern period by Europeans and Egyptians led to the scientific investigation of Egyptian civilization and a greater appreciation of its cultural legacy.

The Nile has been the lifeline of its region for much of human history. The fertile floodplain of the Nile gave humans the opportunity to develop a settled agricultural economy and a more sophisticated, centralized society that became a cornerstone in the history of human civilization. Nomadic modern human hunter-gatherers began living in the Nile valley through the end of the Middle Pleistocene some 120,000 years ago. By the late Paleolithic period, the arid climate of Northern Africa became increasingly hot and dry, forcing the populations of the area to concentrate along the river region.
In Predynastic and Early Dynastic times, the Egyptian climate was much less arid than it is today. Large regions of Egypt were covered in treed savanna and traversed by herds of grazing ungulates. Foliage and fauna were far more prolific in all environs and the Nile region supported large populations of waterfowl. Hunting would have been common for Egyptians, and this is also the period when many animals were first domesticated.

By about 5500 BC, small tribes living in the Nile valley had developed into a series of cultures demonstrating firm control of agriculture and animal husbandry, and identifiable by their pottery and personal items, such as combs, bracelets, and beads. The largest of these early cultures in upper (Southern) Egypt was the Badarian culture, which probably originated in the Western Desert; it was known for its high quality ceramics, stone tools, and its use of copper.
The Badari was followed by the Naqada culture: the Amratian (Naqada I), the Gerzeh (Naqada II), and Semainean (Naqada III). These brought a number of technological improvements. As early as the Naqada I Period, predynastic Egyptians imported obsidian from Ethiopia, used to shape blades and other objects from flakes. In Naqada II times, early evidence exists of contact with the Near East, particularly Canaan and the Byblos coast. Over a period of about 1,000 years, the Naqada culture developed from a few small farming communities into a powerful civilization whose leaders were in complete control of the people and resources of the Nile valley. Establishing a power center at Nekhen (in Greek, Hierakonpolis), and later at Abydos, Naqada III leaders expanded their control of Egypt northwards along the Nile. They also traded with Nubia to the south, the oases of the western desert to the west, and the cultures of the eastern Mediterranean and Near East to the east, initiating a period of Egypt-Mesopotamia relations.

The Naqada culture manufactured a diverse selection of material goods, reflective of the increasing power and wealth of the elite, as well as societal personal-use items, which included combs, small statuary, painted pottery, high quality decorative stone vases, cosmetic palettes, and jewelry made of gold, lapis, and ivory. They also developed a ceramic glaze known as faience, which was used well into the Roman Period to decorate cups, amulets, and figurines. During the last predynastic phase, the Naqada culture began using written symbols that eventually were developed into a full system of hieroglyphs for writing the ancient Egyptian language.

 The Early Dynastic Period was approximately contemporary to the early Sumerian-Akkadian civilisation of Mesopotamia and of ancient Elam. The third-century BC Egyptian priest Manetho grouped the long line of kings from Menes to his own time into 30 dynasties, a system still used today. He began his official history with the king named "Meni" (or "Menes" in Greek) who was believed to have united the two kingdoms of Upper and Lower Egypt.

The transition to a unified state happened more gradually than ancient Egyptian writers represented, and there is no contemporary record of Menes. Some scholars now believe, however, that the mythical Menes may have been the king Narmer, who is depicted wearing royal regalia on the ceremonial "Narmer Palette," in a symbolic act of unification. In the Early Dynastic Period, which began about 3000 BC, the first of the Dynastic kings solidified control over lower Egypt by establishing a capital at Memphis, from which he could control the labour force and agriculture of the fertile delta region, as well as the lucrative and critical trade routes to the Levant. The increasing power and wealth of the kings during the early dynastic period was reflected in their elaborate mastaba tombs and mortuary cult structures at Abydos, which were used to celebrate the deified king after his death. The strong institution of kingship developed by the kings served to legitimize state control over the land, labour, and resources that were essential to the survival and growth of ancient Egyptian civilization.

Major advances in architecture, art, and technology were made during the Old Kingdom, fueled by the increased agricultural productivity and resulting population, made possible by a well-developed central administration. Some of ancient Egypt's crowning achievements, the Giza pyramids and Great Sphinx, were constructed during the Old Kingdom. Under the direction of the vizier, state officials collected taxes, coordinated irrigation projects to improve crop yield, drafted peasants to work on construction projects, and established a justice system to maintain peace and order. 

With the rising importance of central administration in Egypt a new class of educated scribes and officials arose who were granted estates by the king in payment for their services. Kings also made land grants to their mortuary cults and local temples, to ensure that these institutions had the resources to worship the king after his death. Scholars believe that five centuries of these practices slowly eroded the economic vitality of Egypt, and that the economy could no longer afford to support a large centralized administration. As the power of the kings diminished, regional governors called nomarchs began to challenge the supremacy of the office of king. This, coupled with severe droughts between 2200 and 2150 BC, is believed to have caused the country to enter the 140-year period of famine and strife known as the First Intermediate Period.

After Egypt's central government collapsed at the end of the Old Kingdom, the administration could no longer support or stabilize the country's economy. Regional governors could not rely on the king for help in times of crisis, and the ensuing food shortages and political disputes escalated into famines and small-scale civil wars. Yet despite difficult problems, local leaders, owing no tribute to the king, used their new-found independence to establish a thriving culture in the provinces. Once in control of their own resources, the provinces became economically richer—which was demonstrated by larger and better burials among all social classes. In bursts of creativity, provincial artisans adopted and adapted cultural motifs formerly restricted to the royalty of the Old Kingdom, and scribes developed literary styles that expressed the optimism and originality of the period.

Free from their loyalties to the king, local rulers began competing with each other for territorial control and political power. By 2160 BC, rulers in Herakleopolis controlled Lower Egypt in the north, while a rival clan based in Thebes, the Intef family, took control of Upper Egypt in the south. As the Intefs grew in power and expanded their control northward, a clash between the two rival dynasties became inevitable. Around 2055 BC the northern Theban forces under Nebhepetre Mentuhotep II finally defeated the Herakleopolitan rulers, reuniting the Two Lands. They inaugurated a period of economic and cultural renaissance known as the Middle Kingdom.

The kings of the Middle Kingdom restored the country's stability and prosperity, thereby stimulating a resurgence of art, literature, and monumental building projects. Mentuhotep II and his Eleventh Dynasty successors ruled from Thebes, but the vizier Amenemhat I, upon assuming the kingship at the beginning of the Twelfth Dynasty around 1985 BC, shifted the kingdom's capital to the city of Itjtawy, located in Faiyum. From Itjtawy, the kings of the Twelfth Dynasty undertook a far-sighted land reclamation and irrigation scheme to increase agricultural output in the region. Moreover, the military reconquered territory in Nubia that was rich in quarries and gold mines, while laborers built a defensive structure in the Eastern Delta, called the "Walls-of-the-Ruler", to defend against foreign attack.

With the kings having secured the country militarily and politically and with vast agricultural and mineral wealth at their disposal, the nation's population, arts, and religion flourished. In contrast to elitist Old Kingdom attitudes towards the gods, the Middle Kingdom displayed an increase in expressions of personal piety. Middle Kingdom literature featured sophisticated themes and characters written in a confident, eloquent style. The relief and portrait sculpture of the period captured subtle, individual details that reached new heights of technical sophistication.

The last great ruler of the Middle Kingdom, Amenemhat III, allowed Semitic-speaking Canaanite settlers from the Near East into the Delta region to provide a sufficient labour force for his especially active mining and building campaigns. These ambitious building and mining activities, however, combined with severe Nile floods later in his reign, strained the economy and precipitated the slow decline into the Second Intermediate Period during the later Thirteenth and Fourteenth dynasties. During this decline, the Canaanite settlers began to assume greater control of the Delta region, eventually coming to power in Egypt as the Hyksos.

Around 1785 BC, as the power of the Middle Kingdom kings weakened, a Western Asian people called the Hyksos, who had already settled in the Delta, seized control of Egypt and established their capital at Avaris, forcing the former central government to retreat to Thebes. The king was treated as a vassal and expected to pay tribute. The Hyksos ("foreign rulers") retained Egyptian models of government and identified as kings, thereby integrating Egyptian elements into their culture. They and other invaders introduced new tools of warfare into Egypt, most notably the composite bow and the horse-drawn chariot.

After retreating south, the native Theban kings found themselves trapped between the Canaanite Hyksos ruling the north and the Hyksos' Nubian allies, the Kushites, to the south. After years of vassalage, Thebes gathered enough strength to challenge the Hyksos in a conflict that lasted more than 30 years, until 1555 BC. The kings Seqenenre Tao II and Kamose were ultimately able to defeat the Nubians to the south of Egypt, but failed to defeat the Hyksos. That task fell to Kamose's successor, Ahmose I, who successfully waged a series of campaigns that permanently eradicated the Hyksos' presence in Egypt. He established a new dynasty and, in the New Kingdom that followed, the military became a central priority for the kings, who sought to expand Egypt's borders and attempted to gain mastery of the Near East.

The New Kingdom pharaohs established a period of unprecedented prosperity by securing their borders and strengthening diplomatic ties with their neighbours, including the Mitanni Empire, Assyria, and Canaan. Military campaigns waged under Tuthmosis I and his grandson Tuthmosis III extended the influence of the pharaohs to the largest empire Egypt had ever seen. Beginning with Merneptah the rulers of Egypt adopted the title of pharaoh.
Between their reigns, Hatshepsut, a queen who established herself as pharaoh, launched many building projects, including restoration of temples damaged by the Hyksos, and sent trading expeditions to Punt and the Sinai. When Tuthmosis III died in 1425 BC, Egypt had an empire extending from Niya in north west Syria to the Fourth Cataract of the Nile in Nubia, cementing loyalties and opening access to critical imports such as bronze and wood.

The New Kingdom pharaohs began a large-scale building campaign to promote the god Amun, whose growing cult was based in Karnak. They also constructed monuments to glorify their own achievements, both real and imagined. The Karnak temple is the largest Egyptian temple ever built.

Around 1350 BC, the stability of the New Kingdom was threatened when Amenhotep IV ascended the throne and instituted a series of radical and chaotic reforms. Changing his name to Akhenaten, he touted the previously obscure sun deity Aten as the supreme deity, suppressed the worship of most other deities, and moved the capital to the new city of Akhetaten (modern-day Amarna). He was devoted to his new religion and artistic style. After his death, the cult of the Aten was quickly abandoned and the traditional religious order restored. The subsequent pharaohs, Tutankhamun, Ay, and Horemheb, worked to erase all mention of Akhenaten's heresy, now known as the Amarna Period.
Around 1279 BC, Ramesses II, also known as Ramesses the Great, ascended the throne, and went on to build more temples, erect more statues and obelisks, and sire more children than any other pharaoh in history. A bold military leader, Ramesses II led his army against the Hittites in the Battle of Kadesh (in modern Syria) and, after fighting to a stalemate, finally agreed to the first recorded peace treaty, around 1258 BC.

Egypt's wealth, however, made it a tempting target for invasion, particularly by the Libyan Berbers to the west, and the Sea Peoples, a conjectured confederation of seafarers from the Aegean Sea. Initially, the military was able to repel these invasions, but Egypt eventually lost control of its remaining territories in southern Canaan, much of it falling to the Assyrians. The effects of external threats were exacerbated by internal problems such as corruption, tomb robbery, and civil unrest. After regaining their power, the high priests at the temple of Amun in Thebes accumulated vast tracts of land and wealth, and their expanded power splintered the country during the Third Intermediate Period.

Following the death of Ramesses XI in 1078 BC, Smendes assumed authority over the northern part of Egypt, ruling from the city of Tanis. The south was effectively controlled by the High Priests of Amun at Thebes, who recognized Smendes in name only. During this time, Libyans had been settling in the western delta, and chieftains of these settlers began increasing their autonomy. Libyan princes took control of the delta under Shoshenq I in 945 BC, founding the so-called Libyan or Bubastite dynasty that would rule for some 200 years. Shoshenq also gained control of southern Egypt by placing his family members in important priestly positions. Libyan control began to erode as a rival dynasty in the delta arose in Leontopolis, and Kushites threatened from the south. Around 727 BC the Kushite king Piye invaded northward, seizing control of Thebes and eventually the Delta.

Egypt's far-reaching prestige declined considerably toward the end of the Third Intermediate Period. Its foreign allies had fallen under the Assyrian sphere of influence, and by 700 BC war between the two states became inevitable. Between 671 and 667 BC the Assyrians began their attack on Egypt. The reigns of both Taharqa and his successor, Tanutamun, were filled with constant conflict with the Assyrians, against whom Egypt enjoyed several victories. Ultimately, the Assyrians pushed the Kushites back into Nubia, occupied Memphis, and sacked the temples of Thebes.

The Assyrians left control of Egypt to a series of vassals who became known as the Saite kings of the Twenty-Sixth Dynasty. By 653 BC, the Saite king Psamtik I was able to oust the Assyrians with the help of Greek mercenaries, who were recruited to form Egypt's first navy. Greek influence expanded greatly as the city-state of Naukratis became the home of Greeks in the Nile Delta. The Saite kings based in the new capital of Sais witnessed a brief but spirited resurgence in the economy and culture, but in 525 BC, the powerful Persians, led by Cambyses II, began their conquest of Egypt, eventually capturing the pharaoh Psamtik III at the battle of Pelusium. Cambyses II then assumed the formal title of pharaoh, but ruled Egypt from Iran, leaving Egypt under the control of a satrapy. A few successful revolts against the Persians marked the 5th century BC, but Egypt was never able to permanently overthrow the Persians.

Following its annexation by Persia, Egypt was joined with Cyprus and Phoenicia in the sixth satrapy of the Achaemenid Persian Empire. This first period of Persian rule over Egypt, also known as the Twenty-Seventh dynasty, ended in 402 BC, when Egypt regained independence under a series of native dynasties. The last of these dynasties, the Thirtieth, proved to be the last native royal house of ancient Egypt, ending with the kingship of Nectanebo II. A brief restoration of Persian rule, sometimes known as the Thirty-First Dynasty, began in 343 BC, but shortly after, in 332 BC, the Persian ruler Mazaces handed Egypt over to Alexander the Great without a fight.

In 332 BC, Alexander the Great conquered Egypt with little resistance from the Persians and was welcomed by the Egyptians as a deliverer. The administration established by Alexander's successors, the Macedonian Ptolemaic Kingdom, was based on an Egyptian model and based in the new capital city of Alexandria. The city showcased the power and prestige of Hellenistic rule, and became a seat of learning and culture, centered at the famous Library of Alexandria. The Lighthouse of Alexandria lit the way for the many ships that kept trade flowing through the city—as the Ptolemies made commerce and revenue-generating enterprises, such as papyrus manufacturing, their top priority.

Hellenistic culture did not supplant native Egyptian culture, as the Ptolemies supported time-honored traditions in an effort to secure the loyalty of the populace. They built new temples in Egyptian style, supported traditional cults, and portrayed themselves as pharaohs. Some traditions merged, as Greek and Egyptian gods were syncretized into composite deities, such as Serapis, and classical Greek forms of sculpture influenced traditional Egyptian motifs. Despite their efforts to appease the Egyptians, the Ptolemies were challenged by native rebellion, bitter family rivalries, and the powerful mob of Alexandria that formed after the death of Ptolemy IV. In addition, as Rome relied more heavily on imports of grain from Egypt, the Romans took great interest in the political situation in the country. Continued Egyptian revolts, ambitious politicians, and powerful opponents from the Near East made this situation unstable, leading Rome to send forces to secure the country as a province of its empire.

Egypt became a province of the Roman Empire in 30 BC, following the defeat of Marc Antony and Ptolemaic Queen Cleopatra VII by Octavian (later Emperor Augustus) in the Battle of Actium. The Romans relied heavily on grain shipments from Egypt, and the Roman army, under the control of a prefect appointed by the Emperor, quelled rebellions, strictly enforced the collection of heavy taxes, and prevented attacks by bandits, which had become a notorious problem during the period. Alexandria became an increasingly important center on the trade route with the orient, as exotic luxuries were in high demand in Rome.

Although the Romans had a more hostile attitude than the Greeks towards the Egyptians, some traditions such as mummification and worship of the traditional gods continued. The art of mummy portraiture flourished, and some Roman emperors had themselves depicted as pharaohs, though not to the extent that the Ptolemies had. The former lived outside Egypt and did not perform the ceremonial functions of Egyptian kingship. Local administration became Roman in style and closed to native Egyptians.

From the mid-first century AD, Christianity took root in Egypt and it was originally seen as another cult that could be accepted. However, it was an uncompromising religion that sought to win converts from Egyptian Religion and Greco-Roman religion and threatened popular religious traditions. This led to the persecution of converts to Christianity, culminating in the great purges of Diocletian starting in 303, but eventually Christianity won out. In 391 the Christian Emperor Theodosius introduced legislation that banned pagan rites and closed temples. Alexandria became the scene of great anti-pagan riots with public and private religious imagery destroyed. As a consequence, Egypt's native religious culture was continually in decline. While the native population certainly continued to speak their language, the ability to read hieroglyphic writing slowly disappeared as the role of the Egyptian temple priests and priestesses diminished. The temples themselves were sometimes converted to churches or abandoned to the desert.

In the fourth century, as the Roman Empire divided, Egypt found itself in the Eastern Empire with its capital at Constantinople. In the waning years of the Empire, Egypt fell to the Sasanian Persian army (618–628 AD), was recaptured by the Roman Emperor Heraclius (629–639 AD), and then was finally captured by Muslim Rashidun army in 639–641 AD, ending Roman rule.

The pharaoh was the absolute monarch of the country and, at least in theory, wielded complete control of the land and its resources. The king was the supreme military commander and head of the government, who relied on a bureaucracy of officials to manage his affairs. In charge of the administration was his second in command, the vizier, who acted as the king's representative and coordinated land surveys, the treasury, building projects, the legal system, and the archives. At a regional level, the country was divided into as many as 42 administrative regions called nomes each governed by a nomarch, who was accountable to the vizier for his jurisdiction. The temples formed the backbone of the economy. Not only were they houses of worship, but were also responsible for collecting and storing the kingdom's wealth in a system of granaries and treasuries administered by overseers, who redistributed grain and goods.

Much of the economy was centrally organized and strictly controlled. Although the ancient Egyptians did not use coinage until the Late period, they did use a type of money-barter system, with standard sacks of grain and the "deben", a weight of roughly of copper or silver, forming a common denominator. Workers were paid in grain; a simple laborer might earn 5 sacks (200 kg or 400 lb) of grain per month, while a foreman might earn 7 sacks (250 kg or 550 lb). Prices were fixed across the country and recorded in lists to facilitate trading; for example a shirt cost five copper deben, while a cow cost 140 deben. Grain could be traded for other goods, according to the fixed price list. During the fifth century BC coined money was introduced into Egypt from abroad. At first the coins were used as standardized pieces of precious metal rather than true money, but in the following centuries international traders came to rely on coinage.

Egyptian society was highly stratified, and social status was expressly displayed. Farmers made up the bulk of the population, but agricultural produce was owned directly by the state, temple, or noble family that owned the land. Farmers were also subject to a labor tax and were required to work on irrigation or construction projects in a corvée system. Artists and craftsmen were of higher status than farmers, but they were also under state control, working in the shops attached to the temples and paid directly from the state treasury. Scribes and officials formed the upper class in ancient Egypt, known as the "white kilt class" in reference to the bleached linen garments that served as a mark of their rank. The upper class prominently displayed their social status in art and literature. Below the nobility were the priests, physicians, and engineers with specialized training in their field. It is unclear whether slavery as understood today existed in ancient Egypt, there is difference of opinions among authors.

The ancient Egyptians viewed men and women, including people from all social classes, as essentially equal under the law, and even the lowliest peasant was entitled to petition the vizier and his court for redress. Although slaves were mostly used as indentured servants, they were able to buy and sell their servitude, work their way to freedom or nobility, and were usually treated by doctors in the workplace. Both men and women had the right to own and sell property, make contracts, marry and divorce, receive inheritance, and pursue legal disputes in court. Married couples could own property jointly and protect themselves from divorce by agreeing to marriage contracts, which stipulated the financial obligations of the husband to his wife and children should the marriage end. Compared with their counterparts in ancient Greece, Rome, and even more modern places around the world, ancient Egyptian women had a greater range of personal choices and opportunities for achievement. Women such as Hatshepsut and Cleopatra VII even became pharaohs, while others wielded power as Divine Wives of Amun. Despite these freedoms, ancient Egyptian women did not often take part in official roles in the administration, served only secondary roles in the temples, and were not as likely to be as educated as men. 

The head of the legal system was officially the pharaoh, who was responsible for enacting laws, delivering justice, and maintaining law and order, a concept the ancient Egyptians referred to as Ma'at. Although no legal codes from ancient Egypt survive, court documents show that Egyptian law was based on a common-sense view of right and wrong that emphasized reaching agreements and resolving conflicts rather than strictly adhering to a complicated set of statutes. Local councils of elders, known as "Kenbet" in the New Kingdom, were responsible for ruling in court cases involving small claims and minor disputes. More serious cases involving murder, major land transactions, and tomb robbery were referred to the "Great Kenbet", over which the vizier or pharaoh presided. Plaintiffs and defendants were expected to represent themselves and were required to swear an oath that they had told the truth. In some cases, the state took on both the role of prosecutor and judge, and it could torture the accused with beatings to obtain a confession and the names of any co-conspirators. Whether the charges were trivial or serious, court scribes documented the complaint, testimony, and verdict of the case for future reference.

Punishment for minor crimes involved either imposition of fines, beatings, facial mutilation, or exile, depending on the severity of the offense. Serious crimes such as murder and tomb robbery were punished by execution, carried out by decapitation, drowning, or impaling the criminal on a stake. Punishment could also be extended to the criminal's family. Beginning in the New Kingdom, oracles played a major role in the legal system, dispensing justice in both civil and criminal cases. The procedure was to ask the god a "yes" or "no" question concerning the right or wrong of an issue. The god, carried by a number of priests, rendered judgment by choosing one or the other, moving forward or backward, or pointing to one of the answers written on a piece of papyrus or an ostracon.

A combination of favorable geographical features contributed to the success of ancient Egyptian culture, the most important of which was the rich fertile soil resulting from annual inundations of the Nile River. The ancient Egyptians were thus able to produce an abundance of food, allowing the population to devote more time and resources to cultural, technological, and artistic pursuits. Land management was crucial in ancient Egypt because taxes were assessed based on the amount of land a person owned.

Farming in Egypt was dependent on the cycle of the Nile River. The Egyptians recognized three seasons: "Akhet" (flooding), "Peret" (planting), and "Shemu" (harvesting). The flooding season lasted from June to September, depositing on the river's banks a layer of mineral-rich silt ideal for growing crops. After the floodwaters had receded, the growing season lasted from October to February. Farmers plowed and planted seeds in the fields, which were irrigated with ditches and canals. Egypt received little rainfall, so farmers relied on the Nile to water their crops. From March to May, farmers used sickles to harvest their crops, which were then threshed with a flail to separate the straw from the grain. Winnowing removed the chaff from the grain, and the grain was then ground into flour, brewed to make beer, or stored for later use.

The ancient Egyptians cultivated emmer and barley, and several other cereal grains, all of which were used to make the two main food staples of bread and beer. Flax plants, uprooted before they started flowering, were grown for the fibers of their stems. These fibers were split along their length and spun into thread, which was used to weave sheets of linen and to make clothing. Papyrus growing on the banks of the Nile River was used to make paper. Vegetables and fruits were grown in garden plots, close to habitations and on higher ground, and had to be watered by hand. Vegetables included leeks, garlic, melons, squashes, pulses, lettuce, and other crops, in addition to grapes that were made into wine.

The Egyptians believed that a balanced relationship between people and animals was an essential element of the cosmic order; thus humans, animals and plants were believed to be members of a single whole. Animals, both domesticated and wild, were therefore a critical source of spirituality, companionship, and sustenance to the ancient Egyptians. Cattle were the most important livestock; the administration collected taxes on livestock in regular censuses, and the size of a herd reflected the prestige and importance of the estate or temple that owned them. In addition to cattle, the ancient Egyptians kept sheep, goats, and pigs. Poultry, such as ducks, geese, and pigeons, were captured in nets and bred on farms, where they were force-fed with dough to fatten them. The Nile provided a plentiful source of fish. Bees were also domesticated from at least the Old Kingdom, and provided both honey and wax.

The ancient Egyptians used donkeys and oxen as beasts of burden, and they were responsible for plowing the fields and trampling seed into the soil. The slaughter of a fattened ox was also a central part of an offering ritual. Horses were introduced by the Hyksos in the Second Intermediate Period. Camels, although known from the New Kingdom, were not used as beasts of burden until the Late Period. There is also evidence to suggest that elephants were briefly utilized in the Late Period but largely abandoned due to lack of grazing land. Dogs, cats, and monkeys were common family pets, while more exotic pets imported from the heart of Africa, such as Sub-Saharan African lions, were reserved for royalty. Herodotus observed that the Egyptians were the only people to keep their animals with them in their houses. During the Late Period, the worship of the gods in their animal form was extremely popular, such as the cat goddess Bastet and the ibis god Thoth, and these animals were kept in large numbers for the purpose of ritual sacrifice.

Egypt is rich in building and decorative stone, copper and lead ores, gold, and semiprecious stones. These natural resources allowed the ancient Egyptians to build monuments, sculpt statues, make tools, and fashion jewelry. Embalmers used salts from the Wadi Natrun for mummification, which also provided the gypsum needed to make plaster. Ore-bearing rock formations were found in distant, inhospitable wadis in the eastern desert and the Sinai, requiring large, state-controlled expeditions to obtain natural resources found there. There were extensive gold mines in Nubia, and one of the first maps known is of a gold mine in this region. The Wadi Hammamat was a notable source of granite, greywacke, and gold. Flint was the first mineral collected and used to make tools, and flint handaxes are the earliest pieces of evidence of habitation in the Nile valley. Nodules of the mineral were carefully flaked to make blades and arrowheads of moderate hardness and durability even after copper was adopted for this purpose. Ancient Egyptians were among the first to use minerals such as sulfur as cosmetic substances.

The Egyptians worked deposits of the lead ore galena at Gebel Rosas to make net sinkers, plumb bobs, and small figurines. Copper was the most important metal for toolmaking in ancient Egypt and was smelted in furnaces from malachite ore mined in the Sinai. Workers collected gold by washing the nuggets out of sediment in alluvial deposits, or by the more labor-intensive process of grinding and washing gold-bearing quartzite. Iron deposits found in upper Egypt were utilized in the Late Period. High-quality building stones were abundant in Egypt; the ancient Egyptians quarried limestone all along the Nile valley, granite from Aswan, and basalt and sandstone from the wadis of the eastern desert. Deposits of decorative stones such as porphyry, greywacke, alabaster, and carnelian dotted the eastern desert and were collected even before the First Dynasty. In the Ptolemaic and Roman Periods, miners worked deposits of emeralds in Wadi Sikait and amethyst in Wadi el-Hudi.

The ancient Egyptians engaged in trade with their foreign neighbors to obtain rare, exotic goods not found in Egypt. In the Predynastic Period, they established trade with Nubia to obtain gold and incense. They also established trade with Palestine, as evidenced by Palestinian-style oil jugs found in the burials of the First Dynasty pharaohs. An Egyptian colony stationed in southern Canaan dates to slightly before the First Dynasty. Narmer had Egyptian pottery produced in Canaan and exported back to Egypt.

By the Second Dynasty at latest, ancient Egyptian trade with Byblos yielded a critical source of quality timber not found in Egypt. By the Fifth Dynasty, trade with Punt provided gold, aromatic resins, ebony, ivory, and wild animals such as monkeys and baboons. Egypt relied on trade with Anatolia for essential quantities of tin as well as supplementary supplies of copper, both metals being necessary for the manufacture of bronze. The ancient Egyptians prized the blue stone lapis lazuli, which had to be imported from far-away Afghanistan. Egypt's Mediterranean trade partners also included Greece and Crete, which provided, among other goods, supplies of olive oil. In exchange for its luxury imports and raw materials, Egypt mainly exported grain, gold, linen, and papyrus, in addition to other finished goods including glass and stone objects.

The Egyptian language is a northern Afro-Asiatic language closely related to the Berber and Semitic languages. It has the second longest known history of any language (after Sumerian), having been written from c. 3200 BC to the Middle Ages and remaining as a spoken language for longer. The phases of ancient Egyptian are Old Egyptian, Middle Egyptian (Classical Egyptian), Late Egyptian, Demotic and Coptic. Egyptian writings do not show dialect differences before Coptic, but it was probably spoken in regional dialects around Memphis and later Thebes.

Ancient Egyptian was a synthetic language, but it became more analytic later on. Late Egyptian developed prefixal definite and indefinite articles, which replaced the older inflectional suffixes. There was a change from the older verb–subject–object word order to subject–verb–object. The Egyptian hieroglyphic, hieratic, and demotic scripts were eventually replaced by the more phonetic Coptic alphabet. Coptic is still used in the liturgy of the Egyptian Orthodox Church, and traces of it are found in modern Egyptian Arabic.

Ancient Egyptian has 25 consonants similar to those of other Afro-Asiatic languages. These include pharyngeal and emphatic consonants, voiced and voiceless stops, voiceless fricatives and voiced and voiceless affricates. It has three long and three short vowels, which expanded in Late Egyptian to about nine. The basic word in Egyptian, similar to Semitic and Berber, is a triliteral or biliteral root of consonants and semiconsonants. Suffixes are added to form words. The verb conjugation corresponds to the person. For example, the triconsonantal skeleton is the semantic core of the word 'hear'; its basic conjugation is ', 'he hears'. If the subject is a noun, suffixes are not added to the verb: ', 'the woman hears'.

Adjectives are derived from nouns through a process that Egyptologists call "nisbation" because of its similarity with Arabic. The word order is in verbal and adjectival sentences, and in nominal and adverbial sentences. The subject can be moved to the beginning of sentences if it is long and is followed by a resumptive pronoun. Verbs and nouns are negated by the particle "n", but "nn" is used for adverbial and adjectival sentences. Stress falls on the ultimate or penultimate syllable, which can be open (CV) or closed (CVC).

Hieroglyphic writing dates from c. 3000 BC, and is composed of hundreds of symbols. A hieroglyph can represent a word, a sound, or a silent determinative; and the same symbol can serve different purposes in different contexts. Hieroglyphs were a formal script, used on stone monuments and in tombs, that could be as detailed as individual works of art. In day-to-day writing, scribes used a cursive form of writing, called hieratic, which was quicker and easier. While formal hieroglyphs may be read in rows or columns in either direction (though typically written from right to left), hieratic was always written from right to left, usually in horizontal rows. A new form of writing, Demotic, became the prevalent writing style, and it is this form of writing—along with formal hieroglyphs—that accompany the Greek text on the Rosetta Stone.

Around the first century AD, the Coptic alphabet started to be used alongside the Demotic script. Coptic is a modified Greek alphabet with the addition of some Demotic signs. Although formal hieroglyphs were used in a ceremonial role until the fourth century, towards the end only a small handful of priests could still read them. As the traditional religious establishments were disbanded, knowledge of hieroglyphic writing was mostly lost. Attempts to decipher them date to the Byzantine and Islamic periods in Egypt, but only in the 1820s, after the discovery of the Rosetta Stone and years of research by Thomas Young and Jean-François Champollion, were hieroglyphs substantially deciphered.

Writing first appeared in association with kingship on labels and tags for items found in royal tombs. It was primarily an occupation of the scribes, who worked out of the "Per Ankh" institution or the House of Life. The latter comprised offices, libraries (called House of Books), laboratories and observatories. Some of the best-known pieces of ancient Egyptian literature, such as the Pyramid and Coffin Texts, were written in Classical Egyptian, which continued to be the language of writing until about 1300 BC. Late Egyptian was spoken from the New Kingdom onward and is represented in Ramesside administrative documents, love poetry and tales, as well as in Demotic and Coptic texts. During this period, the tradition of writing had evolved into the tomb autobiography, such as those of Harkhuf and Weni. The genre known as "Sebayt" ("instructions") was developed to communicate teachings and guidance from famous nobles; the Ipuwer papyrus, a poem of lamentations describing natural disasters and social upheaval, is a famous example.

The Story of Sinuhe, written in Middle Egyptian, might be the classic of Egyptian literature. Also written at this time was the Westcar Papyrus, a set of stories told to Khufu by his sons relating the marvels performed by priests. The Instruction of Amenemope is considered a masterpiece of Near Eastern literature. Towards the end of the New Kingdom, the vernacular language was more often employed to write popular pieces like the Story of Wenamun and the Instruction of Any. The former tells the story of a noble who is robbed on his way to buy cedar from Lebanon and of his struggle to return to Egypt. From about 700 BC, narrative stories and instructions, such as the popular Instructions of Onchsheshonqy, as well as personal and business documents were written in the demotic script and phase of Egyptian. Many stories written in demotic during the Greco-Roman period were set in previous historical eras, when Egypt was an independent nation ruled by great pharaohs such as Ramesses II.

Most ancient Egyptians were farmers tied to the land. Their dwellings were restricted to immediate family members, and were constructed of mud-brick designed to remain cool in the heat of the day. Each home had a kitchen with an open roof, which contained a grindstone for milling grain and a small oven for baking the bread. Walls were painted white and could be covered with dyed linen wall hangings. Floors were covered with reed mats, while wooden stools, beds raised from the floor and individual tables comprised the furniture.

The ancient Egyptians placed a great value on hygiene and appearance. Most bathed in the Nile and used a pasty soap made from animal fat and chalk. Men shaved their entire bodies for cleanliness; perfumes and aromatic ointments covered bad odors and soothed skin. Clothing was made from simple linen sheets that were bleached white, and both men and women of the upper classes wore wigs, jewelry, and cosmetics. Children went without clothing until maturity, at about age 12, and at this age males were circumcised and had their heads shaved. Mothers were responsible for taking care of the children, while the father provided the family's income.

Music and dance were popular entertainments for those who could afford them. Early instruments included flutes and harps, while instruments similar to trumpets, oboes, and pipes developed later and became popular. In the New Kingdom, the Egyptians played on bells, cymbals, tambourines, drums, and imported lutes and lyres from Asia. The sistrum was a rattle-like musical instrument that was especially important in religious ceremonies.

The ancient Egyptians enjoyed a variety of leisure activities, including games and music. Senet, a board game where pieces moved according to random chance, was particularly popular from the earliest times; another similar game was mehen, which had a circular gaming board. “Hounds and Jackals” also known as 58 holes is another example of board games played in ancient Egypt. The first complete set of this game was discovered from a Theban tomb of the Egyptian pharaoh Amenemhat IV that dates to the 13th Dynasty. Juggling and ball games were popular with children, and wrestling is also documented in a tomb at Beni Hasan. The wealthy members of ancient Egyptian society enjoyed hunting and boating as well.

The excavation of the workers village of Deir el-Medina has resulted in one of the most thoroughly documented accounts of community life in the ancient world, which spans almost four hundred years. There is no comparable site in which the organization, social interactions, working and living conditions of a community have been studied in such detail.

Egyptian cuisine remained remarkably stable over time; indeed, the cuisine of modern Egypt retains some striking similarities to the cuisine of the ancients. The staple diet consisted of bread and beer, supplemented with vegetables such as onions and garlic, and fruit such as dates and figs. Wine and meat were enjoyed by all on feast days while the upper classes indulged on a more regular basis. Fish, meat, and fowl could be salted or dried, and could be cooked in stews or roasted on a grill.

The architecture of ancient Egypt includes some of the most famous structures in the world: the Great Pyramids of Giza and the temples at Thebes. Building projects were organized and funded by the state for religious and commemorative purposes, but also to reinforce the wide-ranging power of the pharaoh. The ancient Egyptians were skilled builders; using only simple but effective tools and sighting instruments, architects could build large stone structures with great accuracy and precision that is still envied today.

The domestic dwellings of elite and ordinary Egyptians alike were constructed from perishable materials such as mud bricks and wood, and have not survived. Peasants lived in simple homes, while the palaces of the elite and the pharaoh were more elaborate structures. A few surviving New Kingdom palaces, such as those in Malkata and Amarna, show richly decorated walls and floors with scenes of people, birds, water pools, deities and geometric designs. Important structures such as temples and tombs that were intended to last forever were constructed of stone instead of mud bricks. The architectural elements used in the world's first large-scale stone building, Djoser's mortuary complex, include post and lintel supports in the papyrus and lotus motif.

The earliest preserved ancient Egyptian temples, such as those at Giza, consist of single, enclosed halls with roof slabs supported by columns. In the New Kingdom, architects added the pylon, the open courtyard, and the enclosed hypostyle hall to the front of the temple's sanctuary, a style that was standard until the Greco-Roman period. The earliest and most popular tomb architecture in the Old Kingdom was the mastaba, a flat-roofed rectangular structure of mudbrick or stone built over an underground burial chamber. The step pyramid of Djoser is a series of stone mastabas stacked on top of each other. Pyramids were built during the Old and Middle Kingdoms, but most later rulers abandoned them in favor of less conspicuous rock-cut tombs. The use of the pyramid form continued in private tomb chapels of the New Kingdom and in the royal pyramids of Nubia.

The ancient Egyptians produced art to serve functional purposes. For over 3500 years, artists adhered to artistic forms and iconography that were developed during the Old Kingdom, following a strict set of principles that resisted foreign influence and internal change. These artistic standards—simple lines, shapes, and flat areas of color combined with the characteristic flat projection of figures with no indication of spatial depth—created a sense of order and balance within a composition. Images and text were intimately interwoven on tomb and temple walls, coffins, stelae, and even statues. The Narmer Palette, for example, displays figures that can also be read as hieroglyphs. Because of the rigid rules that governed its highly stylized and symbolic appearance, ancient Egyptian art served its political and religious purposes with precision and clarity.

Ancient Egyptian artisans used stone as a medium for carving statues and fine reliefs, but used wood as a cheap and easily carved substitute. Paints were obtained from minerals such as iron ores (red and yellow ochres), copper ores (blue and green), soot or charcoal (black), and limestone (white). Paints could be mixed with gum arabic as a binder and pressed into cakes, which could be moistened with water when needed.

Pharaohs used reliefs to record victories in battle, royal decrees, and religious scenes. Common citizens had access to pieces of funerary art, such as shabti statues and books of the dead, which they believed would protect them in the afterlife. During the Middle Kingdom, wooden or clay models depicting scenes from everyday life became popular additions to the tomb. In an attempt to duplicate the activities of the living in the afterlife, these models show laborers, houses, boats, and even military formations that are scale representations of the ideal ancient Egyptian afterlife.

Despite the homogeneity of ancient Egyptian art, the styles of particular times and places sometimes reflected changing cultural or political attitudes. After the invasion of the Hyksos in the Second Intermediate Period, Minoan-style frescoes were found in Avaris. The most striking example of a politically driven change in artistic forms comes from the Amarna period, where figures were radically altered to conform to Akhenaten's revolutionary religious ideas. This style, known as Amarna art, was quickly abandoned after Akhenaten's death and replaced by the traditional forms.

Beliefs in the divine and in the afterlife were ingrained in ancient Egyptian civilization from its inception; pharaonic rule was based on the divine right of kings. The Egyptian pantheon was populated by gods who had supernatural powers and were called on for help or protection. However, the gods were not always viewed as benevolent, and Egyptians believed they had to be appeased with offerings and prayers. The structure of this pantheon changed continually as new deities were promoted in the hierarchy, but priests made no effort to organize the diverse and sometimes conflicting myths and stories into a coherent system. These various conceptions of divinity were not considered contradictory but rather layers in the multiple facets of reality.

Gods were worshiped in cult temples administered by priests acting on the king's behalf. At the center of the temple was the cult statue in a shrine. Temples were not places of public worship or congregation, and only on select feast days and celebrations was a shrine carrying the statue of the god brought out for public worship. Normally, the god's domain was sealed off from the outside world and was only accessible to temple officials. Common citizens could worship private statues in their homes, and amulets offered protection against the forces of chaos. After the New Kingdom, the pharaoh's role as a spiritual intermediary was de-emphasized as religious customs shifted to direct worship of the gods. As a result, priests developed a system of oracles to communicate the will of the gods directly to the people.

The Egyptians believed that every human being was composed of physical and spiritual parts or "aspects". In addition to the body, each person had a "šwt" (shadow), a "ba" (personality or soul), a "ka" (life-force), and a "name". The heart, rather than the brain, was considered the seat of thoughts and emotions. After death, the spiritual aspects were released from the body and could move at will, but they required the physical remains (or a substitute, such as a statue) as a permanent home. The ultimate goal of the deceased was to rejoin his "ka" and "ba" and become one of the "blessed dead", living on as an "akh", or "effective one". For this to happen, the deceased had to be judged worthy in a trial, in which the heart was weighed against a "feather of truth." If deemed worthy, the deceased could continue their existence on earth in spiritual form.

The ancient Egyptians maintained an elaborate set of burial customs that they believed were necessary to ensure immortality after death. These customs involved preserving the body by mummification, performing burial ceremonies, and interring with the body goods the deceased would use in the afterlife. Before the Old Kingdom, bodies buried in desert pits were naturally preserved by desiccation. The arid, desert conditions were a boon throughout the history of ancient Egypt for burials of the poor, who could not afford the elaborate burial preparations available to the elite. Wealthier Egyptians began to bury their dead in stone tombs and use artificial mummification, which involved removing the internal organs, wrapping the body in linen, and burying it in a rectangular stone sarcophagus or wooden coffin. Beginning in the Fourth Dynasty, some parts were preserved separately in canopic jars.

By the New Kingdom, the ancient Egyptians had perfected the art of mummification; the best technique took 70 days and involved removing the internal organs, removing the brain through the nose, and desiccating the body in a mixture of salts called natron. The body was then wrapped in linen with protective amulets inserted between layers and placed in a decorated anthropoid coffin. Mummies of the Late Period were also placed in painted cartonnage mummy cases. Actual preservation practices declined during the Ptolemaic and Roman eras, while greater emphasis was placed on the outer appearance of the mummy, which was decorated.

Wealthy Egyptians were buried with larger quantities of luxury items, but all burials, regardless of social status, included goods for the deceased. Funerary texts were often included in the grave, and, beginning in the New Kingdom, so were shabti statues that were believed to perform manual labor for them in the afterlife. Rituals in which the deceased was magically re-animated accompanied burials. After burial, living relatives were expected to occasionally bring food to the tomb and recite prayers on behalf of the deceased.

The ancient Egyptian military was responsible for defending Egypt against foreign invasion, and for maintaining Egypt's domination in the ancient Near East. The military protected mining expeditions to the Sinai during the Old Kingdom and fought civil wars during the First and Second Intermediate Periods. The military was responsible for maintaining fortifications along important trade routes, such as those found at the city of Buhen on the way to Nubia. Forts also were constructed to serve as military bases, such as the fortress at Sile, which was a base of operations for expeditions to the Levant. In the New Kingdom, a series of pharaohs used the standing Egyptian army to attack and conquer Kush and parts of the Levant.

Typical military equipment included bows and arrows, spears, and round-topped shields made by stretching animal skin over a wooden frame. In the New Kingdom, the military began using chariots that had earlier been introduced by the Hyksos invaders. Weapons and armor continued to improve after the adoption of bronze: shields were now made from solid wood with a bronze buckle, spears were tipped with a bronze point, and the Khopesh was adopted from Asiatic soldiers. The pharaoh was usually depicted in art and literature riding at the head of the army; it has been suggested that at least a few pharaohs, such as Seqenenre Tao II and his sons, did do so. However, it has also been argued that "kings of this period did not personally act as frontline war leaders, fighting alongside their troops." Soldiers were recruited from the general population, but during, and especially after, the New Kingdom, mercenaries from Nubia, Kush, and Libya were hired to fight for Egypt.

In technology, medicine, and mathematics, ancient Egypt achieved a relatively high standard of productivity and sophistication. Traditional empiricism, as evidenced by the Edwin Smith and Ebers papyri (c. 1600 BC), is first credited to Egypt. The Egyptians created their own alphabet and decimal system.

Even before the Old Kingdom, the ancient Egyptians had developed a glassy material known as faience, which they treated as a type of artificial semi-precious stone. Faience is a non-clay ceramic made of silica, small amounts of lime and soda, and a colorant, typically copper. The material was used to make beads, tiles, figurines, and small wares. Several methods can be used to create faience, but typically production involved application of the powdered materials in the form of a paste over a clay core, which was then fired. By a related technique, the ancient Egyptians produced a pigment known as Egyptian Blue, also called blue frit, which is produced by fusing (or sintering) silica, copper, lime, and an alkali such as natron. The product can be ground up and used as a pigment.

The ancient Egyptians could fabricate a wide variety of objects from glass with great skill, but it is not clear whether they developed the process independently. It is also unclear whether they made their own raw glass or merely imported pre-made ingots, which they melted and finished. However, they did have technical expertise in making objects, as well as adding trace elements to control the color of the finished glass. A range of colors could be produced, including yellow, red, green, blue, purple, and white, and the glass could be made either transparent or opaque.

The medical problems of the ancient Egyptians stemmed directly from their environment. Living and working close to the Nile brought hazards from malaria and debilitating schistosomiasis parasites, which caused liver and intestinal damage. Dangerous wildlife such as crocodiles and hippos were also a common threat. The lifelong labors of farming and building put stress on the spine and joints, and traumatic injuries from construction and warfare all took a significant toll on the body. The grit and sand from stone-ground flour abraded teeth, leaving them susceptible to abscesses (though caries were rare).

The diets of the wealthy were rich in sugars, which promoted periodontal disease. Despite the flattering physiques portrayed on tomb walls, the overweight mummies of many of the upper class show the effects of a life of overindulgence. Adult life expectancy was about 35 for men and 30 for women, but reaching adulthood was difficult as about one-third of the population died in infancy.

Ancient Egyptian physicians were renowned in the ancient Near East for their healing skills, and some, such as Imhotep, remained famous long after their deaths. Herodotus remarked that there was a high degree of specialization among Egyptian physicians, with some treating only the head or the stomach, while others were eye-doctors and dentists. Training of physicians took place at the "Per Ankh" or "House of Life" institution, most notably those headquartered in Per-Bastet during the New Kingdom and at Abydos and Saïs in the Late period. Medical papyri show empirical knowledge of anatomy, injuries, and practical treatments.

Wounds were treated by bandaging with raw meat, white linen, sutures, nets, pads, and swabs soaked with honey to prevent infection, while opium thyme and belladona were used to relieve pain. The earliest records of burn treatment describe burn dressings that use the milk from mothers of male babies. Prayers were made to the goddess Isis. Moldy bread, honey and copper salts were also used to prevent infection from dirt in burns. Garlic and onions were used regularly to promote good health and were thought to relieve asthma symptoms. Ancient Egyptian surgeons stitched wounds, set broken bones, and amputated diseased limbs, but they recognized that some injuries were so serious that they could only make the patient comfortable until death occurred.

Early Egyptians knew how to assemble planks of wood into a ship hull and had mastered advanced forms of shipbuilding as early as 3000 BC. The Archaeological Institute of America reports that the oldest planked ships known are the Abydos boats. A group of 14 discovered ships in Abydos were constructed of wooden planks "sewn" together. Discovered by Egyptologist David O'Connor of New York University, woven straps were found to have been used to lash the planks together, and reeds or grass stuffed between the planks helped to seal the seams. Because the ships are all buried together and near a mortuary belonging to Pharaoh Khasekhemwy, originally they were all thought to have belonged to him, but one of the 14 ships dates to 3000 BC, and the associated pottery jars buried with the vessels also suggest earlier dating. The ship dating to 3000 BC was long and is now thought to perhaps have belonged to an earlier pharaoh, perhaps one as early as Hor-Aha.

Early Egyptians also knew how to assemble planks of wood with treenails to fasten them together, using pitch for caulking the seams. The "Khufu ship", a vessel sealed into a pit in the Giza pyramid complex at the foot of the Great Pyramid of Giza in the Fourth Dynasty around 2500 BC, is a full-size surviving example that may have filled the symbolic function of a solar barque. Early Egyptians also knew how to fasten the planks of this ship together with mortise and tenon joints.

Large seagoing ships are known to have been heavily used by the Egyptians in their trade with the city states of the eastern Mediterranean, especially Byblos (on the coast of modern-day Lebanon), and in several expeditions down the Red Sea to the Land of Punt. In fact one of the earliest Egyptian words for a seagoing ship is a "Byblos Ship", which originally defined a class of Egyptian seagoing ships used on the Byblos run; however, by the end of the Old Kingdom, the term had come to include large seagoing ships, whatever their destination.

In 2011 archaeologists from Italy, the United States, and Egypt excavating a dried-up lagoon known as Mersa Gawasis have unearthed traces of an ancient harbor that once launched early voyages like Hatshepsut's Punt expedition onto the open ocean. Some of the site's most evocative evidence for the ancient Egyptians' seafaring prowess include large ship timbers and hundreds of feet of ropes, made from papyrus, coiled in huge bundles. And in 2013 a team of Franco-Egyptian archaeologists discovered what is believed to be the world's oldest port, dating back about 4500 years, from the time of King Cheops on the Red Sea coast near Wadi el-Jarf (about 110 miles south of Suez).

In 1977, an ancient north–south canal dating to the Middle Kingdom of Egypt was discovered extending from Lake Timsah to the Ballah Lakes. It was dated to the Middle Kingdom of Egypt by extrapolating dates of ancient sites constructed along its course.

The earliest attested examples of mathematical calculations date to the predynastic Naqada period, and show a fully developed numeral system. The importance of mathematics to an educated Egyptian is suggested by a New Kingdom fictional letter in which the writer proposes a scholarly competition between himself and another scribe regarding everyday calculation tasks such as accounting of land, labor, and grain. Texts such as the Rhind Mathematical Papyrus and the Moscow Mathematical Papyrus show that the ancient Egyptians could perform the four basic mathematical operations—addition, subtraction, multiplication, and division—use fractions, calculate the areas of rectangles, triangles, and circles and compute the volumes of boxes, columns and pyramids. They understood basic concepts of algebra and geometry, and could solve simple sets of simultaneous equations.

Mathematical notation was decimal, and based on hieroglyphic signs for each power of ten up to one million. Each of these could be written as many times as necessary to add up to the desired number; so to write the number eighty or eight hundred, the symbol for ten or one hundred was written eight times respectively. Because their methods of calculation could not handle most fractions with a numerator greater than one, they had to write fractions as the sum of several fractions. For example, they resolved the fraction "two-fifths" into the sum of "one-third" + "one-fifteenth". Standard tables of values facilitated this. Some common fractions, however, were written with a special glyph—the equivalent of the modern two-thirds is shown on the right.

Ancient Egyptian mathematicians knew the Pythagorean theorem as an empirical formula. They were aware, for example, that a triangle had a right angle opposite the hypotenuse when its sides were in a 3–4–5 ratio. They were able to estimate the area of a circle by subtracting one-ninth from its diameter and squaring the result:

a reasonable approximation of the formula π"r".

The golden ratio seems to be reflected in many Egyptian constructions, including the pyramids, but its use may have been an unintended consequence of the ancient Egyptian practice of combining the use of knotted ropes with an intuitive sense of proportion and harmony.

A team led by Johannes Krause managed the first reliable sequencing of the genomes of 90 mummified individuals in 2017 from northern Egypt (buried near modern-day Cairo), which constituted "the first reliable data set obtained from ancient Egyptians using high-throughput DNA sequencing methods." Whilst not conclusive, because of the non-exhaustive time frame and restricted location that the mummies represent, their study nevertheless showed that these ancient Egyptians "closely resembled ancient and modern Near Eastern populations, especially those in the Levant, and had almost no DNA from sub-Saharan Africa. What's more, the genetics of the mummies remained remarkably consistent even as different powers—including Nubians, Greeks, and Romans—conquered the empire." Later, however, something did alter the genomes of Egyptians. Some 15% to 20% of modern Egyptians' DNA reflects sub-Saharan ancestry, but the ancient mummies had only 6–15% sub-Saharan DNA. They called for additional research to be undertaken. Other genetic studies show much greater levels of sub-Saharan African ancestry in the current-day populations of southern as opposed to northern Egypt, and anticipate that mummies from southern Egypt would contain greater levels of sub-Saharan African ancestry than Lower Egyptian mummies.

The culture and monuments of ancient Egypt have left a lasting legacy on the world. The cult of the goddess Isis, for example, became popular in the Roman Empire, as obelisks and other relics were transported back to Rome. The Romans also imported building materials from Egypt to erect Egyptian-style structures. Early historians such as Herodotus, Strabo, and Diodorus Siculus studied and wrote about the land, which Romans came to view as a place of mystery.

During the Middle Ages and the Renaissance, Egyptian pagan culture was in decline after the rise of Christianity and later Islam, but interest in Egyptian antiquity continued in the writings of medieval scholars such as Dhul-Nun al-Misri and al-Maqrizi. In the seventeenth and eighteenth centuries, European travelers and tourists brought back antiquities and wrote stories of their journeys, leading to a wave of Egyptomania across Europe. This renewed interest sent collectors to Egypt, who took, purchased, or were given many important antiquities.

Although the European colonial occupation of Egypt destroyed a significant portion of the country's historical legacy, some foreigners left more positive marks. Napoleon, for example, arranged the first studies in Egyptology when he brought some 150 scientists and artists to study and document Egypt's natural history, which was published in the "Description de l'Égypte".

In the 20th century, the Egyptian Government and archaeologists alike recognized the importance of cultural respect and integrity in excavations. The Supreme Council of Antiquities now approves and oversees all excavations, which are aimed at finding information rather than treasure. The council also supervises museums and monument reconstruction programs designed to preserve the historical legacy of Egypt.




</doc>
<doc id="875" url="https://en.wikipedia.org/wiki?curid=875" title="Analog Brothers">
Analog Brothers

Analog Brothers were an experimental hip hop band featuring Tracy 'Ice Oscillator' Marrow (Body Count's Ice-T) on keyboards, drums and vocals, Keith 'Keith Korg' Thornton (Ultramagnetic MCs' Kool Keith) on bass, strings and vocals, Marc 'Mark Moog' Giveand (Raw Breed's Marc Live) on drums, violins and vocals, Christopher 'Silver Synth' Rodgers (Black Silver) on synthesizer, lazar bell and vocals, and Rex Colonel 'Rex Roland JX3P' Doby Jr. (Pimpin' Rex) on keyboards, vocals and production. Its album "Pimp to Eat" featured guest appearances by various members of Rhyme Syndicate, Odd Oberheim, Jacky Jasper (who appears as Jacky Jasper on the song "We Sleep Days" and H-Bomb on "War"), D.J. Cisco from S.M., Synth-A-Size Sisters and Teflon.

While the group only recorded one album together as the Analog Brothers, a few bootlegs of its live concert performances, including freestyles with original lyrics, have occasionally surfaced online. After "Pimp to Eat", the Analog Brothers continued performing together in various line ups. Kool Keith and Marc Live joined with Jacky Jasper to release two albums as KHM. Marc Live rapped with Ice T's group SMG. Marc also formed a group with Black Silver called Live Black, but while five of their tracks were released on a demo CD sold at concerts, Live Black's first album has yet to be released.

In 2008, Ice-T and Black Silver toured together as Black Ice, and released an album together called "Urban Legends".

In 2013 Black Silver and newest member to Analog Brothers, Kiew Kurzweil (Kiew Nikon of Kinetic) collaborated on the joint album called "Slang Banging (Return to Analog)" with production by Junkadelic Music. In addition to all this, the Analog Brothers continue to make frequent appearances on each other's solo albums.



</doc>
<doc id="876" url="https://en.wikipedia.org/wiki?curid=876" title="Motor neuron disease">
Motor neuron disease

Motor neuron diseases (MNDs) are a group of neurodegenerative disorders that selectively affect motor neurons, the cells which control voluntary muscles of the body.

According to ICD-11, the following disorders are counted among motor neuron diseases: amyotrophic lateral sclerosis (ALS), progressive bulbar palsy (PBP), pseudobulbar palsy, progressive muscular atrophy (PMA), primary lateral sclerosis (PLS), and monomelic amyotrophy (MMA), as well as some rarer variants resembling ALS.

Motor neuron diseases affect both children and adults. While each motor neuron disease affects patients differently, they all cause movement-related symptoms, mainly muscle weakness. Most of these diseases seem to occur randomly without known causes, but some forms are inherited. Studies into these inherited forms have led to discoveries of various genes (e.g. "SOD1") that are thought be important in understanding how the disease occurs.

Symptoms of motor neuron diseases can be first seen at birth or can come on slowly later in life. Most of these diseases worsen over time; while some of them shorten one's life expectancy (e.g. ALS), others do not.

Currently, there are no approved treatments for the majority of motor neuron disorders, and care is mostly symptomatic.

Signs and symptoms depend on the specific disease, but motor neuron diseases typically manifest as a group of movement-related symptoms. They come on slowly, and worsen over the course of more than three months. Various patterns of muscle weakness are seen, and muscle cramps and spasms may occur. One can have difficulty breathing with climbing stairs (exertion), difficulty breathing when lying down (orthopnea), or even respiratory failure if breathing muscles become involved. Bulbar symptoms, including difficulty speaking (dysarthria), difficulty swallowing (dysphagia), and excessive saliva production (sialorrhea), can also occur. Sensation, or the ability to feel, is typically not affected. Emotional disturbance (e.g. pseudobulbar affect) and cognitive and behavioral changes (e.g. problems in word fluency, decision-making, and memory) are also seen. There can be lower motor neuron findings (e.g. muscle wasting, muscle twitching), upper motor neuron findings (e.g. brisk reflexes, Babinski reflex, Hoffman's reflex, increased muscle tone), or both.

Motor neuron diseases are seen both in children and in adults. Those that affect children tend to be inherited or familial, and their symptoms are either present at birth or appear before learning to walk. Those that affect adults tend to appear after age 40. The clinical course depends on the specific disease, but most progress or worsen over the course of months. Some are fatal (e.g. ALS), while others are not (e.g. PLS).

Various patterns of muscle weakness occur in different motor neuron diseases. Weakness can be symmetric or asymmetric, and it can occur in body parts that are distal, proximal, or both... According to Statland et al., there are three main weakness patterns that are seen in motor neuron diseases, which are:


Motor neuron diseases are on a spectrum in terms of upper and lower motor neuron involvement. Some have just lower or upper motor neuron findings, while others have a mix of both. Lower motor neuron (LMN) findings include muscle atrophy and fasciculations, and upper motor neuron (UMN) findings include hyperreflexia, spasticity, muscle spasm, and abnormal reflexes.

Pure upper motor neuron diseases, or those with just UMN findings, include PLS.

Pure lower motor neuron diseases, or those with just LMN findings, include PMA.

Motor neuron diseases with both UMN and LMN findings include both familial and sporadic ALS.

Most cases are sporadic and their causes are usually not known. It is thought that environmental, toxic, viral, or genetic factors may be involved.

TARDBP (TAR DNA-binding protein 43), also referred to as TDP-43, is a critical component of the non-homologous end joining (NHEJ) enzymatic pathway that repairs DNA double-strand breaks in pluripotent stem cell-derived motor neurons. TDP-43 is rapidly recruited to double-strand breaks where it acts as a scaffold for the recruitment of the XRCC4-DNA ligase protein complex that then acts to repair double-strand breaks. About 95% of ALS patients have abnormalities in the nucleus-cytoplasmic localization in spinal motor neurons of TDP43. In TDP-43 depleted human neural stem cell-derived motor neurons, as well as in sporadic ALS patients’ spinal cord specimens there is significant double-strand break accumulation and reduced levels of NHEJ.

In adults, men are more commonly affected than women.

Differential diagnosis can be challenging due to the number of overlapping symptoms, shared between several motor neuron diseases. Frequently, the diagnosis is based on clinical findings (i.e. LMN vs. UMN signs and symptoms, patterns of weakness), family history of MND, and a variation of tests, many of which are used to rule out disease mimics, which can manifest with identical symptoms.

Please refer to individual articles for the diagnostic methods used in each individual motor neuron disease.

Motor neuron disease describes a collection of clinical disorders, characterized by progressive muscle weakness and the degeneration of the motor neuron on electrophysiological testing. As discussed above, the term "motor neuron disease" has varying meanings in different countries. Similarly, the literature inconsistently classifies which degenerative motor neuron disorders can be included under the umbrella term "motor neuron disease". The four main types of MND are marked (*) in the table below.

All types of MND can be differentiated by two defining characteristics:


Sporadic or acquired MNDs occur in patients with no family history of degenerative motor neuron disease. Inherited or genetic MNDs adhere to one of the following inheritance patterns: autosomal dominant, autosomal recessive, or X-linked. Some disorders, like ALS, can occur sporadically (85%) or can have a genetic cause (15%) with the same clinical symptoms and progression of disease.

UMNs are motor neurons that project from the cortex down to the brainstem or spinal cord. LMNs originate in the anterior horns of the spinal cord and synapse on peripheral muscles. Both motor neurons are necessary for the strong contraction of a muscle, but damage to an UMN can be distinguished from damage to a LMN by physical exam.


There are no known curative treatments for the majority of motor neuron disorders. Please refer to the articles on individual disorders for more details.

The table below lists life expectancy for patients who are diagnosed with MND. Please refer to individual articles for more detail.
In the United States, the term "motor neuron disease" is often used to denote amyotrophic lateral sclerosis (Lou Gehrig's disease), the most common disorder in the group. In the United Kingdom, the term is spelled "motor neurone disease" and is frequently used for the entire group, but can also refer specifically to ALS.

While MND refers to a specific subset of similar diseases, there are numerous other diseases of motor neurons that are referred to collectively as "motor neuron disorders", for instance the diseases belonging to the spinal muscular atrophies group. However, they are not classified as "motor neuron diseases" by the 11th edition of the International Statistical Classification of Diseases and Related Health Problems (ICD-11), which is the definition followed in this article.



</doc>
<doc id="877" url="https://en.wikipedia.org/wiki?curid=877" title="Abjad">
Abjad

An abjad (pronounced or ) is a type of writing system in which each symbol or glyph stands for a consonant, leaving the reader to supply the appropriate vowel. So-called impure abjads do represent vowels, either with optional diacritics, a limited number of distinct vowel glyphs, or both. The name "abjad" is based on the old Arabic alphabet's first four letters—a, b, j, d—to replace the common terms "consonantary" or "consonantal alphabet" to refer to the family of scripts called West Semitic.

The name "abjad" (' ) is derived from pronouncing the first letters of the Old alphabet order in Arabic. The ordering (') of Arabic letters used to match that of the older Hebrew, Phoenician and Semitic alphabets: "".

According to the formulations of Peter T. Daniels, abjads differ from alphabets in that only consonants, not vowels, are represented among the basic graphemes. Abjads differ from abugidas, another category defined by Daniels, in that in abjads, the vowel sound is "implied" by phonology, and where vowel marks exist for the system, such as nikkud for Hebrew and ḥarakāt for Arabic, their use is optional and not the dominant (or literate) form. Abugidas mark all vowels (other than the "inherent" vowel) with a diacritic, a minor attachment to the letter, or a standalone glyph. Some abugidas use a special symbol to "suppress" the inherent vowel so that the consonant alone can be properly represented. In a syllabary, a grapheme denotes a complete syllable, that is, either a lone vowel sound or a combination of a vowel sound with one or more consonant sounds.

The antagonism of abjad versus alphabet, as it was formulated by Daniels, has been rejected by some other scholars because abjad is also used as a term not only for the Arabic numeral system but, which is most important in terms of historical grammatology, also as term for the alphabetic device (i.e. letter order) of ancient Northwest Semitic scripts in opposition to the 'south Arabian' order. This caused fatal effects on terminology in general and especially in (ancient) Semitic philology. Also, it suggests that consonantal alphabets, in opposition to, for instance, the Greek alphabet, were not yet true alphabets and not yet entirely complete, lacking something important to be a fully working script system. It has also been objected that, as a set of letters, an alphabet is not the mirror of what should be there in a language from a phonological point of view; rather, it is the data stock of what provides maximum efficiency with least effort from a semantic point of view.

The first abjad to gain widespread usage was the Phoenician abjad. Unlike other contemporary scripts, such as cuneiform and Egyptian hieroglyphs, the Phoenician script consisted of only a few dozen symbols. This made the script easy to learn, and seafaring Phoenician merchants took the script throughout the then-known world.

The Phoenician abjad was a radical simplification of phonetic writing, since hieroglyphics required the writer to pick a hieroglyph starting with the same sound that the writer wanted to write in order to write phonetically, much as "man'yōgana" (Chinese characters used solely for phonetic use) was used to represent Japanese phonetically before the invention of kana.

Phoenician gave rise to a number of new writing systems, including the Greek alphabet and Aramaic, a widely used abjad. The Greek alphabet evolved into the modern western alphabets, such as Latin and Cyrillic, while Aramaic became the ancestor of many modern abjads and abugidas of Asia.

Impure abjads have characters for some vowels, optional vowel diacritics, or both. The term pure abjad refers to scripts entirely lacking in vowel indicators. However, most modern abjads, such as Arabic, Hebrew, Aramaic, and Pahlavi, are "impure" abjadsthat is, they also contain symbols for some of the vowel phonemes, although the said non-diacritic vowel letters are also used to write certain consonants, particularly approximants that sound similar to long vowels. A "pure" abjad is exemplified (perhaps) by very early forms of ancient Phoenician, though at some point (at least by the 9th century BC) it and most of the contemporary Semitic abjads had begun to overload a few of the consonant symbols with a secondary function as vowel markers, called "matres lectionis". This practice was at first rare and limited in scope but became increasingly common and more developed in later times.

In the 9th century BC the Greeks adapted the Phoenician script for use in their own language. The phonetic structure of the Greek language created too many ambiguities when vowels went unrepresented, so the script was modified. They did not need letters for the guttural sounds represented by "aleph", "he", "heth" or "ayin", so these symbols were assigned vocalic values. The letters "waw" and "yod" were also adapted into vowel signs; along with "he", these were already used as "matres lectionis" in Phoenician. The major innovation of Greek was to dedicate these symbols exclusively and unambiguously to vowel sounds that could be combined arbitrarily with consonants (as opposed to syllabaries such as Linear B which usually have vowel symbols but cannot combine them with consonants to form arbitrary syllables).

Abugidas developed along a slightly different route. The basic consonantal symbol was considered to have an inherent "a" vowel sound. Hooks or short lines attached to various parts of the basic letter modify the vowel. In this way, the South Arabian alphabet evolved into the Ge'ez alphabet between the 5th century BC and the 5th century AD. Similarly, around the 3rd century BC, the Brāhmī script developed (from the Aramaic abjad, it has been hypothesized).

The other major family of abugidas, Canadian Aboriginal syllabics, was initially developed in the 1840s by missionary and linguist James Evans for the Cree and Ojibwe languages. Evans used features of Devanagari script and Pitman shorthand to create his initial abugida. Later in the 19th century, other missionaries adapted Evans' system to other Canadian aboriginal languages. Canadian syllabics differ from other abugidas in that the vowel is indicated by rotation of the consonantal symbol, with each vowel having a consistent orientation.

The abjad form of writing is well-adapted to the morphological structure of the Semitic languages it was developed to write. This is because words in Semitic languages are formed from a root consisting of (usually) three consonants, the vowels being used to indicate inflectional or derived forms. For instance, according to Classical Arabic and Modern Standard Arabic, from the Arabic root "Dh-B-Ḥ" (to slaughter) can be derived the forms ' (he slaughtered), ' (you (masculine singular) slaughtered), ' (he slaughters), and ' (slaughterhouse). In most cases, the absence of full glyphs for vowels makes the common root clearer, allowing readers to guess the meaning of unfamiliar words from familiar roots (especially in conjunction with context clues) and improving word recognition while reading for practiced readers.

By contrast, the Arabic and Hebrew scripts sometimes perform the role of true alphabets rather than abjads when used to write certain Indo-European languages, including Kurdish, Bosnian, and Yiddish.




</doc>
<doc id="878" url="https://en.wikipedia.org/wiki?curid=878" title="Abugida">
Abugida

An abugida (from Ge'ez: አቡጊዳ "’abugida"), or alphasyllabary, is a segmental writing system in which consonant–vowel sequences are written as a unit; each unit is based on a consonant letter, and vowel notation is secondary. This contrasts with a full alphabet, in which vowels have status equal to consonants, and with an abjad, in which vowel marking is absent, partial, or optional (although in less formal contexts, all three types of script may be termed alphabets). The terms also contrast them with a syllabary, in which the symbols cannot be split into separate consonants and vowels. 

Abugidas include the extensive Brahmic family of scripts of Tibet, South and Southeast Asia, Semitic Ethiopic scripts, and Canadian Aboriginal syllabics (which are themselves based in part on Brahmic scripts).

As is the case for syllabaries, the units of the writing system may consist of the representations both of syllables and of consonants. For scripts of the Brahmic family, the term "akshara" is used for the units.

"’Äbugida" is an Ethiopian name for the Ge‘ez script, taken from four letters of that script, "ä bu gi da", in much the same way that "abecedary" is derived from Latin "a be ce de", "abjad" is derived from the Arabic "a b j d", and "alphabet" is derived from the names of the two first letters in the Greek alphabet, "alpha" and "beta". 

"Abugida" as a term in linguistics was proposed by Peter T. Daniels in his 1990 typology of writing systems. As Daniels used the word, an abugida is in contrast with a syllabary, where letters with shared consonants or vowels show no particular resemblance to one another, and also with an alphabet proper, where independent letters are used to denote both consonants and vowels. The term "alphasyllabary" was suggested for the Indic scripts in 1997 by William Bright, following South Asian linguistic usage, to convey the idea that "they share features of both alphabet and syllabary."

Abugidas were long considered to be syllabaries, or intermediate between syllabaries and alphabets, and the term "syllabics" is retained in the name of Canadian Aboriginal Syllabics. Other terms that have been used include "neosyllabary" (Février 1959), "pseudo-alphabet" (Householder 1959), "semisyllabary" (Diringer 1968; a word that has other uses) and "syllabic alphabet" (Coulmas 1996; this term is also a synonym for syllabary).

The formal definitions given by Daniels and Bright for abugida and alphasyllabary differ; some writing systems are abugidas but not alphasyllabaries, and some are alphasyllabaries but not abugidas. An abugida is defined as "a type of writing system whose basic characters denote consonants followed by a particular vowel, and in which diacritics denote other vowels". (This 'particular vowel' is referred to as the "inherent" or "implicit" vowel, as opposed to the "explicit" vowels marked by the 'diacritics'.) An alphasyllabary is defined as "a type of writing system in which the vowels are denoted by subsidiary symbols not all of which occur in a linear order (with relation to the consonant symbols) that is congruent with their temporal order in speech". Bright did not require that an alphabet explicitly represent all vowels. ʼPhags-pa is an example of an abugida that is not an alphasyllabary, and modern Lao is an example of an alphasyllabary that is not an abugida, for its vowels are always explicit.

This description is expressed in terms of an abugida. Formally, an alphasyllabary that is not an abugida can be converted to an abugida by adding a purely formal vowel sound that is never used and declaring that to be the inherent vowel of the letters representing consonants. This may formally make the system ambiguous, but in 'practice' this is not a problem, for then the interpretation with the never used inherent vowel sound will always be a wrong interpretation. Note that the actual pronunciation may be complicated by interactions between the sounds apparently written just as the sounds of the letters in the English words "wan, gem" and "war" are affected by neighbouring letters.

The fundamental principles of an abugida apply to words made up of consonant-vowel (CV) syllables. The syllables are written as a linear sequences of the units of the script. Each syllable is either a letter that represents the sound of a consonant and the inherent vowel, or a letter with a modification to indicate the vowel, either by means of diacritics, or by changes in the form of the letter itself. If all modifications are by diacritics and all diacritics follow the direction of the writing of the letters, then the abugida is not an alphasyllabary.

However, most languages have words that are more complicated than a sequence of CV syllables, even ignoring tone.

The first complication is syllables that consist of just a vowel (V). This issue does not arise in some languages because every syllable starts with a consonant. This is common in Semitic languages and in languages of mainland SE Asia, and for such languages this issue need not arise. For some languages, a zero consonant letter is used as though every syllable began with a consonant. For other languages, each vowel has a separate letter that is used for each syllable consisting of just the vowel. These letters are known as "independent vowels", and are found in most Indic scripts. These letters may be quite different from the corresponding diacritics, which by contrast are known as "dependent vowels". As a result of the spread of writing systems, independent vowels may be used to represent syllables beginning with a glottal stop, even for non-initial syllables.

The next two complications are sequences of consonants before a vowel (CCV) and syllables ending in a consonant (CVC). The simplest solution, which is not always available, is to break with the principle of writing words as a sequence of syllables and use a unit representing just a consonant (C). This unit may be represented with:

In a true abugida, the lack of distinctive marking may result from the diachronic loss of the inherent vowel, e.g. by syncope and apocope in Hindi.

When not handled by decomposition into C + CV, CCV syllables are handled by combining the two consonants. In the Indic scripts, the earliest method was simply to arrange them vertically, but the two consonants may merge as a conjunct consonant letters, where two or more letters are graphically joined in a ligature, or otherwise change their shapes. Rarely, one of the consonants may be replaced by a gemination mark, e.g. the Gurmukhi "". When they are arranged vertically, as in Burmese or Khmer, they are said to be 'stacked'. Often there has been a change to writing the two consonants side by side. In the latter case, the fact of combination may be indicated by a diacritic on one of the consonants or a change in the form of one of the consonants, e.g. the half forms of Devanagari. Generally, the reading order is top to bottom or the general reading order of the script, but sometimes the order is reversed.

The division of a word into syllables for the purposes of writing does not always accord with the natural phonetics of the language. For example, Brahmic scripts commonly handle a phonetic sequence CVC-CV as CV-CCV or CV-C-CV. However, sometimes phonetic CVC syllables are handled as single units, and the final consonant may be represented:


More complicated unit structures (e.g. CC or CCVC) are handled by combining the various techniques above.

There are three principal families of abugidas, depending on whether vowels are indicated by modifying consonants by "diacritics, distortion," or "orientation."

Tāna of the Maldives has dependent vowels and a zero vowel sign, but no inherent vowel.

Indic scripts originated in India and spread to Southeast Asia. All surviving Indic scripts are descendants of the Brahmi alphabet. Today they are used in most languages of South Asia (although replaced by Perso-Arabic in Urdu, Kashmiri and some other languages of Pakistan and India), mainland Southeast Asia (Myanmar, Thailand, Laos, and Cambodia), and Indonesian archipelago (Javanese, Balinese, Sundanese, etc.). The primary division is into North Indic scripts used in Northern India, Nepal, Tibet and Bhutan, and Southern Indic scripts used in South India, Sri Lanka and Southeast Asia.
South Indic letter forms are very rounded; North Indic less so, though Odia, Golmol and Litumol of Nepal script are rounded.
Most North Indic scripts' full letters incorporate a horizontal line at the top, with Gujarati and Odia as exceptions; South Indic scripts do not.

Indic scripts indicate vowels through dependent vowel signs (diacritics) around the consonants, often including a sign that explicitly indicates the lack of a vowel. If a consonant has no vowel sign, this indicates a default vowel. Vowel diacritics may appear above, below, to the left, to the right, or around the consonant.

The most widely used Indic script is Devanagari, shared by Hindi, Bhojpuri, Marathi, Konkani, Nepali, and often Sanskrit. A basic letter such as क in Hindi represents a syllable with the default vowel, in this case "ka" (). In some languages, including Hindi, it becomes a final closing consonant at the end of a word, in this case "k". The inherent vowel may be changed by adding vowel mark (diacritics), producing syllables such as कि "ki," कु "ku," के "ke," को "ko."

In many of the Brahmic scripts, a syllable beginning with a cluster is treated as a single character for purposes of vowel marking, so a vowel marker like ि "-i," falling before the character it modifies, may appear several positions before the place where it is pronounced. For example, the game cricket in Hindi is क्रिकेट "cricket;" the diacritic for appears before the consonant cluster , not before the . A more unusual example is seen in the Batak alphabet: Here the syllable "bim" is written "ba-ma-i-(virama)". That is, the vowel diacritic and virama are both written after the consonants for the whole syllable.

In many abugidas, there is also a diacritic to suppress the inherent vowel, yielding the bare consonant. In Devanagari, क् is "k," and ल् is "l". This is called the "virāma" or "halantam" in Sanskrit. It may be used to form consonant clusters, or to indicate that a consonant occurs at the end of a word. Thus in Sanskrit, a default vowel consonant such as क does not take on a final consonant sound. Instead, it keeps its vowel. For writing two consonants without a vowel in between, instead of using diacritics on the first consonant to remove its vowel, another popular method of special conjunct forms is used in which two or more consonant characters are merged to express a cluster, such as Devanagari: क्ल "kla." (Note that some fonts display this as क् followed by ल, rather than forming a conjunct. This expedient is used by ISCII and South Asian scripts of Unicode.) Thus a closed syllable such as "kal" requires two "aksharas" to write.

The Róng script used for the Lepcha language goes further than other Indic abugidas, in that a single "akshara" can represent a closed syllable: Not only the vowel, but any final consonant is indicated by a diacritic. For example, the syllable [sok] would be written as something like s̥̽, here with an underring representing and an overcross representing the diacritic for final . Most other Indic abugidas can only indicate a very limited set of final consonants with diacritics, such as or , if they can indicate any at all.

In Ethiopic (where the term "abugida" originates) the diacritics have been fused to the consonants to the point that they must be considered modifications of the form of the letters. Children learn each modification separately, as in a syllabary; nonetheless, the graphic similarities between syllables with the same consonant is readily apparent, unlike the case in a true syllabary.

Though now an abugida, the Ge'ez script, until the advent of Christianity ("ca." AD 350), had originally been what would now be termed an "abjad". In the Ge'ez abugida (or "fidel"), the base form of the letter (also known as "fidel") may be altered. For example, ሀ "hä" (base form), ሁ "hu" (with a right-side diacritic that doesn't alter the letter), ሂ "hi" (with a subdiacritic that compresses the consonant, so it is the same height), ህ "hə" or (where the letter is modified with a kink in the left arm).

In the family known as Canadian Aboriginal syllabics, which was inspired by the Devanagari script of India, vowels are indicated by changing the orientation of the syllabogram. Each vowel has a consistent orientation; for example, Inuktitut ᐱ "pi," ᐳ "pu," ᐸ "pa;" ᑎ "ti," ᑐ "tu," ᑕ "ta". Although there is a vowel inherent in each, all rotations have equal status and none can be identified as basic. Bare consonants are indicated either by separate diacritics, or by superscript versions of the "aksharas"; there is no vowel-killer mark.

Consonantal scripts ("abjads") are normally written without indication of many vowels. However, in some contexts like teaching materials or scriptures, Arabic and Hebrew are written with full indication of vowels via diacritic marks ("harakat", "niqqud") making them effectively alphasyllabaries. The Brahmic and Ethiopic families are thought to have originated from the Semitic abjads by the addition of vowel marks.
The Arabic scripts used for Kurdish in Iraq and for Uyghur in Xinjiang, China, as well as the Hebrew script of Yiddish, are fully vowelled, but because the vowels are written with full letters rather than diacritics (with the exception of distinguishing between /a/ and /o/ in the latter) and there are no inherent vowels, these are considered alphabets, not abugidas.

The imperial Mongol script called Phagspa was derived from the Tibetan abugida, but all vowels are written in-line rather than as diacritics. However, it retains the features of having an inherent vowel /a/ and having distinct initial vowel letters.

Pahawh Hmong is a non-segmental script that indicates syllable onsets and rimes, such as consonant clusters and vowels with final consonants. Thus it is not segmental and cannot be considered an abugida. However, it superficially resembles an abugida with the roles of consonant and vowel reversed. Most syllables are written with two letters in the order rime–onset (typically vowel-consonant), even though they are pronounced as onset-rime (consonant-vowel), rather like the position of the vowel in Devanagari, which is written before the consonant. Pahawh is also unusual in that, while an inherent rime (with mid tone) is unwritten, it also has an inherent onset . For the syllable , which requires one or the other of the inherent sounds to be overt, it is that is written. Thus it is the rime (vowel) that is basic to the system.

It is difficult to draw a dividing line between abugidas and other segmental scripts. For example, the Meroitic script of ancient Sudan did not indicate an inherent "a" (one symbol stood for both "m" and "ma," for example), and is thus similar to Brahmic family of abugidas. However, the other vowels were indicated with full letters, not diacritics or modification, so the system was essentially an alphabet that did not bother to write the most common vowel.

Several systems of shorthand use diacritics for vowels, but they do not have an inherent vowel, and are thus more similar to Thaana and Kurdish script than to the Brahmic scripts. The Gabelsberger shorthand system and its derivatives modify the "following" consonant to represent vowels. The Pollard script, which was based on shorthand, also uses diacritics for vowels; the placements of the vowel relative to the consonant indicates tone. Pitman shorthand uses straight strokes and quarter-circle marks in different orientations as the principal "alphabet" of consonants; vowels are shown as light and heavy dots, dashes and other marks in one of 3 possible positions to indicate the various vowel-sounds. However, to increase writing speed, Pitman has rules for "vowel indication" using the positioning or choice of consonant signs so that writing vowel-marks can be dispensed with.

As the term "alphasyllabary" suggests, abugidas have been considered an intermediate step between alphabets and syllabaries. Historically, abugidas appear to have evolved from abjads (vowelless alphabets). They contrast with syllabaries, where there is a distinct symbol for each syllable or consonant-vowel combination, and where these have no systematic similarity to each other, and typically develop directly from logographic scripts. Compare the examples above to sets of syllables in the Japanese hiragana syllabary: か "ka", き "ki", く "ku", け "ke", こ "ko" have nothing in common to indicate "k;" while ら "ra", り "ri", る "ru", れ "re", ろ "ro" have neither anything in common for "r", nor anything to indicate that they have the same vowels as the "k" set.

Most Indian and Indochinese abugidas appear to have first been developed from abjads with the Kharoṣṭhī and Brāhmī scripts; the abjad in question is usually considered to be the Aramaic one, but while the link between Aramaic and Kharosthi is more or less undisputed, this is not the case with Brahmi. The Kharosthi family does not survive today, but Brahmi's descendants include most of the modern scripts of South and Southeast Asia. Ge'ez derived from a different abjad, the Sabean script of Yemen; the advent of vowels coincided with the introduction of Christianity about AD 350.

The Ethiopic script is the elaboration of an abjad.

The Cree syllabary was invented with full knowledge of the Devanagari system.

The Meroitic script was developed from Egyptian hieroglyphs, within which various schemes of 'group writing' had been used for showing vowels.







</doc>

