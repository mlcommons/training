name: Llama 3.1
description: Large Language Model pretraining - Llama 3.1
authors:
  - { name: "MLCommons Best Practices Working Group" }

platform:
  accelerator_count: 1

docker:
  # Image name.
  image: dfjbtest/llama_3.1:0.0.1
  # Docker build context relative to $MLCUBE_ROOT. Default is `build`.
  build_context: "../"
  # Docker file name within docker build context, default is `Dockerfile`.
  build_file: "Dockerfile_mlcube"
  # GPU arguments
  gpu_args: "--gpus=all --shm-size=1g --network=host --ipc=host -v ~/.ssh:/root/.ssh"

tasks:
  download_demo:
    entrypoint: ./utils/download_demo.sh -a
    parameters:
      inputs:
        rclone_config: rclone.conf
      outputs:
        data_dir: demo_data/
        model_dir: demo_model/
  demo:
    entrypoint: ./run_demo.sh -a
    parameters:
      inputs:
        preprocessed_path: demo_data/mixtral_8x22b_preprocessed
        tokenizer_path: demo_data/tokenizer
        model_ckpt: demo_model/
      outputs:
        job_dir: demo_result/